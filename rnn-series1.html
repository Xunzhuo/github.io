<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta name="google-site-verification" content="TZE0rZyIqLl10trYu3BWBWa1Vmz6HFwhb2OcNEK4u-s" />
     <link rel="shortcut icon" href= https://picreso.oss-cn-beijing.aliyuncs.com/favicon.ico >
    <title>
        Coder主题 - 刘训灼
    </title>
    <meta name="description" content= 嘿，我是刘训灼～这里用于展示写的Hexo主题：Coder。 >
    <meta name="keywords" content= Blog,Hexo,Theme,刘训灼,LiuXunzhuo >
    
<link rel="stylesheet" href="/libs/highlight/styles/monokai-sublime.css">

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.1"></head>
<body id="bodyx">
    <div class="hd posts">
    <a href="/index.html"><i class="fa fa-home
 replay-btn" aria-hidden="true"></i></a>
    <div class="post-title">
        <p>
            RNN成长记(一)：CHAR-RNN🔥
        </p>
        <hr>
    </div>
    <div class="post-content">
        <p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/RNN-trip.png" alt=""></p>
<p><strong>提示：</strong>关于 RNN 的内容将横跨好几篇文章，包括基本的 RNN 结构、支持字符级序列生成的纯 TensorFlow 实现等等。而关于 RNN 的后续文章会包含更多高级主题，比如更加复杂的用于机器翻译任务的 Attention 机制等。</p>
<h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>使用循环结构拥有很多优势，最突出的一个优势就是它们能够在内存中存储前一个输入的表示。如此，我们就可以更好的预测后续的输出内容。持续追踪内存中的长数据流会出现很多的问题，比如 BPTT 算法中出现的梯度消失（gradient vanishing）问题就是其中之一。幸运的是，我们可以对架构做出一些改进来解决这个问题。</p>
<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d352d35342d31332d616d2e706e673f773d363230.png" alt=""></p>
<h2 id="二、CHAR-RNN"><a href="#二、CHAR-RNN" class="headerlink" title="二、CHAR-RNN"></a>二、CHAR-RNN</h2><p>我们不会去专门实现一个纯 TensorFlow 版本的单字符生成模型。相反，现在这个模型的目标是从每个输入句子中以每次一个字母的方式来读取字符流，并预测下一个字母是什么。在训练期间，我们将句子中的字母提供给网络，并用于生成输出的字母。而在推断（生成）期间，我们则会将上一次的输出作为新的输入（使用随机的 token 作为第一个输入）。</p>
<p>对于文本数据来说，我们做了一些预处理，请查看这个 GitHub 仓库来获取更多信息。</p>
<p><strong>输入样例</strong>：Hello there Charlie, how are you? Today I want a nice day. All I want for myself is a car.</p>
<ul>
<li>DATA_SIZE：输入的长度，即 <code>len(input)</code>；</li>
<li>BATCH_SIZE：每批的序列个数；</li>
<li>NUM_STEPS：每个切片的 token 数，即序列的长度 <code>seq_len</code>；</li>
<li>STATE_SIZE：每个隐层状态的隐层节点数，即值 <code>H</code>；</li>
<li>num_batches：数据集小批量化后的批量数</li>
</ul>
<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d362d31352d35372d616d2e706e673f773d363230.png" alt=""></p>
<p><strong>注意：</strong>由于我们是一行一行的将数据输入进 RNN 单元的，因此我们需要一列一列的将数据组成张量输入到网络中去，即我们必须把原始数据进行 reshape 处理。此外，每个字母都将作为一个被嵌入的独热编码（one-hot encoding，译注：又称 1-of-k encoding）的向量输入。在上图中，每个句子数据都被完美的切分进了一组组小批量数据，这只不过是为了达到更好的可视化目的，这样你就可以看到输入是怎样被切分的了。在实际的 -RNN 实现中，我们并不关心一个具体的句子，我们只是将整个输入切分成 num_batches 个批次，每个批次彼此独立，所以每个输入的长度都是 <code>num_steps</code>，即 <code>seq_len</code>。</p>
<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d362d33302d31372d616d2e706e673f773d363230.png" alt=""></p>
<h2 id="三、反向传播"><a href="#三、反向传播" class="headerlink" title="三、反向传播"></a>三、反向传播</h2><p>RNN 版本的反向传播 BPTT 刚开始可能有点混乱，尤其是计算隐藏状态对输入的影响之时。下面的代码使用原生 numpy 实现，符合下图中我的公式推导逻辑。</p>
<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d362d33332d33372d616d2e706e673f773d363230.png" alt=""></p>
<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d362d33352d32392d616d312e706e673f773d363230.png" alt=""></p>
<p><strong>前向传播：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> xrange(len(inputs)):</span><br><span class="line">    xs[t] = np.zeros((vocab_size,<span class="number">1</span>)) <span class="comment"># 独热编码</span></span><br><span class="line">    xs[t][inputs[t]] = <span class="number">1</span></span><br><span class="line">    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t<span class="number">-1</span>]) + bh) <span class="comment"># 隐藏状态</span></span><br><span class="line">    ys[t] = np.dot(Why, hs[t]) + by <span class="comment"># 下一个字符的未归一化对数似然概率</span></span><br><span class="line">    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) <span class="comment"># 下一个字符的概率</span></span><br><span class="line">    loss += -np.log(ps[t][targets[t],<span class="number">0</span>]) <span class="comment"># softmax（交叉熵损失）</span></span><br></pre></td></tr></table></figure>

<p><strong>反向传播:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(len(inputs))):</span><br><span class="line">    dy = np.copy(ps[t])</span><br><span class="line">    dy[targets[t]] -= <span class="number">1</span></span><br><span class="line">    dWhy += np.dot(dy, hs[t].T)</span><br><span class="line">    dby += dy</span><br><span class="line">    dh = np.dot(Why.T, dy) + dhnext  <span class="comment"># 反向传播给 h</span></span><br><span class="line">    dhraw = (<span class="number">1</span> - hs[t] * hs[t]) * dh <span class="comment"># 通过 tanh 的非线性进行反向传播</span></span><br><span class="line">    dbh += dhraw</span><br><span class="line">    dWxh += np.dot(dhraw, xs[t].T)</span><br><span class="line">    dWhh += np.dot(dhraw, hs[t<span class="number">-1</span>].T)</span><br><span class="line">    dhnext = np.dot(Whh.T, dhraw)</span><br></pre></td></tr></table></figure>

<h2 id="张量的形状"><a href="#张量的形状" class="headerlink" title="张量的形状"></a>张量的形状</h2><p>在实现之前，我们来谈谈张量的形状。在这个 CHAR-RNN 的例子上讲述张量形状这个概念有点奇怪，因此我会向你解释如何对其进行批量化以及它们是怎样完成 seq2seq 任务的。</p>
<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d33312d61742d382d34352d30372d706d2e706e673f773d363230.png" alt=""></p>
<p>这个任务对于一次性输入整行（全部 <code>batch_size</code> 个序列的）<code>seq_len</code> 这点上有点奇怪。通常来说，我们一次只传递一个批量，每个批量都有 <code>batch_size</code> 个序列，所以形状为<code>(batch_size, seq_len)</code>。我们通常也不会用 <code>seq_len</code> 来做分割，而是取整个序列的长度。对于 seq2seq 任务而言，本系列的第 2、3 和 5 篇文章中看到，我们会将大小为 <code>batch_size</code> 一个批量的序列作为输入，其中每个序列的长度为<code>seq_len</code> 。我们不能像在图中那样指定 <code>seq_len</code>，因为实际的<code>seq_len</code> 会根据全部样本的特点填充到最大值。我们会在比最大长度短的所有句子后填充一些填充符，从而达到最大值。不过现在还不是深入讨论这个问题的时候。</p>
<h2 id="四、Char-RNN-的-TensorFlow-实现（无-RNN-抽象）"><a href="#四、Char-RNN-的-TensorFlow-实现（无-RNN-抽象）" class="headerlink" title="四、Char-RNN 的 TensorFlow 实现（无 RNN 抽象）"></a>四、Char-RNN 的 TensorFlow 实现（无 RNN 抽象）</h2><p>我们将使用没有 RNN 类抽象的纯 TensorFlow 进行实现。同时还将使用我们自己的权重集来真正理解输入数据的流向以及输出是如何生成的。在这里我们只讨论代码里一些重点部分，而完整的代码我将给出相关链接。如果你想要使用 TF RNN 类进行实现，请转到本文第五小节。</p>
<p><strong>重点：</strong></p>
<p>首先我想讨论下如何生成批量化的数据。你可能注意到了，我们有一个额外的步骤，那就是将数据进行批量化处理，然后再将数据分割进 <code>seq_len</code>。这么做的原因是为了消除 RNN 结构中 BPTT 算法中产生的梯度消失问题。本质上来说，我们并不能同时处理多个字符。这是因为在反向传播中，如果序列太长，梯度就会下降得很快。因此，一个简单的技巧是保存一个 <code>seq_len</code> 长度的输出状态，然后将其作为下一个 <code>seq_len</code> 的 <code>initial_state</code>。这种由我们自行选择（使用 BPTT 来）处理的个数和更新频率的做法，就是所谓的截断反向传播（truncated backpropagation）。<code>initial_state</code> 从 0 开始，并在每轮计算中进行重置。因此，我们仍然能在一个特定的批次中从之前的 <code>seq_len</code> 序列里保存表示的某些类型。这么做的原因在于，在字符这种级别上，一个极小的序列并不能够学习到足够多的表示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(FLAGS, raw_data)</span>:</span></span><br><span class="line">    raw_X, raw_y = raw_data</span><br><span class="line">    data_length = len(raw_X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从原始数据中创建批量数据</span></span><br><span class="line">    num_batches = FLAGS.DATA_SIZE // FLAGS.BATCH_SIZE <span class="comment"># 每批的 token</span></span><br><span class="line">    data_X = np.zeros([num_batches, FLAGS.BATCH_SIZE], dtype=np.int32)</span><br><span class="line">    data_y = np.zeros([num_batches, FLAGS.BATCH_SIZE], dtype=np.int32)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches):</span><br><span class="line">        data_X[i, :] = raw_X[FLAGS.BATCH_SIZE * i: FLAGS.BATCH_SIZE * (i+<span class="number">1</span>)]</span><br><span class="line">        data_y[i, :] = raw_y[FLAGS.BATCH_SIZE * i: FLAGS.BATCH_SIZE * (i+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 尽管每个批次都有很多的 token</span></span><br><span class="line">    <span class="comment"># 但我们每次只想输入 seq_len 个 token</span></span><br><span class="line">    feed_size = FLAGS.BATCH_SIZE // FLAGS.SEQ_LEN</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(feed_size):</span><br><span class="line">        X = data_X[:, i * FLAGS.SEQ_LEN:(i+<span class="number">1</span>) * FLAGS.SEQ_LEN]</span><br><span class="line">        y = data_y[:, i * FLAGS.SEQ_LEN:(i+<span class="number">1</span>) * FLAGS.SEQ_LEN]</span><br><span class="line">        <span class="keyword">yield</span> (X, y)</span><br></pre></td></tr></table></figure>

<p>下面是使用我们自己的权重的代码。<code>rnn_cell</code> 函数用来接收来自前一个单元的输入和状态，从而生成 RNN 的输出，同时也是下一个单元的输入状态。下一个函数 <code>rnn_logits</code> 使用权重将我们的 RNN 输出进行转换，从而通过 softmax 生成 logits 概率并用于分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">(FLAGS, rnn_input, state)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_cell'</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        W_input = tf.get_variable(<span class="string">'W_input'</span>,</span><br><span class="line">            [FLAGS.NUM_CLASSES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">        W_hidden = tf.get_variable(<span class="string">'W_hidden'</span>,</span><br><span class="line">            [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">        b_hidden = tf.get_variable(<span class="string">'b_hidden'</span>, [FLAGS.NUM_HIDDEN_UNITS],</span><br><span class="line">            initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    <span class="keyword">return</span> tf.tanh(tf.matmul(rnn_input, W_input) +</span><br><span class="line">                   tf.matmul(state, W_hidden) + b_hidden)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_logits</span><span class="params">(FLAGS, rnn_output)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax'</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        W_softmax = tf.get_variable(<span class="string">'W_softmax'</span>,</span><br><span class="line">            [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_CLASSES])</span><br><span class="line">        b_softmax = tf.get_variable(<span class="string">'b_softmax'</span>,</span><br><span class="line">            [FLAGS.NUM_CLASSES], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    <span class="keyword">return</span> tf.matmul(rnn_output, W_softmax) + b_softmax</span><br></pre></td></tr></table></figure>

<p>我们将输入和独热编码在 RNN 的批处理中进行 reshape 操作。然后，我们就可以使用 <code>rnn_cell</code>、<code>rnn_logits</code> 和 softmax 来运行我们的 RNN 从而预测下一个 token 了。你可以看到，我们生成的状态与我们在这个简单实现中的 RNN 输出是一致的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, FLAGS)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 占位符</span></span><br><span class="line">        self.X = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="literal">None</span>],</span><br><span class="line">            name=<span class="string">'input_placeholder'</span>)</span><br><span class="line">        self.y = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="literal">None</span>],</span><br><span class="line">            name=<span class="string">'labels_placeholder'</span>)</span><br><span class="line">        self.initial_state = tf.zeros([FLAGS.NUM_BATCHES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备输入</span></span><br><span class="line">        X_one_hot = tf.one_hot(self.X, FLAGS.NUM_CLASSES)</span><br><span class="line">        rnn_inputs = [tf.squeeze(i, squeeze_dims=[<span class="number">1</span>]) \</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> tf.split(<span class="number">1</span>, FLAGS.SEQ_LEN, X_one_hot)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义 RNN cell</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_cell'</span>):</span><br><span class="line">            W_input = tf.get_variable(<span class="string">'W_input'</span>,</span><br><span class="line">                [FLAGS.NUM_CLASSES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">            W_hidden = tf.get_variable(<span class="string">'W_hidden'</span>,</span><br><span class="line">                [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">            b_hidden = tf.get_variable(<span class="string">'b_hidden'</span>,</span><br><span class="line">                [FLAGS.NUM_HIDDEN_UNITS],</span><br><span class="line">                initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建 RNN</span></span><br><span class="line">        state = self.initial_state</span><br><span class="line">        rnn_outputs = []</span><br><span class="line">        <span class="keyword">for</span> rnn_input <span class="keyword">in</span> rnn_inputs:</span><br><span class="line">            state = rnn_cell(FLAGS, rnn_input, state)</span><br><span class="line">            rnn_outputs.append(state)</span><br><span class="line">        self.final_state = rnn_outputs[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Logits 概率及预测</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax'</span>):</span><br><span class="line">            W_softmax = tf.get_variable(<span class="string">'W_softmax'</span>,</span><br><span class="line">                [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_CLASSES])</span><br><span class="line">            b_softmax = tf.get_variable(<span class="string">'b_softmax'</span>,</span><br><span class="line">                [FLAGS.NUM_CLASSES],</span><br><span class="line">                initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line">        logits = [rnn_logits(FLAGS, rnn_output) <span class="keyword">for</span> rnn_output <span class="keyword">in</span> rnn_outputs]</span><br><span class="line">        self.predictions = [tf.nn.softmax(logit) <span class="keyword">for</span> logit <span class="keyword">in</span> logits]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 损失与优化</span></span><br><span class="line">        y_as_list = [tf.squeeze(i, squeeze_dims=[<span class="number">1</span>]) \</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> tf.split(<span class="number">1</span>, FLAGS.SEQ_LEN, self.y)]</span><br><span class="line">        losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logit, label) \</span><br><span class="line">            <span class="keyword">for</span> logit, label <span class="keyword">in</span> zip(logits, y_as_list)]</span><br><span class="line">        self.total_loss = tf.reduce_mean(losses)</span><br><span class="line">        self.train_step = tf.train.AdagradOptimizer(</span><br><span class="line">            FLAGS.LEARNING_RATE).minimize(self.total_loss)</span><br></pre></td></tr></table></figure>

<p>我们偶尔也会从模型中进行采样。对于采样而言，可以选择使用 logits 概率中的最大值，或者在选择的类别中引入 <code>temperature</code> 参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, FLAGS, sampling_type=<span class="number">1</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    initial_state = tf.zeros([<span class="number">1</span>,FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">    predictions = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理预设 token</span></span><br><span class="line">    state = initial_state</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> FLAGS.START_TOKEN:</span><br><span class="line">        idx = FLAGS.char_to_idx[char]</span><br><span class="line">        idx_one_hot = tf.one_hot(idx, FLAGS.NUM_CLASSES)</span><br><span class="line">        rnn_input = tf.reshape(idx_one_hot, [<span class="number">1</span>, <span class="number">65</span>])</span><br><span class="line">        state =  rnn_cell(FLAGS, rnn_input, state)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在预设 token 后进行预测</span></span><br><span class="line">    logit = rnn_logits(FLAGS, state)</span><br><span class="line">    prediction = tf.argmax(tf.nn.softmax(logit), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    predictions.append(prediction.eval())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> token_num <span class="keyword">in</span> range(FLAGS.PREDICTION_LENGTH<span class="number">-1</span>):</span><br><span class="line">        idx_one_hot = tf.one_hot(prediction, FLAGS.NUM_CLASSES)</span><br><span class="line">        rnn_input = tf.reshape(idx_one_hot, [<span class="number">1</span>, <span class="number">65</span>])</span><br><span class="line">        state =  rnn_cell(FLAGS, rnn_input, state)</span><br><span class="line">        logit = rnn_logits(FLAGS, state)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对分布进行缩放</span></span><br><span class="line">        <span class="comment"># temperature 越高，产生的新词越多，也就越炫酷</span></span><br><span class="line">        <span class="comment"># 但同时也需要更多的样本</span></span><br><span class="line">        next_char_dist = logit/FLAGS.TEMPERATURE</span><br><span class="line">        next_char_dist = tf.exp(next_char_dist)</span><br><span class="line">        next_char_dist /= tf.reduce_sum(next_char_dist)</span><br><span class="line"></span><br><span class="line">        dist = next_char_dist.eval()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 单字符采样</span></span><br><span class="line">        <span class="keyword">if</span> sampling_type == <span class="number">0</span>:</span><br><span class="line">            prediction = tf.argmax(tf.nn.softmax(</span><br><span class="line">                                    next_char_dist), <span class="number">1</span>)[<span class="number">0</span>].eval()</span><br><span class="line">        <span class="keyword">elif</span> sampling_type == <span class="number">1</span>:</span><br><span class="line">            prediction = FLAGS.NUM_CLASSES - <span class="number">1</span></span><br><span class="line">            point = random.random()</span><br><span class="line">            weight = <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, FLAGS.NUM_CLASSES):</span><br><span class="line">                weight += dist[<span class="number">0</span>][index]</span><br><span class="line">                <span class="keyword">if</span> weight &gt;= point:</span><br><span class="line">                    prediction = index</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Pick a valid sampling_type!"</span>)</span><br><span class="line">        predictions.append(prediction)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure>

<p>我们还需要看看如何向数据流中传递 <code>initial_state</code> 参数。为了避免梯度消失的出现，每次处理完一个序列后，它和 <code>final_state</code> 都会被更新。注意，我们将零初始状态作为起始状态，然后在将这个状态传递给随后的序列，并将前一个序列的 <code>final_state</code> 作为新的输入状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">state = np.zeros([FLAGS.NUM_BATCHES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, (input_X, input_y) <span class="keyword">in</span> enumerate(epoch):</span><br><span class="line">	predictions, total_loss, state, _= model.step(sess, input_X,</span><br><span class="line">										 input_y, state)</span><br><span class="line">	training_losses.append(total_loss)</span><br></pre></td></tr></table></figure>

<h2 id="五、使用-TF-RNN-实现"><a href="#五、使用-TF-RNN-实现" class="headerlink" title="五、使用 TF RNN 实现"></a>五、使用 TF RNN 实现</h2><p>与上面不同的是，在下面这个实现中，我们将使用 TensorFlow 的 NN 工具来创建 RNN 抽象类。在使用这些类之前，理解这些类的输入内容、内部操作及输出结果是很重要的。由于我们仍然是使用基本的 <code>rnn_cell</code>，因此我们将使用截断误差反向传播，但是如果使用 GRU 或 LSTM，就没有必要了。其实，只需将整个数据分割成 <code>batch_size</code>，然后处理整个序列就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">(FLAGS)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取 cell 类型</span></span><br><span class="line">    <span class="keyword">if</span> FLAGS.MODEL == <span class="string">'rnn'</span>:</span><br><span class="line">        rnn_cell_type = tf.nn.rnn_cell.BasicRNNCell</span><br><span class="line">    <span class="keyword">elif</span> FLAGS.MODEL == <span class="string">'gru'</span>:</span><br><span class="line">        rnn_cell_type = tf.nn.rnn_cell.GRUCell</span><br><span class="line">    <span class="keyword">elif</span> FLAGS.MODEL == <span class="string">'lstm'</span>:</span><br><span class="line">        rnn_cell_type = tf.nn.rnn_cell.BasicLSTMCell</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"Choose a valid RNN unit type."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单一 cell</span></span><br><span class="line">    single_cell = rnn_cell_type(FLAGS.NUM_HIDDEN_UNITS)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Dropout</span></span><br><span class="line">    single_cell = tf.nn.rnn_cell.DropoutWrapper(single_cell,</span><br><span class="line">        output_keep_prob=<span class="number">1</span>-FLAGS.DROPOUT)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个状态作为单个 cell</span></span><br><span class="line">    stacked_cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * FLAGS.NUM_LAYERS)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> stacked_cell</span><br></pre></td></tr></table></figure>

<p>上面的代码创建的是我们特定的 RNN 结构。我们可以从许多不同的 RNN 单元类型中进行选择，但是在这里你可以看到三个最常见的类型（BasicRNN、GRU 和 LSTM）。我们用一定数量的隐藏单元来创建每个 RNN 单元。然后，我们可以在每个单元层之后之后添加一个 Dropout 层来进行正则化处理。最后，我们可以通过复制 <code>single_cell</code> 来实现堆叠的 RNN 结构。注意，<code>state_is_tuple=True</code> 条件被附加到了 <code>single_cell</code> 和 <code>stacked_cell</code> 里。这保证了在给定序列的每个输入之后返回一个包含状态的元组。如果使用 LSTM 单元，上述语句为真；否则无视。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_inputs</span><span class="params">(FLAGS, input_data)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_inputs'</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        W_input = tf.get_variable(<span class="string">"W_input"</span>,</span><br><span class="line">            [FLAGS.NUM_CLASSES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># &lt;BATCH_SIZE, seq_len, num_hidden_units&gt;</span></span><br><span class="line">    embeddings = tf.nn.embedding_lookup(W_input, input_data)</span><br><span class="line">    <span class="comment"># &lt;seq_len, BATCH_SIZE, num_hidden_units&gt;</span></span><br><span class="line">    <span class="comment"># BATCH_SIZE will be in columns bc we feed in row by row into RNN.</span></span><br><span class="line">    <span class="comment"># 1st row = 1st tokens from each batch</span></span><br><span class="line">    <span class="comment">#inputs = [tf.squeeze(i, [1]) for i in tf.split(1, FLAGS.SEQ_LEN, embeddings)]</span></span><br><span class="line">    <span class="comment"># NO NEED if using dynamic_rnn(time_major=False)</span></span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_softmax</span><span class="params">(FLAGS, outputs)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_softmax'</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        W_softmax = tf.get_variable(<span class="string">"W_softmax"</span>,</span><br><span class="line">            [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_CLASSES])</span><br><span class="line">        b_softmax = tf.get_variable(<span class="string">"b_softmax"</span>, [FLAGS.NUM_CLASSES])</span><br><span class="line"></span><br><span class="line">    logits = tf.matmul(outputs, W_softmax) + b_softmax</span><br><span class="line">    <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>

<p>这里的 <code>rnn_inputs</code> 函数与原生 TensorFlow 版本的实现由一些不同。正如你所看到的，我们不再需要 reshape 输入。这是因为 <code>tf.nn.dynamic_rnn</code> 会帮我们处理来自 RNN 的 output 和 state。这是一种效率非常高的 RNN 抽象，它还要求输入的数据不被预先 reshape，因此我们所有全部内容都是嵌入的。<code>rnn_softmax</code> 类提供的 logits 功能和前面所实现内容的完全一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, FLAGS)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="string">''' 数据占位符'''</span></span><br><span class="line">        self.input_data = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="literal">None</span>])</span><br><span class="line">        self.targets = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="literal">None</span>])</span><br><span class="line"></span><br><span class="line">        <span class="string">''' RNN 单元 '''</span></span><br><span class="line">        self.stacked_cell = rnn_cell(FLAGS)</span><br><span class="line">        self.initial_state = self.stacked_cell.zero_state(</span><br><span class="line">            FLAGS.NUM_BATCHES, tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="string">''' RNN 输入 '''</span></span><br><span class="line">        <span class="comment"># 嵌入权重 W_input)</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_inputs'</span>):</span><br><span class="line">            W_input = tf.get_variable(<span class="string">"W_input"</span>,</span><br><span class="line">                [FLAGS.NUM_CLASSES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">        inputs = rnn_inputs(FLAGS, self.input_data)</span><br><span class="line"></span><br><span class="line">        <span class="string">''' RNN 输出 '''</span></span><br><span class="line">        <span class="comment"># outputs: &lt;seq_len, BATCH_SIZE, num_hidden_units&gt;</span></span><br><span class="line">        <span class="comment"># state: &lt;BATCH_SIZE, num_layers*num_hidden_units&gt;</span></span><br><span class="line">        outputs, state = tf.nn.dynamic_rnn(cell=self.stacked_cell, inputs=inputs,</span><br><span class="line">                                           initial_state=self.initial_state)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># &lt;seq_len*BATCH_SIZE, num_hidden_units&gt;</span></span><br><span class="line">        outputs = tf.reshape(tf.concat(<span class="number">1</span>, outputs), [<span class="number">-1</span>, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line"></span><br><span class="line">        <span class="string">''' 处理 RNN 输出 '''</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_softmax'</span>):</span><br><span class="line">            W_softmax = tf.get_variable(<span class="string">"W_softmax"</span>,</span><br><span class="line">                [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_CLASSES])</span><br><span class="line">            b_softmax = tf.get_variable(<span class="string">"b_softmax"</span>, [FLAGS.NUM_CLASSES])</span><br><span class="line">        <span class="comment"># Logit</span></span><br><span class="line">        self.logits = rnn_softmax(FLAGS, outputs)</span><br><span class="line">        self.probabilities = tf.nn.softmax(self.logits)</span><br><span class="line"></span><br><span class="line">        <span class="string">''' Loss '''</span></span><br><span class="line">        y_as_list = tf.reshape(self.targets, [<span class="number">-1</span>])</span><br><span class="line">        self.loss = tf.reduce_mean(</span><br><span class="line">            tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">                self.logits, y_as_list))</span><br><span class="line">        self.final_state = state</span><br><span class="line"></span><br><span class="line">        <span class="string">''' 优化 '''</span></span><br><span class="line">        self.lr = tf.Variable(<span class="number">0.0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">        trainable_vars = tf.trainable_variables()</span><br><span class="line">        <span class="comment"># 梯度截断防止梯度消失或梯度爆炸</span></span><br><span class="line">        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, trainable_vars),</span><br><span class="line">                                          FLAGS.GRAD_CLIP)</span><br><span class="line">        optimizer = tf.train.AdamOptimizer(self.lr)</span><br><span class="line">        self.train_optimizer = optimizer.apply_gradients(</span><br><span class="line">            zip(grads, trainable_vars))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存模型的组件</span></span><br><span class="line">        self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">        self.saver = tf.train.Saver(tf.all_variables())</span><br></pre></td></tr></table></figure>

<p>还要注意的是，我们不会手动的在嵌入之前对输入 token 进行独热编码，这是因为<code>rnn_inputs</code> 函数里的 <code>tf.nn.embedding_lookup</code> 会自动帮我们完成。</p>
<p>为了生成输出，我们使用了 <code>tf.nn.dynamic_rnn</code> ，其输出结果为每个输入的输出以及返回状态（即包含上一次每个输入批次的状态的元组）。最后，我们将输出进行了 reshape ，从而得到 logits 概率并用于与 targets 进行比较。</p>
<p>注意到 <code>self.initial_state</code> 由 <code>stacked_cell.zero_state</code> 初始化，我们只需要指定的 <code>batch_size</code> 就够了。对于这里的 <code>NUM_BATCHES</code> 请查看前面的张量形状一节中的说明。有一种替代方法可以不包含初始状态，<code>dynamic_rnn()</code> 会自行处理，我们所需要做的就是指定数据类型（即<code>dtype = tf.float32</code> 等)。可惜我们并不能这样做，因为我们要把序列的 <code>final_state</code> 作为了下一个序列的 <code>initial_state</code> 。你可能还会注意到，尽管 <code>self.initial_state</code> 不是占位符，我们还是把前一次 <code>final_state</code> 传给了新的 <code>initial_state</code>。当然，我们可以通过重新定义 <code>step()</code> 里的 <code>self.initial_state</code> 来输入自己的初始值。不管怎样，一旦用到 <code>input_feeds</code> ，我们就需要计算 <code>output_feed</code>，而如果没有用到，那么就会跳回使用重载之前的值（也就是 <code>stacked_cell.zero_state</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, sess, batch_X, batch_y, initial_state=None)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> initial_state == <span class="literal">None</span>:</span><br><span class="line">        input_feed = &#123;self.input_data: batch_X,</span><br><span class="line">                      self.targets: batch_y&#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_feed = &#123;self.input_data: batch_X,</span><br><span class="line">                      self.targets: batch_y,</span><br><span class="line">                      self.initial_state: initial_state&#125;</span><br><span class="line"></span><br><span class="line">    output_feed = [self.loss,</span><br><span class="line">                   self.final_state,</span><br><span class="line">                   self.logits,</span><br><span class="line">                   self.train_optimizer]</span><br><span class="line">    outputs = sess.run(output_feed, input_feed)</span><br><span class="line">    <span class="keyword">return</span> outputs[<span class="number">0</span>], outputs[<span class="number">1</span>], outputs[<span class="number">2</span>], outputs[<span class="number">3</span>]</span><br></pre></td></tr></table></figure>

<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>我们来看看结果。这绝不是一种惊天动地的创造，但我确实是用了 <code>temperature</code> 而不是 <code>argmax</code> 进行生成。因此，我们可以看到生成结果里包含很多新奇的创意，但同时错误也很多（包括语法、拼写、排序等）。我只让网络训练了 10 轮，但已经开始看到单词和句子结构了，甚至还能看到每个角色的表演台词（数据集是莎士比亚的作品）。为了获得不错的结果，可以让它通宵在 GPU 上进行训练。</p>
<p>看到这，估计连莎士比亚都要给跪了。</p>
<p><strong>更新</strong>：我对典型输入、输出和状态张量的形状有很多疑问。</p>
<ul>
<li><strong>输入</strong>：[num_batches, seq_len, num_classes]</li>
<li><strong>输出</strong>：[num_batches, seq_len, num_hidden_units] （每个状态的全部输出）</li>
<li><strong>状态</strong>：[num_batches, num_hidden_units] （上一次状态的输出）</li>
</ul>
<p>在下一篇文章中，我们将处理变长序列，展示文本分类的具体实现。</p>

    </div>

    
        <hr class="fhr">
        <div id="vcomments"></div>
    
</div>
    <div class="footer" id="footer">
    <p><h4>版权所有 © 2020 | 作者: 刘训灼 | 主题 By <a class="theme-author" href="https://github.com/Xunzhuo/hexo-theme-coder" target="_blank" rel="noopener" style="font-size:14px; color: #969696">Coder</a></h4>
    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <span id="busuanzi_container_site_pv">本站浏览总访问量: <span id="busuanzi_value_site_pv"></span></span>
        <span class="post-meta-divider">|</span>
        <span id="busuanzi_container_site_uv">本站访问人数: <span id="busuanzi_value_site_uv"></span></span>
    
    <label class="el-switch el-switch-blue el-switch-sm" style="vertical-align: sub;">
        <input type="checkbox" name="switch" id="update_style">
        <span class="el-switch-style"></span>
    </label>

    <!--         <script type="text/javascript">
    var cnzz_protocol = (("https:" == document.location.protocol) ? "https://" : "http://");
    document.write(unescape("%3Cspan id='cnzz_stat_icon_1278548644'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/stat.php%3Fid%3D1278548644%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
    </script> -->
</p>
</div>

<input type="hidden" id="web_style" value="black">
<input type="hidden" id="valine_appid" value="NOsswOncKgc8HOxqo9oxIWlX-gzGzoHsz">
<input type="hidden" id="valine_appKey" value="z1FihjWEbS8uIfUQdmCtK7zz">

<script src="/libs/jquery.min.js"></script>


<script src="/libs/highlight/highlight.pack.js"></script>

<script src='//cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>

<script src="/js/js.js"></script>

<style type="text/css">
.v * {
color: #698fca;
}
.v .vlist .vcard .vhead .vsys {
color: #3a3e4a;
}
.v .vlist .vcard .vh .vmeta .vat {
color: #638fd5;
}
.v .vlist .vcard .vhead .vnick {
color: #6ba1ff;
}
.v a {
color: #8696b1;
}
.v .vlist .vcard .vhead .vnick:hover {
color: #669bfc;
}
</style>
    <script type="text/javascript" color="173,174,173" opacity='1' zIndex="-2" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>
</body>
</html>
