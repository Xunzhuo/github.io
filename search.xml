<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>算法竞赛资料整理分享🌲</title>
      <link href="/oishare.html"/>
      <url>/oishare.html</url>
      
        <content type="html"><![CDATA[<h1 id="算法竞赛资料分享🤹🏼‍♀️"><a href="#算法竞赛资料分享🤹🏼‍♀️" class="headerlink" title="算法竞赛资料分享🤹🏼‍♀️"></a>算法竞赛资料分享🤹🏼‍♀️</h1><p>因为准备实习👔，今天早上整理了一下算法的课件、书籍、论文、习题</p><ul><li>不管是准备校招，进BAT🚀；</li><li>还是自学算法竞赛💼；</li><li>或者单纯的课外拓展🤷；</li></ul><p>对程序员👔而言，算法学习都是有必要的，只是可能要求深浅不同，所以，开始学起来吧🌈～</p><p><img src="http://picreso.oss-cn-beijing.aliyuncs.com/alo.jpg" alt=""></p><h3 id="全面收集、整理了从高中参加竞赛到现在的算法竞赛课件、论文集、书籍、习题等，并分类如下👀："><a href="#全面收集、整理了从高中参加竞赛到现在的算法竞赛课件、论文集、书籍、习题等，并分类如下👀：" class="headerlink" title="全面收集、整理了从高中参加竞赛到现在的算法竞赛课件、论文集、书籍、习题等，并分类如下👀："></a>全面收集、整理了从高中参加竞赛到现在的<code>算法竞赛课件</code>、<code>论文集</code>、<code>书籍</code>、<code>习题</code>等，并分类如下👀：</h3><blockquote><p>欢迎大家贡献你的资料丰富这个Repo</p><p>文件很多，目录很长，所以分为文件夹目录和文件树，如有不便之处，敬请谅解😣</p><p>如果对你有所帮助，请 star✨ 或者 follow👨 支持一下</p><p>算法路上加油⛽️</p><p>如有侵权，麻烦提 Issues 或联系 <a href="mailto:981242367@qq.com">981242367@qq.com</a> 删改</p></blockquote><hr><h2 id="文件导图👀："><a href="#文件导图👀：" class="headerlink" title="文件导图👀："></a>文件导图👀：</h2><p><img src="http://picreso.oss-cn-beijing.aliyuncs.com/mindmap.png" alt=""></p><h2 id="文件夹目录👀："><a href="#文件夹目录👀：" class="headerlink" title="文件夹目录👀："></a>文件夹目录👀：</h2><ul><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E9%AB%98%E4%B8%AD%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%A5%97%E4%BB%B6" target="_blank" rel="noopener">数据结构套件</a>🚀</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/高中算法套件" target="_blank" rel="noopener">算法套件</a>🌲</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E9%AB%98%E4%B8%ADC%2B%2B%20%E5%A5%97%E4%BB%B6" target="_blank" rel="noopener">C++套件</a>🍟</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/基础算法" target="_blank" rel="noopener">基础算法</a>🤹🏼‍♀️<ul><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90" target="_blank" rel="noopener">复杂度分析</a>🌟</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E9%AB%98%E7%B2%BE%E5%BA%A6" target="_blank" rel="noopener">高精度</a>🌟</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E6%9A%B4%E5%8A%9B" target="_blank" rel="noopener">暴力</a>🌟</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%88%86" target="_blank" rel="noopener">二分</a>🌟</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E5%88%86%E6%B2%BB" target="_blank" rel="noopener">分治</a>🌟</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E6%90%9C%E7%B4%A2" target="_blank" rel="noopener">搜索</a>🌟</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E8%B4%AA%E5%BF%83" target="_blank" rel="noopener">贪心</a>🌟</li></ul></li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/数据结构" target="_blank" rel="noopener">数据结构</a>❄️<ul><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84" target="_blank" rel="noopener">基础数据结构</a>💫</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%A0%91" target="_blank" rel="noopener">树</a>💫</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%9B%BE%E8%AE%BA" target="_blank" rel="noopener">图</a>💫</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%88%86%E5%9D%97" target="_blank" rel="noopener">分块</a>💫</li></ul></li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/动态规划" target="_blank" rel="noopener">动态规划</a>🎮</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/C%2B%2B" target="_blank" rel="noopener">C++</a>🌈</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E5%AD%97%E7%AC%A6%E4%B8%B2" target="_blank" rel="noopener">字符串</a>☂️</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E6%95%B0%E5%AD%A6" target="_blank" rel="noopener">数学</a>🎱</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/计算几何" target="_blank" rel="noopener">计算几何</a>🧠</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E4%B9%A6%E7%B1%8D" target="_blank" rel="noopener">书籍</a>🍟<ul><li><code>算法</code>：<ul><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E7%AE%97%E6%B3%95/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E.pdf" target="_blank" rel="noopener">数学之美</a>👑</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95.pdf" target="_blank" rel="noopener">数据结构与算法（Java）</a>👑</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B%E5%85%A5%E9%97%A8%E7%BB%8F%E5%85%B8.pdf" target="_blank" rel="noopener">算法竞赛</a>👑</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%EF%BC%88%E7%AC%AC%E5%9B%9B%E7%89%88%EF%BC%89.pdf" target="_blank" rel="noopener">算法</a>👑</li></ul></li><li><code>语言</code>：<ul><li><code>C</code>：<ul><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E8%AF%AD%E8%A8%80/C/c%20primerplus.pdf" target="_blank" rel="noopener">CPrimerPlus</a>👑</li></ul></li><li><code>C++</code>：<ul><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E4%B9%A6%E7%B1%8D/%E8%AF%AD%E8%A8%80/C%2B%2B" target="_blank" rel="noopener">C++PrimerPlus</a>👑(TOO LARGE TO SUBMIT)</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E4%B9%A6%E7%B1%8D/%E8%AF%AD%E8%A8%80/C%2B%2B" target="_blank" rel="noopener">C++Primer</a>👑(TOO LARGE TO SUBMMIMT)</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E8%AF%AD%E8%A8%80/C%2B%2B/Effective%20C%2B%2B.pdf" target="_blank" rel="noopener">Effective C++</a>👑</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E8%AF%AD%E8%A8%80/C%2B%2B/Effective%20STL.pdf" target="_blank" rel="noopener">Effective STL</a>👑</li></ul></li><li><code>Python</code>：<ul><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E8%AF%AD%E8%A8%80/Python/%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5.pdf" target="_blank" rel="noopener">从入门到实践</a>👑</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E8%AF%AD%E8%A8%80/Python/%E6%B5%81%E7%95%85%E7%9A%84python.pdf" target="_blank" rel="noopener">流畅的Python</a>👑</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E8%AF%AD%E8%A8%80/Python/Effective%20Python.pdf" target="_blank" rel="noopener">Effective Python</a>👑</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/blob/master/%20%E4%B9%A6%E7%B1%8D/%E8%AF%AD%E8%A8%80/Python/PythonCookbook.pdf" target="_blank" rel="noopener">PythonCookbook</a>👑</li></ul></li></ul></li></ul></li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E9%A2%98" target="_blank" rel="noopener">习题</a>⛄️</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">研究性论文集</a>🎄<ul><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F1999%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">1999论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2000%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2000论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2001%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2001论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2002%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2002论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2003%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2003论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2004%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2004论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2005%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2005论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2006%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2006论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2007%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2007论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2008%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2008论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2009%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2009论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2013%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2013论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2014%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2014论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2015%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2015论文集</a>🧳</li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E7%A0%94%E7%A9%B6%E6%80%A7%E8%AE%BA%E6%96%87%E9%9B%86/%E5%9B%BD%E5%AE%B6%E9%9B%86%E8%AE%AD%E9%98%9F2018%E8%AE%BA%E6%96%87%E9%9B%86" target="_blank" rel="noopener">2018论文集</a>🧳</li></ul></li><li><a href="https://github.com/Xunzhuo/OI_Sharing/tree/master/%20%E6%9D%82" target="_blank" rel="noopener">杂</a>🍀</li></ul><p><img src="http://picreso.oss-cn-beijing.aliyuncs.com/alpha.jpeg" alt=""></p><hr><h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      
      
      <categories>
          
          <category> Coder必知必会 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 分享 </tag>
            
            <tag> GitHub </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法竞赛刷题网站汇总🌲</title>
      <link href="/OJ.html"/>
      <url>/OJ.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/1_w_-9_SBx5-p2tXHYp0zMuw.png" alt=""></p><h2 id="OJ网站汇总🚀："><a href="#OJ网站汇总🚀：" class="headerlink" title="OJ网站汇总🚀："></a>OJ网站汇总🚀：</h2><hr><h3 id="🚀国内Online-Judge"><a href="#🚀国内Online-Judge" class="headerlink" title="🚀国内Online Judge"></a>🚀国内Online Judge</h3><h4 id="🌲三大OJ🌲"><a href="#🌲三大OJ🌲" class="headerlink" title="🌲三大OJ🌲"></a>🌲三大OJ🌲</h4><ol><li>🎄浙江大学 <a href="http://acm.zju.edu.cn/" target="_blank" rel="noopener">http://acm.zju.edu.cn</a> 超过2000题，支持C/C++/Pascal/Java/Python/Perl/Scheme/PHP</li><li>🎄北京大学 <a href="http://poj.org/" target="_blank" rel="noopener">http://poj.org</a> 超过2000题，支持C/C++/Pascal/Java/Fortran</li><li>🎄哈尔滨工业大学 <a href="http://acm.hit.edu.cn/" target="_blank" rel="noopener">http://acm.hit.edu.cn</a> 超过2000题，支持C/C++/Pascal/Java/Fortran</li></ol><h4 id="🌲非常🔥的hdu🌲"><a href="#🌲非常🔥的hdu🌲" class="headerlink" title="🌲非常🔥的hdu🌲"></a>🌲非常🔥的hdu🌲</h4><ul><li>🎄杭州电子科技大学 <a href="http://acm.hdu.edu.cn/" target="_blank" rel="noopener">http://acm.hdu.edu.cn</a> 超过2000题，支持C/C++/Pascal/Java杭电OJ是国内最为活跃的OJ</li><li>🎄每周都会举办bestcoder比赛，相当于国内的codeforce：<a href="http://bestcoder.hdu.edu.cn/" target="_blank" rel="noopener">http://bestcoder.hdu.edu.cn/</a></li></ul><h4 id="🌲Set-of-OJ：vjudge🌲"><a href="#🌲Set-of-OJ：vjudge🌲" class="headerlink" title="🌲Set of OJ：vjudge🌲"></a>🌲Set of OJ：vjudge🌲</h4><ul><li>🎄虚拟OJ：<a href="https://vjudge.net/" target="_blank" rel="noopener">https://vjudge.net/</a> 这个网站的特色就是用户可以自己举办比赛，vjudge支持数十个OJ网站，用户可以从这些OJ网站上选择题目，可以选择一些同类型题目形成一个题集。</li><li>🎄但是vjudge上的题目并不会永久保存，过一段时间就被清空了。</li></ul><hr><h3 id="🚀国外Online-Judge"><a href="#🚀国外Online-Judge" class="headerlink" title="🚀国外Online Judge"></a>🚀国外Online Judge</h3><ol><li>🎄CF：CodeForce：<a href="http://codeforces.com/problemset" target="_blank" rel="noopener">http://codeforces.com/problemset</a> 世界顶级OJ，CodeForce还提供了API接口：<a href="http://codeforces.com/api/help" target="_blank" rel="noopener">http://codeforces.com/api/help</a></li><li>🎄Saratov State University <a href="http://acm.sgu.ru/" target="_blank" rel="noopener">http://acm.sgu.ru</a> 超过400题，支持C/C++/C#/Java/Delphi</li><li>🎄UVA：University of Valladolid <a href="http://uva.onlinejudge.org/" target="_blank" rel="noopener">http://uva.onlinejudge.org</a> 超过800题，支持C/C++/Pascal/Java</li><li>🎄Ural State University <a href="http://acm.timus.ru/" target="_blank" rel="noopener">http://acm.timus.ru</a> 超过800题，支持C/C++/C#/Pascal/Java</li><li>🎄Sphere Research Labs <a href="http://www.spoj.pl/" target="_blank" rel="noopener">http://www.spoj.pl</a> 超过1000题，支持几乎所有常见语言</li></ol><hr><h3 id="🚀入门到进阶的Online-Judge"><a href="#🚀入门到进阶的Online-Judge" class="headerlink" title="🚀入门到进阶的Online Judge"></a>🚀入门到进阶的Online Judge</h3><ol><li>🎄vijos：大部分题目是NOI题目 <a href="https://vijos.org/" target="_blank" rel="noopener">https://vijos.org/</a></li><li>🎄洛谷：<a href="https://www.luogu.org/problemnew/lists" target="_blank" rel="noopener">https://www.luogu.org/problemnew/lists</a></li><li>🎄RQNOJ：和vijos很像，适合NOI刷题 <a href="http://www.rqnoj.cn/problem" target="_blank" rel="noopener">http://www.rqnoj.cn/problem</a></li></ol><hr><h3 id="🚀招聘面试Online-Judge"><a href="#🚀招聘面试Online-Judge" class="headerlink" title="🚀招聘面试Online Judge"></a>🚀招聘面试Online Judge</h3><ol><li>🎄牛客网：<a href="https://www.nowcoder.com/" target="_blank" rel="noopener">https://www.nowcoder.com/</a></li><li>🎄leetcode：<a href="https://leetcode.com/problemset/all/" target="_blank" rel="noopener">https://leetcode.com/problemset/all/</a></li><li>🎄LintCode：<a href="https://www.lintcode.com/zh-cn/" target="_blank" rel="noopener">https://www.lintcode.com/zh-cn/</a></li><li>🎄51nod：<a href="http://www.51nod.com/Challenge/ProblemList.html#!#isAsc=false" target="_blank" rel="noopener">http://www.51nod.com/Challenge/ProblemList.html#!#isAsc=false</a></li><li>🎄hackerrank：<a href="https://www.hackerrank.com/" target="_blank" rel="noopener">https://www.hackerrank.com/</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Coder必知必会 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法竞赛 </tag>
            
            <tag> 刷题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何用神经网络预测股票趋势❓</title>
      <link href="/tensor-stock.html"/>
      <url>/tensor-stock.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/stock1.jpg" alt=""></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>疫情期间，我爸妈又开始炒股了，鉴于之前做过一个AI结合的量化交易项目，但是不是负责算法部分，所以想自己尝试一下，实现一个算法引擎。</p><p>纯数据科学只能做参考，最好结合传统量化交易和舆情分析，我后面有时间会尝试三者结合，希望有更好效果。</p><hr><p>在本教程中，你将了解到如何使用被称作长短期记忆网络（LSTM）的时间序列模型。LSTM 模型在保持长期记忆方面非常强大。阅读这篇教程时，你将：</p><ul><li>明白预测股市走势的动机；</li><li>下载股票数据 — 你将使用由 Alpha Vantage 或 Kaggle 收集的股票数据；</li><li>将数据划分为训练集和测试集，并将其标准化；</li><li>简要讨论一下为什么 LSTM 模型可以预测未来多步的情形；</li><li>使用现有数据预测股票趋势，并将结果可视化。</li></ul><p><strong>注意：请不要认为 LSTM 是一种可以完美预测股票趋势的可靠模型，也不要盲目使用它进行股票交易</strong>。我只是出于对机器学习的兴趣做了这个实验。在大部分情况下，这个模型的确能发现数据中的特定规律并准确预测股票的走势。但是否将其用于实际的股票市场取决于你自己。</p><h3 id="为什么要用时间序列模型？"><a href="#为什么要用时间序列模型？" class="headerlink" title="为什么要用时间序列模型？"></a>为什么要用时间序列模型？</h3><p>作为一名股民，如果你能对股票价格进行正确的建模，你就可以通过在合适的时机买入或卖出来获取利益。因此，你需要能通过一组历史数据来预测未来数据的模型——时间序列模型。</p><p><strong>警告</strong>：股价本身因受到诸多因素影响而难以预测，这意味着你难以找到一种能完美预测股价的模型。并不只有我一人如此认为。普林斯顿大学的经济学教授 Burton Malkiel 在他 1973 年出版的《A Random Walk Down Wall Street》一书中写道：“如果股市足够高效，以至于人们能从公开的股价中知晓影响它的全部因素，那么人人都能像投资专业人士那样炒股”。</p><p>但是，请保持信心，用机器学习的方法来预测这完全随机的股价仍有一丝希望。我们至少能通过建模来预测这组数据的实际走势。换而言之，不必知晓股价的确切值，你只要能预测股价要涨还是要跌就万事大吉了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 请确保你安装了这些包，并且能运行成功以下代码</span></span><br><span class="line"><span class="keyword">from</span> pandas_datareader <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> datetime <span class="keyword">as</span> dt</span><br><span class="line"><span class="keyword">import</span> urllib.request, json </span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># TensorFlow 1.6 版本下测试通过</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br></pre></td></tr></table></figure><h3 id="下载数据"><a href="#下载数据" class="headerlink" title="下载数据"></a>下载数据</h3><p>你可以从以下来源下载数据：</p><ol><li>Alpha Vantage。首先，你必须从 <a href="https://www.alphavantage.co/support/#api-key" target="_blank" rel="noopener">这个网站</a> 获取所需的 API key。在此之后，将它的值赋给变量 <code>api_key</code>。</li><li>从 <a href="https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs" target="_blank" rel="noopener">这个页面</a> 下载并将其中的 <em>Stocks</em> 文件夹拷贝到你的工程目录下。</li></ol><p>股价中包含几种不同的数据，它们是：</p><ul><li>开盘价：一天中股票刚开盘时的价格；</li><li>收盘价：一天中股票收盘时的价格；</li><li>最高价：一天中股价的最大值；</li><li>最低价：一天中股价的最小值。</li></ul><h3 id="从-Alpha-Vantage-获取数据"><a href="#从-Alpha-Vantage-获取数据" class="headerlink" title="从 Alpha Vantage 获取数据"></a>从 Alpha Vantage 获取数据</h3><p>为了从 Alpha Vantage 上下载美国航空公司的股价数据用于分析，你要将行情显示代号 <code>ticker</code> 设置为 <code>&quot;AAL&quot;</code>。同时，你也要定义一个 <code>url_string</code> 变量来获取包含最近 20 年内的全部股价信息的 JSON 文件，以及文件保存路径 <code>file_to_save</code>。别忘了用你的 <code>ticker</code> 变量来帮助你命名你下载下来的文件。</p><p>接下来，设定一个条件：如果本地没有保存的数据文件，就从 <code>url_string</code> 指明的 URL 下载数据，并将其中的日期、最低价、最高价、交易量、开盘价和收盘价存入 Pandas 的 DataFrame <code>df</code> 中，再将其保存到 <code>file_to_save</code>；否则直接从本地读取 csv 文件就好了。</p><h3 id="从-Kaggle-获取数据"><a href="#从-Kaggle-获取数据" class="headerlink" title="从 Kaggle 获取数据"></a>从 Kaggle 获取数据</h3><p>从 Kaggle 上找到的数据是一系列 csv 表格，你不需要对它进行任何处理就可以直接读入 Pandas 的 DataFrame 中。确保你正确地将 <em>Stocks</em> 文件夹放在项目的主目录中。</p><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><p>现在，将这些数据打印到 DataFrame 中吧！由于数据的顺序在时间序列模型中至关重要，所以请确保你的数据已经按照日期排好序了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按日期排序</span></span><br><span class="line">df = df.sort_values(<span class="string">'Date'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查结果</span></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><p>看看你的数据，并从中找到伴随时间推移而具有的不同规律。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize = (<span class="number">18</span>,<span class="number">9</span>))</span><br><span class="line">plt.plot(range(df.shape[<span class="number">0</span>]),(df[<span class="string">'Low'</span>]+df[<span class="string">'High'</span>])/<span class="number">2.0</span>)</span><br><span class="line">plt.xticks(range(<span class="number">0</span>,df.shape[<span class="number">0</span>],<span class="number">500</span>),df[<span class="string">'Date'</span>].loc[::<span class="number">500</span>],rotation=<span class="number">45</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Date'</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Mid Price'</span>,fontsize=<span class="number">18</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="https://picreso.oss-cn-beijing.aliyuncs.com/stock11.png" alt="" style="zoom:50%;" /><p>这幅图包含了很多信息。我特意选取了这家公司的股价图，因为它包含了股价的多种不同规律。这将使你的模型更健壮，也让它能更好地预测不同情形下的股价。</p><p>另一件值得注意的事情是 2017 年的股价远比上世纪七十年代的股价高且波动更大。因此，你要在<strong>数据标准化</strong>的过程中，注意让这些部分的数据落在相近的数值区间内。</p><h3 id="将数据划分为训练集和测试集"><a href="#将数据划分为训练集和测试集" class="headerlink" title="将数据划分为训练集和测试集"></a>将数据划分为训练集和测试集</h3><p>首先通过对每一天的最高和最低价的平均值来算出 <code>mid_prices</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先用最高和最低价来算出中间价</span></span><br><span class="line">high_prices = df.loc[:,<span class="string">'High'</span>].as_matrix()</span><br><span class="line">low_prices = df.loc[:,<span class="string">'Low'</span>].as_matrix()</span><br><span class="line">mid_prices = (high_prices+low_prices)/<span class="number">2.0</span></span><br></pre></td></tr></table></figure><p>然后你就可以划分数据集了。前 11000 个数据属于训练集，剩下的都属于测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = mid_prices[:<span class="number">11000</span>] </span><br><span class="line">test_data = mid_prices[<span class="number">11000</span>:]</span><br></pre></td></tr></table></figure><p>接下来我们需要一个换算器 <code>scaler</code> 用于标准化数据。<code>MinMaxScalar</code> 会将所有数据换算到 0 和 1 之间。同时，你也可以将两个数据集都调整为 <code>[data_size, num_features]</code> 的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将所有数据缩放到 0 和 1 之间</span></span><br><span class="line"><span class="comment"># 在缩放时请注意，缩放测试集数据时请使用缩放训练集数据的参数</span></span><br><span class="line"><span class="comment"># 因为在测试前你是不应当知道测试集数据的</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">train_data = train_data.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">test_data = test_data.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>上面我们注意到不同年代的股价处于不同的价位，如果不做特殊处理的话，在标准化后的数据中，上世纪的股价数据将非常接近于 0。这对模型的学习过程没啥好处。所以我们将整个时间序列划分为若干个区间，并在每一个区间上做标准化。这里每一个区间的长度取值为 2500。</p><p><strong>提示</strong>：因为每一个区间都被独立地初始化，所以在两个区间的交界处会引入一个“突变”。为了避免这个“突变”给我们的模型带来大麻烦，这里的每一个区间长度不要太小。</p><p>本例中会引入 4 个“突变”，鉴于数据有 11000 组，所以它们无关紧要。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用训练集来训练换算器 scaler，并且调整数据使之更平滑</span></span><br><span class="line">smoothing_window_size = <span class="number">2500</span></span><br><span class="line"><span class="keyword">for</span> di <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">10000</span>,smoothing_window_size):</span><br><span class="line">    scaler.fit(train_data[di:di+smoothing_window_size,:])</span><br><span class="line">    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化所有的数据</span></span><br><span class="line">scaler.fit(train_data[di+smoothing_window_size:,:])</span><br><span class="line">train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])</span><br></pre></td></tr></table></figure><p>将数据矩阵调整回 <code>[data_size]</code> 的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新调整测试集和训练集</span></span><br><span class="line">train_data = train_data.reshape(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将测试集标准化</span></span><br><span class="line">test_data = scaler.transform(test_data).reshape(<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p>为了产生一条更平滑的曲线，我们使用一种叫做指数加权平均的算法。</p><p><strong>注意</strong>：我们只使用训练集来训练换算器 <code>scaler</code>，否则在标准化测试集时将得到不准确的结果。</p><p><strong>注意</strong>：只允许对训练集做平滑处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 应用指数加权平均</span></span><br><span class="line"><span class="comment"># 现在数据将比之间更为平滑</span></span><br><span class="line">EMA = <span class="number">0.0</span></span><br><span class="line">gamma = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">for</span> ti <span class="keyword">in</span> range(<span class="number">11000</span>):</span><br><span class="line">  EMA = gamma*train_data[ti] + (<span class="number">1</span>-gamma)*EMA</span><br><span class="line">  train_data[ti] = EMA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用于可视化和调试</span></span><br><span class="line">all_mid_data = np.concatenate([train_data,test_data],axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h3><p>为了评估训练出来的模型，我们将计算其预测值与真实值的均方误差（MSE）。将每一个预测值与真实值误差的平方取均值，即为这个模型的均方误差。</p><h3 id="股价建模中的平均值"><a href="#股价建模中的平均值" class="headerlink" title="股价建模中的平均值"></a>股价建模中的平均值</h3><blockquote><p>取平均值在预测单步上效果不错，但对股市预测这种需要预测许多步的情形不适用。</p></blockquote><h3 id="使用-LSTM-预测未来股价走势"><a href="#使用-LSTM-预测未来股价走势" class="headerlink" title="使用 LSTM 预测未来股价走势"></a>使用 LSTM 预测未来股价走势</h3><p>长短期记忆网络模型是非常强大的基于时间序列的模型，它们能向后预测任意步。一个 LSTM 模块（或者一个 LSTM 单元）使用 5 个重要的参数来对长期和短期数据建模。</p><ul><li>单元状态（$c_{t}$）- 这代表了单元存储的短期和长期记忆；</li><li>隐藏状态（$h_{t}$）- 这是根据当前输入、以前的隐藏状态和当前单元输入计算的用于预测未来股价的输出状态信息 。此外，隐藏状态还决定着是否只使用单元状态中的记忆（短期、长期或两者都使用）来进行下一次预测；</li><li>输入门（$i_{t}$）- 从输入门流入到单元状态中的信息；</li><li>遗忘门（$f_{t}$）- 从当前输入和前一个单元状态流到当前单元状态的信息；</li><li>输出门（$o_{t}$）- 从当前单元状态流到隐藏状态的信息，这决定了 LSTM 接下来使用的记忆类型。</li></ul><p>下图展示了一个 LSTM 单元。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/stock111.png" alt="" style="zoom:50%;" /><p>其中计算的算式如下：</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/stock11111.png" alt="" style="zoom:50%;" /><h3 id="数据生成器"><a href="#数据生成器" class="headerlink" title="数据生成器"></a>数据生成器</h3><p>最简单的想法是将总量为 N 的数据集，平均分割成 N/b 个序列，每个序列包含 b 个数据点。然后我们假想若干个指针，它们指向每一个序列的第一个元素。然后我们就可以开始采样生成数据了。我们将当前段的指针指向的元素下标当作输入，并在其后面的 1~5 个元素中随机挑选一个作为正确的预测值，因为模型并不总是只预测紧靠当前时间点的后一个数据。这样可以有效避免过拟合。每一次取样之后，我们将指针的下标加一，并开始生成下一个数据点。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/stock1123.png" alt="" style="zoom:50%;" /><h3 id="定义超参数"><a href="#定义超参数" class="headerlink" title="定义超参数"></a>定义超参数</h3><p>在本节中，我们将定义若干个超参数。<code>D</code> 是输入的维数。因为你使用前一天的股价来预测后面的股价，所以 <code>D</code> 应当是 <code>1</code>。</p><p><code>num_unrollings</code> 表示单个步骤中考虑的连续时间点个数，越大越好。</p><p>然后是 <code>batch_size</code>。它是在单个时间点中考虑的数据样本数量。它越大越好，因为选取的样本数量越大，模型可以参考的数据也就更多。</p><p>最后是 <code>num_nodes</code> 决定了每个单元中包含了多少隐藏神经元。在本例中，网络中包含三层 LSTM。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">D = <span class="number">1</span> <span class="comment"># 数据的维度</span></span><br><span class="line">num_unrollings = <span class="number">50</span> <span class="comment"># 你想预测多远的结果</span></span><br><span class="line">batch_size = <span class="number">500</span> <span class="comment"># 一次批处理中包含的数据个数</span></span><br><span class="line">num_nodes = [<span class="number">200</span>,<span class="number">200</span>,<span class="number">150</span>] <span class="comment"># 使用的深层 LSTM 网络的每一层中的隐藏节点数</span></span><br><span class="line">n_layers = len(num_nodes) <span class="comment"># 层数</span></span><br><span class="line">dropout = <span class="number">0.2</span> <span class="comment"># dropout 概率</span></span><br><span class="line"></span><br><span class="line">tf.reset_default_graph() <span class="comment"># 如果你想要多次运行，这个语句至关重要</span></span><br></pre></td></tr></table></figure><h3 id="定义输入和输出"><a href="#定义输入和输出" class="headerlink" title="定义输入和输出"></a>定义输入和输出</h3><p>接下来定义用于输入训练数据和标签的 placeholder。因为每个 placeholder 中只包含一批一维数据，所以这并不难。对于每一个优化步骤，我们需要 <code>num_unrollings</code> 个 placeholder。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入数据</span></span><br><span class="line">train_inputs, train_outputs = [],[]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据时间顺序展开输入，为每个时间点定义一个 placeholder</span></span><br><span class="line"><span class="keyword">for</span> ui <span class="keyword">in</span> range(num_unrollings):</span><br><span class="line">    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name=<span class="string">'train_inputs_%d'</span>%ui))</span><br><span class="line">    train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,<span class="number">1</span>], name = <span class="string">'train_outputs_%d'</span>%ui))</span><br></pre></td></tr></table></figure><h3 id="定义-LSTM-和回归层的参数"><a href="#定义-LSTM-和回归层的参数" class="headerlink" title="定义 LSTM 和回归层的参数"></a>定义 LSTM 和回归层的参数</h3><p>您将有一个包含三层 LSTM 和一层线性回归层的神经网络，分别用 <code>w</code> 和 <code>b</code> 表示，它获取上一个长短期记忆单元的输出，并输出对下一个时间的预测。你可以使用 TensorFlow 中的 <code>MultiRNNCell</code> 来封装您创建的三个 <code>LSTMCell</code> 对象。此外，LSTM 单元上还可以加上 dropout 来提高性能并减少过拟合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">stm_cells = [</span><br><span class="line">    tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],</span><br><span class="line">                            state_is_tuple=<span class="literal">True</span>,</span><br><span class="line">                            initializer= tf.contrib.layers.xavier_initializer()</span><br><span class="line">                           )</span><br><span class="line"> <span class="keyword">for</span> li <span class="keyword">in</span> range(n_layers)]</span><br><span class="line"></span><br><span class="line">drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(</span><br><span class="line">    lstm, input_keep_prob=<span class="number">1.0</span>,output_keep_prob=<span class="number">1.0</span>-dropout, state_keep_prob=<span class="number">1.0</span>-dropout</span><br><span class="line">) <span class="keyword">for</span> lstm <span class="keyword">in</span> lstm_cells]</span><br><span class="line">drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)</span><br><span class="line">multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)</span><br><span class="line"></span><br><span class="line">w = tf.get_variable(<span class="string">'w'</span>,shape=[num_nodes[<span class="number">-1</span>], <span class="number">1</span>], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">b = tf.get_variable(<span class="string">'b'</span>,initializer=tf.random_uniform([<span class="number">1</span>],<span class="number">-0.1</span>,<span class="number">0.1</span>))</span><br></pre></td></tr></table></figure><h3 id="计算-LSTM-输出并将结果代入回归层进行预测"><a href="#计算-LSTM-输出并将结果代入回归层进行预测" class="headerlink" title="计算 LSTM 输出并将结果代入回归层进行预测"></a>计算 LSTM 输出并将结果代入回归层进行预测</h3><p>在本节中，首先创建 TensorFlow 张量 <code>c</code> 和 <code>h</code> 用来保存 LSTM 单元的单元状态和隐藏状态。然后将 <code>train_input</code> 转换为 <code>[num_unrollings, batch_size, D]</code> 的形状，这是计算 <code>tf.nn.dynamic_rnn</code> 函数的输出所必需的。然后用 <code>tf.nn.dynamic_rnn</code> 计算 LSTM 输出，并将输出转化为一系列 <code>num_unrolling</code> 张量来预测和真实股价之间的损失函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 LSTM 的单元状态 c 和隐藏状态 h</span></span><br><span class="line">c, h = [],[]</span><br><span class="line">initial_state = []</span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> range(n_layers):</span><br><span class="line">  c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=<span class="literal">False</span>))</span><br><span class="line">  h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=<span class="literal">False</span>))</span><br><span class="line">  initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为 dynamic_rnn 函数需要特定的输出格式，所以我们对张量进行一些变换</span></span><br><span class="line"><span class="comment"># 请访问 https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn 来了解更多</span></span><br><span class="line">all_inputs = tf.concat([tf.expand_dims(t,<span class="number">0</span>) <span class="keyword">for</span> t <span class="keyword">in</span> train_inputs],axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># all_outputs 张量的尺寸是 [seq_length, batch_size, num_nodes]</span></span><br><span class="line">all_lstm_outputs, state = tf.nn.dynamic_rnn(</span><br><span class="line">    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),</span><br><span class="line">    time_major = <span class="literal">True</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[<span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line">all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)</span><br><span class="line"></span><br><span class="line">split_outputs = tf.split(all_outputs,num_unrollings,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="损失函数的计算与优化"><a href="#损失函数的计算与优化" class="headerlink" title="损失函数的计算与优化"></a>损失函数的计算与优化</h3><p>然后计算损失函数。但是在计算它时有一个值得注意的点。对于每批预测和真实输出，计算均方误差。然后将这些均方损失加起来（而非平均值）。最后，定义用于优化神经网络的优化器。我推荐使用 Adam 这种最新的、性能良好的优化器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在计算损失函数时，你需要注意准确的计算方法</span></span><br><span class="line"><span class="comment"># 因为你要同时计算所有展开步骤的损失函数</span></span><br><span class="line"><span class="comment"># 因此，在展开时取每批数据的平均误差，并将它们相加得到最终损失函数</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Defining training Loss'</span>)</span><br><span class="line">loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([tf.assign(c[li], state[li][<span class="number">0</span>]) <span class="keyword">for</span> li <span class="keyword">in</span> range(n_layers)]+</span><br><span class="line">                             [tf.assign(h[li], state[li][<span class="number">1</span>]) <span class="keyword">for</span> li <span class="keyword">in</span> range(n_layers)]):</span><br><span class="line">  <span class="keyword">for</span> ui <span class="keyword">in</span> range(num_unrollings):</span><br><span class="line">    loss += tf.reduce_mean(<span class="number">0.5</span>*(split_outputs[ui]-train_outputs[ui])**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Learning rate decay operations'</span>)</span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">inc_gstep = tf.assign(global_step,global_step + <span class="number">1</span>)</span><br><span class="line">tf_learning_rate = tf.placeholder(shape=<span class="literal">None</span>,dtype=tf.float32)</span><br><span class="line">tf_min_learning_rate = tf.placeholder(shape=<span class="literal">None</span>,dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">learning_rate = tf.maximum(</span><br><span class="line">    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=<span class="number">1</span>, decay_rate=<span class="number">0.5</span>, staircase=<span class="literal">True</span>),</span><br><span class="line">    tf_min_learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">print(<span class="string">'TF Optimization operations'</span>)</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate)</span><br><span class="line">gradients, v = zip(*optimizer.compute_gradients(loss))</span><br><span class="line">gradients, _ = tf.clip_by_global_norm(gradients, <span class="number">5.0</span>)</span><br><span class="line">optimizer = optimizer.apply_gradients(</span><br><span class="line">    zip(gradients, v))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\tAll done'</span>)</span><br></pre></td></tr></table></figure><p>这里定义与预测相关的 TensorFlow 操作。首先，定义用于输入的占位符（<code>sample_input</code>）。然后像训练阶段那样，定义用于预测的状态变量（<code>sample_c</code> 和 <code>sample_h</code>）。再然后用 <code>tf.nn.dynamic_rnn</code> 函数计算预测值。最后通过线性回归层（<code>w</code> 和 <code>b</code>）发送输出。您还应该定义 <code>reset_sample_state</code> 操作用于重置单元格状态和隐藏状态。每次进行一系列预测时，都应该在开始时执行此操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Defining prediction related TF functions'</span>)</span><br><span class="line"></span><br><span class="line">sample_inputs = tf.placeholder(tf.float32, shape=[<span class="number">1</span>,D])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在预测阶段更新 LSTM 状态</span></span><br><span class="line">sample_c, sample_h, initial_sample_state = [],[],[]</span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> range(n_layers):</span><br><span class="line">  sample_c.append(tf.Variable(tf.zeros([<span class="number">1</span>, num_nodes[li]]), trainable=<span class="literal">False</span>))</span><br><span class="line">  sample_h.append(tf.Variable(tf.zeros([<span class="number">1</span>, num_nodes[li]]), trainable=<span class="literal">False</span>))</span><br><span class="line">  initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))</span><br><span class="line"></span><br><span class="line">reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([<span class="number">1</span>, num_nodes[li]])) <span class="keyword">for</span> li <span class="keyword">in</span> range(n_layers)],</span><br><span class="line">                               *[tf.assign(sample_h[li],tf.zeros([<span class="number">1</span>, num_nodes[li]])) <span class="keyword">for</span> li <span class="keyword">in</span> range(n_layers)])</span><br><span class="line"></span><br><span class="line">sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,<span class="number">0</span>),</span><br><span class="line">                                   initial_state=tuple(initial_sample_state),</span><br><span class="line">                                   time_major = <span class="literal">True</span>,</span><br><span class="line">                                   dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][<span class="number">0</span>]) <span class="keyword">for</span> li <span class="keyword">in</span> range(n_layers)]+</span><br><span class="line">                              [tf.assign(sample_h[li],sample_state[li][<span class="number">1</span>]) <span class="keyword">for</span> li <span class="keyword">in</span> range(n_layers)]):  </span><br><span class="line">  sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[<span class="number">1</span>,<span class="number">-1</span>]), w, b)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'\tAll done'</span>)</span><br></pre></td></tr></table></figure><h3 id="运行-LSTM"><a href="#运行-LSTM" class="headerlink" title="运行 LSTM"></a>运行 LSTM</h3><p>在这里，你将训练并预测股票价格在接下来一段时间内的变动趋势，并观察预测是否正确。按照以下步骤操作我的 Jupyter Notebook（我制作好后会发布在GitHub上）。</p><blockquote><p>★ 在时间序列上定义一系列起始点 <code>test_points_seq</code> 用于评估你的模型</p><p>★ 对于每一个时间点</p><p>★★ 对于全部的训练数据</p><p>★★★ 将 <code>num_unrollings</code> 展开</p><p>★★★ 使用展开的数据训练神经网络</p><p>★★ 计算训练的平均损失函数</p><p>★★ 对于测试集中的每一个起始点</p><p>★★★ 通过迭代测试点之前找到的 <code>num_unrollings</code> 中的数据点来更新 LSTM 状态</p><p>★★★ 连续预测接下来的 <code>n_predict_once</code> 步，然后将前一次的预测作为本次的输入</p><p>★★★ 计算预测值和真实股价之间的均方误差</p></blockquote><h3 id="将预测结果可视化"><a href="#将预测结果可视化" class="headerlink" title="将预测结果可视化"></a>将预测结果可视化</h3><p>你可以发现，模型的均方误差在显著地下降，这意味着模型确实学习到了有用的信息。你可以通过比较神经网络产生的均方误差以及对股价取标准平均的均方误差（0.004）来量化你的成果。显然，LSTM 优于标准平均，同时你也能明白股价的标准平均能较好地反映股价地变化。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/stocks231.png" alt="" style="zoom:50%;" /><p>尽管并不完美，LSTM 在大部分情况下都能正确预测接下来的股价。而且你只能预测到股票接下来是涨是跌，而非股价的确切值。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>但愿本教程能帮到你，写这篇教程也让我受益匪浅。在本教程中，我了解到建立能够正确预测股价走势的模型是非常困难的。首先我们探讨了预测股价的动机。接下来我们了解到如何去下载并处理数据。然后我们介绍了两种可以向后预测一步的平均技术，这两种方法在预测多步时并不管用。之后，我们讨论了如何使用 LSTM 对未来的多步进行预测。最后，结果可视化，并发现这个模型（尽管并不完美）能出色地预测股价走势。</p><p>下面是本教程中对几个要点：</p><ol><li>股票价格/走势预测是一项极其困难的任务。就我个人而言，我认为任何股票预测模型都不完全正确，因此它们不应该被盲目地依赖。模型并不总是正确的。</li><li>不要相信那些声称预测曲线与真实股价完全重合的文章。那些取平均的方法在实践中并不管用。更明智的做法是预测股价走势。</li><li>模型的超参数会显著影响训练结果。所以最好使用一些诸如 Grid search 和 Random search 的调参技巧，下面是一系列非常重要的超参数：<strong>优化器的学习速率、网络层数、每层中的隐藏节点个数、优化器（Adam 是最好用的）以及模型的种类（GRU / LSTM / 增加 peephole connection 的 LSTM）</strong>。</li><li>在本教程中，由于数据集太小，我们根据测试损失函数来降低学习速率，这本身是不对的，因为这间接地将有关测试集的信息泄露到训练过程中。一种更好的处理方法是使用一个独立的验证集（与测试集不同），并根据验证集的性能降低学习速率。</li></ol><img src="https://picreso.oss-cn-beijing.aliyuncs.com/stocks.jpg" alt="" style="zoom:80%;" />]]></content>
      
      
      <categories>
          
          <category> 深度学习探索之路 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> 预测股票 </tag>
            
            <tag> 深度学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>趣味AI：用循环神经网络创造音乐🎤</title>
      <link href="/lstm-music.html"/>
      <url>/lstm-music.html</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><code>神经网络正在被使用去提升我们生活的方方面面，同时也在往创造力方面不断提升。</code></p><ul><li>自然语言处理技术去写一本书</li><li>计算机视觉技术去创造一幅画</li><li>…….</li></ul><p>在这篇文章中我们将介绍如何通过LSTM，使用 Python 和 Keras 库去创作音乐。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/v2-41a322ac1c0cc52c07e726381b9165d5_1200x500.jpg" alt=""></p><hr><h2 id="先欣赏一下"><a href="#先欣赏一下" class="headerlink" title="先欣赏一下"></a>先欣赏一下</h2><blockquote><p><code>这就是AI做出来的音乐。</code></p><p>其实从结果来看，在乐理上有不少的错误</p><p>和我的神经网络训练的时间和规模有关，毕竟是昨天晚上跑一晚上出来的结果</p></blockquote><div style="height: 0;padding-bottom:65%;position: relative;"><br><iframe width="760" height="510" src="//player.bilibili.com/player.html?aid=540712438&bvid=BV1qi4y147LW&cid=192453618&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="" style="position: absolute;height: 105%;width: 100%;"> </iframe><br></div><hr><p>现在是见证奇迹的时刻，图中包含了一页通过 LSTM 神经网络创作的音乐乐谱。瞅一眼就能看到它的结构，这在第二页的第三行到最后一行尤为明显。</p><p>有音乐常识，能阅读乐谱的人呢可以看到在这一页里有一些奇怪的音符。这就是网络不能创作完美的旋律的结果。在我们目前的成果里将总会有一些错误的音符。如果想获得更好的结果我们得有更大的网络才行。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/ezgif.com-webp-to-jpg.jpg" alt=""></p><p>这个相对较浅的网络的结果仍然令人印象深刻，从示例音乐中可以听到。对于那些感兴趣的人来说，图中的乐谱代表了神经网络创作音乐迈出了一大步。</p><h2 id="为什么想起做这个？"><a href="#为什么想起做这个？" class="headerlink" title="为什么想起做这个？"></a>为什么想起做这个？</h2><p>其实总的代码量不大，大概一千行的样子。</p><p>心血来潮在昨天晚上就把这个给做出来了。</p><p>毕竟我也挺喜欢音乐的，所以就尝试了和人工智能的结合</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/codeflow.png" alt=""></p><h2 id="GitHub-仓库地址-https-github-com-Xunzhuo-LSTM-Music"><a href="#GitHub-仓库地址-https-github-com-Xunzhuo-LSTM-Music" class="headerlink" title="GitHub 仓库地址: https://github.com/Xunzhuo/LSTM-Music"></a>GitHub 仓库地址: <a href="https://github.com/Xunzhuo/LSTM-Music" target="_blank" rel="noopener">https://github.com/Xunzhuo/LSTM-Music</a></h2><blockquote><p>欢迎<code>star follow</code>~</p></blockquote><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/git23232.png" alt=""></p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在进入具体的实现之前必须先弄清一些专业术语。</p><h3 id="循环神经网络（RNN）"><a href="#循环神经网络（RNN）" class="headerlink" title="循环神经网络（RNN）"></a>循环神经网络（RNN）</h3><p>循环神经网络是一类让我们使用时序信息的人工神经网络。之所以称之为循环是因为他们对数据序列中的每一个元素都执行相同的函数。每次的结果依赖于之前的运算。传统的神经网络则与之相反，输出不依赖于之前的计算。</p><p>在这篇教程中，我们使用一个 长短期记忆（LSTM）神经网络。这类循环神经网络可以通过梯度下降法高效的学习。使用闸门机制，LSTM 可以识别和编码长期模式。LSTM 对于解决那些长期记忆信息的案例如创作音乐和文本特别有用。</p><h3 id="Music21"><a href="#Music21" class="headerlink" title="Music21"></a>Music21</h3><p><a href="http://web.mit.edu/music21/" target="_blank" rel="noopener">Music21</a> 是一个被使用在计算机辅助音乐学的 Python 工具包。它使我们可以去教授音乐的基本原理，创作音乐范例并且学音乐。这个工具包提供了一个简单的接口去获得 MIDI 文件中的音乐谱号。除此之外，我们还能使用它去创作音符与和弦来轻松制作属于自己的 MIDI 文件。</p><p>在这篇教程中我们将使用 Music21 来提取我们数据集的内容，获取神经网络的输出，再将之转换成音符。</p><h3 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h3><p><a href="https://keras.io/" target="_blank" rel="noopener">Keras</a> 是一个 high-level 神经网络接口，它简化了和 <a href="https://www.tensorflow.org/" target="_blank" rel="noopener">Tensorflow</a> 的交互。它的开发重点是实现快速实验。</p><p>在本教程中我们将使用 Keras 库去创建和训练 LSTM 模型。一旦这个模型被训练出来，我们将使用它去给我们的音乐创作音符。</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>在本节中我们将讲解如何为我们的模型收集数据，如何整理数据使它能够在 LSTM 模型中被使用，以及我们模型的结构是什么。</p><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>下面我们看到的是来自于一个被 Music21 读取后的 midi 文件的摘录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">&lt;music21.note.Note F&gt;</span><br><span class="line">&lt;music21.chord.Chord A2 E3&gt;</span><br><span class="line">&lt;music21.chord.Chord A2 E3&gt;</span><br><span class="line">&lt;music21.note.Note E&gt;</span><br><span class="line">&lt;music21.chord.Chord B-2 F3&gt;</span><br><span class="line">&lt;music21.note.Note F&gt;</span><br><span class="line">&lt;music21.note.Note G&gt;</span><br><span class="line">&lt;music21.note.Note D&gt;</span><br><span class="line">&lt;music21.chord.Chord B-2 F3&gt;</span><br><span class="line">&lt;music21.note.Note F&gt;</span><br><span class="line">&lt;music21.chord.Chord B-2 F3&gt;</span><br><span class="line">&lt;music21.note.Note E&gt;</span><br><span class="line">&lt;music21.chord.Chord B-2 F3&gt;</span><br><span class="line">&lt;music21.note.Note D&gt;</span><br><span class="line">&lt;music21.chord.Chord B-2 F3&gt;</span><br><span class="line">&lt;music21.note.Note E&gt;</span><br><span class="line">&lt;music21.chord.Chord A2 E3&gt;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这个数据被拆分成两种类型：<a href="http://web.mit.edu/music21/doc/moduleReference/moduleNote.html#note" target="_blank" rel="noopener">Note</a>（译者注：音符集）和 <a href="http://web.mit.edu/music21/doc/moduleReference/moduleChord.html" target="_blank" rel="noopener">Chord</a>（译者注：和弦集）。音符对象包括<strong>音高</strong>，<strong>音阶</strong>和音符的<strong>偏移量</strong></p><ul><li><strong>音高</strong>是指声音的频率，或者用 [A, B, C, D, E, F, G] 来表示它是高还是低。其中 A 是最高，G 是最低。</li><li><strong><a href="http://web.mst.edu/~kosbar/test/ff/fourier/notes_pitchnames.html" target="_blank" rel="noopener">音阶</a></strong> 是指你将选择在钢琴上使用哪些音高。</li><li><strong>偏移量</strong>是指音符在作品的位置。</li></ul><p>而和弦对象的本质是一个同时播放一组音符的容器。</p><p>现在我们可以看到要想精确创作音乐，我们的神经网络将必须有能力去预测哪个音符或和弦将被使用。这意味着我们的预测集将必须包含每一个我们训练集中遇到的的音符与和弦对象。在 Github 页面的训练集上，不同的音符与和弦的数量总计达 352 个。这似乎交给了网络许多种可能的预测去输出，但是一个 LSTM 网络可以轻松处理它。</p><p>接下来我得考虑把这些音符放到哪里了。正如大部分人听音乐时注意到的，音符的间隔通常不同。你可以听到一连串快速的音符，然后接下来又是一段空白，这时没有任何音符演奏。</p><p>接下来我们从另外一个被 Music21 读取过的 midi 文件里找一个摘录，这次我们仅仅在它后面添加了偏移量。这使我们可以看到每个音符与和弦之间的间隔。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">&lt;music21.note.Note B&gt; 72.0</span><br><span class="line">&lt;music21.chord.Chord E3 A3&gt; 72.0</span><br><span class="line">&lt;music21.note.Note A&gt; 72.5</span><br><span class="line">&lt;music21.chord.Chord E3 A3&gt; 72.5</span><br><span class="line">&lt;music21.note.Note E&gt; 73.0</span><br><span class="line">&lt;music21.chord.Chord E3 A3&gt; 73.0</span><br><span class="line">&lt;music21.chord.Chord E3 A3&gt; 73.5</span><br><span class="line">&lt;music21.note.Note E-&gt; 74.0</span><br><span class="line">&lt;music21.chord.Chord F3 A3&gt; 74.0</span><br><span class="line">&lt;music21.chord.Chord F3 A3&gt; 74.5</span><br><span class="line">&lt;music21.chord.Chord F3 A3&gt; 75.0</span><br><span class="line">&lt;music21.chord.Chord F3 A3&gt; 75.5</span><br><span class="line">&lt;music21.chord.Chord E3 A3&gt; 76.0</span><br><span class="line">&lt;music21.chord.Chord E3 A3&gt; 76.5</span><br><span class="line">&lt;music21.chord.Chord E3 A3&gt; 77.0</span><br><span class="line">&lt;music21.chord.Chord E3 A3&gt; 77.5</span><br><span class="line">&lt;music21.chord.Chord F3 A3&gt; 78.0</span><br><span class="line">&lt;music21.chord.Chord F3 A3&gt; 78.5</span><br><span class="line">&lt;music21.chord.Chord F3 A3&gt; 79.0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>如这段摘录里所示，midi 文件里大部分数据集的音符的间隔都是 0.5。因此，我们可以通过忽略不同输出的偏移量来简化数据和模型。这不会太剧烈的影响神经网络创作的音乐旋律。因此我们将忽视教程中的偏移量并且把我们的可能输出列表保持在 352。</p><h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><p>既然我们已经检查了数据并且决定了我们要使用音符与和弦作为网络输出与输出的特征，那么现在就要为网络准备数据了。</p><p>首先，我们把数据加载到一个数组中，就像下面的代码这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> music21 <span class="keyword">import</span> converter, instrument, note, chord</span><br><span class="line"></span><br><span class="line">notes = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> glob.glob(<span class="string">"midi_songs/*.mid"</span>):</span><br><span class="line">    midi = converter.parse(file)</span><br><span class="line">    notes_to_parse = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    parts = instrument.partitionByInstrument(midi)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> parts: <span class="comment"># 文件包含乐器</span></span><br><span class="line">        notes_to_parse = parts.parts[<span class="number">0</span>].recurse()</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># 文件有扁平结构的音符</span></span><br><span class="line">        notes_to_parse = midi.flat.notes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> element <span class="keyword">in</span> notes_to_parse:</span><br><span class="line">        <span class="keyword">if</span> isinstance(element, note.Note):</span><br><span class="line">            notes.append(str(element.pitch))</span><br><span class="line">        <span class="keyword">elif</span> isinstance(element, chord.Chord):</span><br><span class="line">            notes.append(<span class="string">'.'</span>.join(str(n) <span class="keyword">for</span> n <span class="keyword">in</span> element.normalOrder))</span><br></pre></td></tr></table></figure><p>使用 <code>converter.parse(file)</code> 函数，我们开始把每一个文件加载到一个 Music21 流对象中。使用这个流对象，我们在文件中得到一个包含所有的音符与和弦的列表。把数组符号贴到到每个音符对象的音高上，因为使用数组符号可以重新创造音符中最重要的部分。将每个和弦的 ID 编码成一个单独的字符串，每个音符用一个点分隔。这些代码使我们可以轻松的把由网络生成的输出解码为正确的音符与和弦。</p><p>既然我们已经把所有的音符与和弦放入一个序列表中，我们就可以创造一个序列，作为网络的输入。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/2332311.png" alt=""></p><p>图 1：当一个数据由分类数据转换成数值数据时，此数据被转换成了一个整数索引来表示某一类在一组不同值中的位置。例如，苹果是第一个明确的值，因此它被映射成 0。桔子在第二个因此被映射成 1，菠萝就是 3，等等。</p><p>首先，我们将写一个映射函数去把字符型分类数据映射成整型数值数据。这么做是因为神经网络处理整型数值数据（的性能）远比处理字符型分类数据好的多。图 1 就是一个把分类转换成数值的例子。</p><p>接下来，我们必须为网络及其输出分别创建输入序列。每一个输入序列对应的输出序列将是第一个音符或者和弦，它在音符列表的输入序列中，位于音符列表之后。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">sequence_length = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到所有的音高名称</span></span><br><span class="line">pitchnames = sorted(set(item <span class="keyword">for</span> item <span class="keyword">in</span> notes))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个音高到音符的映射字典</span></span><br><span class="line">note_to_int = dict((note, number) <span class="keyword">for</span> number, note <span class="keyword">in</span> enumerate(pitchnames))</span><br><span class="line"></span><br><span class="line">network_input = []</span><br><span class="line">network_output = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建输入序列和与之对应的输出</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(notes) - sequence_length, <span class="number">1</span>):</span><br><span class="line">    sequence_in = notes[i:i + sequence_length]</span><br><span class="line">    sequence_out = notes[i + sequence_length]</span><br><span class="line">    network_input.append([note_to_int[char] <span class="keyword">for</span> char <span class="keyword">in</span> sequence_in])</span><br><span class="line">    network_output.append(note_to_int[sequence_out])</span><br><span class="line"></span><br><span class="line">n_patterns = len(network_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 整理输入格式使之与 LSTM 兼容</span></span><br><span class="line">network_input = numpy.reshape(network_input, (n_patterns, sequence_length, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 归一化输入</span></span><br><span class="line">network_input = network_input / float(n_vocab)</span><br><span class="line"></span><br><span class="line">network_output = np_utils.to_categorical(network_output)</span><br></pre></td></tr></table></figure><p>在这段示例代码汇总，我们把每一个序列的长度都设为 100 个音符或者和弦。这意味着要想去在序列中去预测下一个音符，网络已经有 100 个音符来帮助预测了。我极其推荐使用不同长度的序列去训练网络然后观察这些不同长度的序列对由网络产生的音乐的影响。</p><p>为网络准备数据的最后一步是将输入归一化处理并且 one-hot 编码输出。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>最后我们来设计这个模型的架构。在模型中我们使用到了四种不同类型的层：</p><p><strong>LSTM 层</strong>是一个循环的神经网络层，它把一个序列作为输入然后返回另一个序列（返回序列的值为真）或者一个矩阵。</p><p><strong>Dropout 层</strong>是一个正则化规则，这其中包含了在训练期间每次更新时将输入单位的一小部分置于 0，以防止过拟合。它由和层一起使用的参数决定。</p><p><strong>Dense 层</strong>或 <strong>fully connected 层</strong>是一个完全连接神经网络的层，这里的每一个输入节点都连接着输出节点。</p><p><strong>The Activation 层</strong>决定使用神经网络中的哪个激活函数去计算输出节点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">    model.add(LSTM(</span><br><span class="line">        <span class="number">256</span>,</span><br><span class="line">        input_shape=(network_input.shape[<span class="number">1</span>], network_input.shape[<span class="number">2</span>]),</span><br><span class="line">        return_sequences=<span class="literal">True</span></span><br><span class="line">    ))</span><br><span class="line">    model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line">    model.add(LSTM(<span class="number">512</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line">    model.add(LSTM(<span class="number">256</span>))</span><br><span class="line">    model.add(Dense(<span class="number">256</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line">    model.add(Dense(n_vocab))</span><br><span class="line">    model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">    model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>)</span><br></pre></td></tr></table></figure><p>既然我们有关于不同层的一些信息，那就把它们加到神经网络的模型中。</p><p>对于每一个 LSTM，Dense 和 Activation 层，第一个参数是层里应该有多少节点。对于 Dropout 层，第一个参数是输入单元中应该在训练中被舍弃的输入单元的片段。</p><p>对于第一层我们必须提供一个唯一的，名字是 <em>input_shape</em> 的参数。这个参数决定了网络中将要训练的数据的格式。</p><p>最后一层应该始终包含和我们输出不同结果数量相同的节点。这确保网络的输出将直接映射到我们的类里。</p><p>在这里我们将使用一个简单的，包含三个 LSTM 层、三个 Dropout 层、两个 Dense 层和一个 activation 层的网络。我推荐调整网络的结构，观察你是否可以提高预测的质量。</p><p>为了计算每次迭代的损失，我们将使用 分类交叉熵，因为我们每次输出属于一个简单类并且我们有不止两个以上的类在为此工作。为了优化网络我们将使用 RMSprop 优化器。通常对于循环神经网络，使用它算是一个好的选择。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">filepath = <span class="string">"weights-improvement-&#123;epoch:02d&#125;-&#123;loss:.4f&#125;-bigger.hdf5"</span>    </span><br><span class="line"></span><br><span class="line">checkpoint = ModelCheckpoint(</span><br><span class="line">    filepath, monitor=<span class="string">'loss'</span>, </span><br><span class="line">    verbose=<span class="number">0</span>,        </span><br><span class="line">    save_best_only=<span class="literal">True</span>,        </span><br><span class="line">    mode=<span class="string">'min'</span></span><br><span class="line">)    </span><br><span class="line">callbacks_list = [checkpoint]     </span><br><span class="line"></span><br><span class="line">model.fit(network_input, network_output, epochs=<span class="number">200</span>, batch_size=<span class="number">64</span>, callbacks=callbacks_list)</span><br></pre></td></tr></table></figure><p>一旦我们决定了网络的结构，就应该开始训练了。使用 Kearas 里的 <code>model.fit()</code> 函数来训练网络。第一个参数是我们早前准备的输入序列表，而第二个参数是它们各自输出的列表。在本教程中我们将训练网络进行 200 次迭代，每一个批次都是通过包含了 60 个分支的网络增殖的。</p><p>为了确保我们可以在任何时间点停止训练而不会将之前的努力付之东流，我们将使用 model checkpionts（模型检查点）。它为我们提供了一种方法，把每次迭代之后的网络节点的权重保存到一个文件中。这使我们一旦对损失值满意了就可以停掉神经网络而不必担心失去权重值。否则我们必须一直等待直到网络完成所有的 200 次迭代次数才能把权重保存到文件中。</p><h2 id="创作音乐"><a href="#创作音乐" class="headerlink" title="创作音乐"></a>创作音乐</h2><p>既然我们已经完成了训练网络，是时候享受一下我们花了几个小时训练的网络了。</p><p>为了能用神经网络去创作音乐，你得把它恢复到原来的状态。简言之我们将再次使用训练部分中的代码，用之前的方式去准备数据和建立网络模型。这并不是重新训练网络，而是把之前网络中的权重加载到模型中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(</span><br><span class="line">    <span class="number">512</span>,</span><br><span class="line">    input_shape=(network_input.shape[<span class="number">1</span>], network_input.shape[<span class="number">2</span>]),</span><br><span class="line">    return_sequences=<span class="literal">True</span></span><br><span class="line">))</span><br><span class="line">model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line">model.add(LSTM(<span class="number">512</span>, return_sequences=<span class="literal">True</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line">model.add(LSTM(<span class="number">512</span>))</span><br><span class="line">model.add(Dense(<span class="number">256</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line">model.add(Dense(n_vocab))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'rmsprop'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给每一个音符赋予权重</span></span><br><span class="line">model.load_weights(<span class="string">'weights.hdf5'</span>)</span><br></pre></td></tr></table></figure><p>现在我们可以使用训练好的模型去开始创作音符了。</p><p>因为我们有一个完整的音符序列表，我们将在列表中选择任意一个索引作为起始点，这允许我们不需要做任何修改就能重新运行代码并且每次都能返回不同的结果。但是，如果希望控制起始点，只需用命令行参数替换随机函数即可。</p><p>这里我也需要写一个映射函数去编码网络的输出。这个函数将数值数据映射成分类数据（把整数变成音符）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">start = numpy.random.randint(<span class="number">0</span>, len(network_input)<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">int_to_note = dict((number, note) <span class="keyword">for</span> number, note <span class="keyword">in</span> enumerate(pitchnames))</span><br><span class="line"></span><br><span class="line">pattern = network_input[start]</span><br><span class="line">prediction_output = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成 500 个音符</span></span><br><span class="line"><span class="keyword">for</span> note_index <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    prediction_input = numpy.reshape(pattern, (<span class="number">1</span>, len(pattern), <span class="number">1</span>))</span><br><span class="line">    prediction_input = prediction_input / float(n_vocab)</span><br><span class="line"></span><br><span class="line">    prediction = model.predict(prediction_input, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    index = numpy.argmax(prediction)</span><br><span class="line">    result = int_to_note[index]</span><br><span class="line">    prediction_output.append(result)</span><br><span class="line"></span><br><span class="line">    pattern.append(index)</span><br><span class="line">    pattern = pattern[<span class="number">1</span>:len(pattern)]</span><br></pre></td></tr></table></figure><p>我们选择使用网络去创作 500 个音符是因为这大约是两分钟的音乐，而且给了网络充足的空间去创造旋律。想要制作任何一个音符我们都必须给网络提交一个序列。我们提交的第一个序列是开始位置的音符序列。对于我们用作输入的每个后续序列，我们将删除序列的第一个音符，并在序列末尾插入上一个迭代的输出，如图 2 所示。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/232323.png" alt=""></p><p>图 2：第一个输入列是 ABCDE。我们依靠网络从流里得到的输出是 F。对于下一次的迭代，我们把 A 从列表里移除，并把 F 追加进去。然后重复这步骤。</p><p>为了从网络的输出中确定出最准确的预测，我们抽取了值最大的索引。输出汇数组中，索引为 <em>X</em> 的列可能对应于下一个音符的 <em>X</em>。图三帮助解释这个。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/231232143.png" alt=""></p><p>图 3：我们看到在一个从网络到类的输出预测的映射。正如我们看到的，下一个值最可能是 D，因此我们选择 D 为最可能的音高集合。</p><p>之后我们把网络的所有输出搜集，放到一个单一数组中。</p><p>既然我们有了数组中所有的音符与和弦的编码，我们可以开始解码它们并且创造一个音符与和弦对象的数组。</p><p>首先必须确定我们解码后的输出是音符还是和弦。</p><p>如果模式是<strong>和弦</strong>，我们必须将音符串拆分成一组音符。然后我们循环遍历每个音符的字符串表示，并为每个音符创建一个音符对象。然后我们可以创建一个包含每个音符的和弦对象。</p><p>如果输出是一个<strong>音符</strong>，我们使用模式中包含的音高字符串表示创建一个音符对象。</p><p>在每次迭代的结尾我们增加 0.5 的偏移时间并且把音符/和弦对象追加到一个列表中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">offset = <span class="number">0</span></span><br><span class="line">output_notes = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于模型生成的值来创建音符与和弦</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pattern <span class="keyword">in</span> prediction_output:</span><br><span class="line">    <span class="comment"># 输出是和弦</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="string">'.'</span> <span class="keyword">in</span> pattern) <span class="keyword">or</span> pattern.isdigit():</span><br><span class="line">        notes_in_chord = pattern.split(<span class="string">'.'</span>)</span><br><span class="line">        notes = []</span><br><span class="line">        <span class="keyword">for</span> current_note <span class="keyword">in</span> notes_in_chord:</span><br><span class="line">            new_note = note.Note(int(current_note))</span><br><span class="line">            new_note.storedInstrument = instrument.Piano()</span><br><span class="line">            notes.append(new_note)</span><br><span class="line">        new_chord = chord.Chord(notes)</span><br><span class="line">        new_chord.offset = offset</span><br><span class="line">        output_notes.append(new_chord)</span><br><span class="line">    <span class="comment"># 输出是音符</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        new_note = note.Note(pattern)</span><br><span class="line">        new_note.offset = offset</span><br><span class="line">        new_note.storedInstrument = instrument.Piano()</span><br><span class="line">        output_notes.append(new_note)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 增加每次迭代的偏移量使音符不会堆叠</span></span><br><span class="line">    offset += <span class="number">0.5</span></span><br></pre></td></tr></table></figure><p>在用网络创造音符与和弦的列表之后，我们可以使用这个列表创造一个 Music21 流对象，它使用此列表作为一个参数。最后，为了创建包含网络生成的音乐的 MIDI 文件，我们使用 Music21 工具包中的 <em>write</em> 函数将流写入文件中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">midi_stream = stream.Stream(output_notes)</span><br><span class="line"></span><br><span class="line">midi_stream.write(<span class="string">'midi'</span>, fp=<span class="string">'test_output.mid'</span>)</span><br></pre></td></tr></table></figure><h2 id="未来的工作（Maybe？）"><a href="#未来的工作（Maybe？）" class="headerlink" title="未来的工作（Maybe？）"></a>未来的工作（Maybe？）</h2><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/v2-4a13d555c3d984b9806dca443b1c004c_720w.jpg" alt=""></p><blockquote><p>我用一个简单的 LSTM 网络和 352 个音高实现了这个非凡的成果。不过，有一些地方还有待提高。</p></blockquote><ol><li><p>目前实现的结果不支持音符的多种音长和音符间的偏移。我们要为添加为不同音长服务的音高和代表音符停顿时间的音调。</p></li><li><p>为了通过增加音调来获得满意的结果我们也必须增加 LSTM 网络的深度，这需要性能更高的计算机去完成。我自用的笔记本电脑大约需要两个小时去训练网络。</p></li><li><p>为乐章增加前奏和结尾。现在网络在两个乐章之间没有间隔，网络不知道一个章节的结尾和另一个的开始在哪里。这允许网络从前奏到结束地创作一个章节而不是像现在这样突然的结束创作。</p></li><li><p>增加一个方法去处理未知的音符。目前的情况是如果网络遇到一个它不认识的音符，它就会返回状态失败。解决这个方法的可能方案是去寻找一个和未知音符最相似的音符或者和弦。</p></li><li><p>为数据集增加更多的乐器（的音乐）。现在网络仅仅支持只有一种单一乐器的作品。如果可以扩展到一整个管弦乐队那将会是非常有趣的。</p></li></ol><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>在本教程中我们演示了如何创建一个 LSTM 神经网络去创作音乐。也许这个结果不尽如人意，但它们还是让人印象深刻。而且它向我们展示了，神经网络可以创作音乐并且可以被用来帮助人们创作更复杂的音乐作品。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/v2-005cc0232839bc566b01d2c8a5f223ca_720w.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习探索之路 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec之把美丽的音乐变向量🎤</title>
      <link href="/vec-music.html"/>
      <url>/vec-music.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>机器学习算法在视觉领域和自然语言处理领域已经带来了很大的改变。但是音乐呢？近几年，音乐信息检索领域一直在飞速发展。这篇文章写的是NLP的一些技术是如何移植到音乐领域的。探寻了一种使用流行的 NLP 技术 word2vec 来表示复调音乐的方法。让我们来探究一下这是如何做到的……</p></blockquote><h2 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h2><p>词嵌入模型使我们能够通过有意义的方式表示词汇，这样机器学习模型就可以更容易地处理它们。这些词嵌入模型让我们可以用包含语义的向量来表示词汇。Word2vec 是一个流行的词向量嵌入模型，由 Mikolov 等人于 2013 年开发，它能够以一种十分有效的方式创建语义向量空间。</p><p>Word2vec 的本质是一个简单的单层神经网络，它有两种构造方式：1）使用连续词袋模型（CBOW）；或 2）使用 skip-gram 结构。这两种结构都非常高效，并且可以相对快速地进行训练。在本研究中，我们使用的是 skip-gram 模型，因为 Mikolov 等人在 2013 年的工作中提到，这个方法对于较小的数据集更加高效。Skip-gram 结构使用当前词 w_t 作为输入（输入层），并尝试预测在窗口范围内与之前后相邻的词（输出层）：</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/vm090.png" alt=""></p><p>由于一些在网上流传的图片，人们对于 skip-gram 结构的样子存在一些疑惑。网络的输出层并不包含多个单词，而是由上下文窗口中的一个单词组成的。那么它如何才能表示整个上下文窗口呢？当训练网络时，我们实际会使用抽样对，它由输入单词和一个上下文窗口中的随机单词组成。</p><p>这种类型的网络的传统训练目标包含一个用 softmax 函数来计算 𝑝(𝑤_{𝑡+𝑖}|𝑤_𝑡) 的过程，而它的梯度计算代价是十分大的。幸运的是，诸如噪音对比估计（Gutmann 和 Hyvärine 于 2012 发表论文）和负采样（Mikolov 等人于 2013 年发表论文）等技术为此提供了一个解决方案。我们用负采样基本地定义一个新的目标：最大化真实单词的概率并最小化噪声样本的概率。一个简单的二元逻辑回归可以用来分类真实单词和噪声样本。</p><p>当 word2vec 模型训练好了，隐藏层上的权重基本上就可以表示习得的、多维的嵌入结果。</p><h2 id="用音乐作为单词？"><a href="#用音乐作为单词？" class="headerlink" title="用音乐作为单词？"></a>用音乐作为单词？</h2><p>音乐和语言是存在内在联系的。它们都由遵从一些语法规则的一系列有序事件组成。更重要的是，它们都会创造出预期。想象一下，如果我说：“我要去比萨店买一个……”。这句话就生成了一个明确的预期……比萨。现在想象我给你哼一段生日快乐的旋律，但是我在最后一个音符前停下了……所以就像一句话一样，旋律生成预期，这些预期可以通过脑电波测量到，比如大脑中的事件相关电位 N400（Besson 和 Schön 于 2002 年发表论文）。</p><p>考虑语到语言和单词的相似性，让我们看看流行的语言模型是否也可以用来对音乐做有意义的表达。为了将一个 midi 文件转换为“语言”，我们在音乐中定义“切片”（相当于语言中的单词）。我们数据库中的每个曲目都被分割成了等时长的、不重叠的、长度为一个节拍的切片。一个节拍的时长可以由 MIDI toolbox 得到，且在每个曲目中可以是不同的。对于每一个切片，我们都会记录一个包含所有音名的列表，也就是没有八度信息的音高。</p><p>下图展示了一个怎样从 Chopin’s Mazurka Op. 67 №4 的第一小节中确定切片的例子。这里一节拍的长度是四分音符。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/vmsssa2.png" style="zoom:50%;" /><h2 id="Word2vec-学习调性-——-音乐的语义分布假设"><a href="#Word2vec-学习调性-——-音乐的语义分布假设" class="headerlink" title="Word2vec 学习调性 —— 音乐的语义分布假设"></a>Word2vec 学习调性 —— 音乐的语义分布假设</h2><p>在语言模型中，语义分布假设是词向量嵌入背后的理论基础之一。它表述为“出现在同一上下文中的单词趋向于含有同样的语义”。翻译到向量空间，这意味着这些单词会在几何关系上彼此接近。让我们看看 word2vec 模型是否在音乐上也学习到了类似的表示。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Chuan 等人使用的 <a href="https://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet" target="_blank" rel="noopener">MIDI 数据集</a> 包含了 8 种不同音乐类型（从古典到金属）。在总共 130,000 个音乐作品中，基于类型标签，我们只选择了其中的 23,178 个。这些曲目包含了 4,076 个唯一的切片。</p><h3 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h3><p>模型的训练只使用了出现最多的 500 个切片（即单词），并使用一个伪造单词来替代所有其他的情况。当包含的单词含有更多的信息（出现次数）时，这个过程提高了模型的准确性。其他的超参数包括学习率（设为 0.1），skip 窗口大小（设为 4），训练步数（设为 1,000,000）和嵌入维度（设为 256）。</p><h3 id="和弦"><a href="#和弦" class="headerlink" title="和弦"></a>和弦</h3><p>为了评估音乐切片的语义是否被模型捕获，让我们来看看和弦。</p><p>在切片词库中，所有包括三和弦的切片都会被识别出来。然后用罗马数字标注这些切片的音级（就像我们在乐理中经常做的那样）。比如，在C调中，C和弦为 I，而G和弦表示为 V。之后我们会使用余弦距离来计算在嵌入中不同音级的和弦之间有多远。</p><p>在 <em>n</em> 维空间中，两个非零向量 A 和 B 的余弦距离 Ds(A, B) 计算如下：</p><p>D𝑐(A,B)=1-cos(𝜃)=1-D𝑠(A,B)</p><p>其中 𝜃 是 A 和 B 的夹角，Ds 是余弦相似度：</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/vmsss.png" style="zoom:50%;" /><p>从乐理视角看，和弦 I 和 V 之间的“音调距离”应该比和弦 I 和 III 之间的小。下图展示了C大三和弦与其他和弦之间的距离。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/vmqqqq.png" style="zoom:50%;" /><p>从三和弦 I 到 V、IV 和 vi 的距离相对比较小！这与他们在乐理中被认为的“音调接近”是一致的，同时也表示 word2vec 模型的确学习到了切片之间有意义的关系。</p><p><em>在 word2vec 空间下，和弦之间的余弦距离似乎反映出了乐理中和弦的功能作用！</em></p><h3 id="调"><a href="#调" class="headerlink" title="调"></a>调</h3><p>通过观察巴赫的《平均律钢琴曲集》（WTC）的 24 首前奏曲，其中包括了全部的 24 个调（大调和小调），我们可以研究新的嵌入空间是否捕获到了调的信息。</p><p>为了扩充数据集，每个曲子都被转换为其他每一种大调或小调（基于原调），这样每个曲子都会有 12 个版本。每个调的切片都会被映射到预先训练好的向量空间里，并使用 k-means 聚类，这样我们就能得到一些中心点，把它们作为新数据集中的曲子。通过把这些曲子变调，我们可以保证这些中心点之间的余弦距离只会受到一个元素的影响：调。</p><p>下图展示了不同调的中心点曲子之间的余弦距离结果。和预期的一样，差五度音程的调在音调上是接近的，它们被表示为对角线旁边较暗的区域。音调上较远的调（比如 F 和 F#）呈橙色，这验证了我们的假设，即 word2vec 空间反映了调之间的音调距离关系！</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/vm1212.png" style="zoom:50%;" /><h3 id="类推"><a href="#类推" class="headerlink" title="类推"></a>类推</h3><p>这张图片</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/word2vec_chart.jpg" style="zoom:50%;" />展示了 word2vec 的一个突出的特性，它可以在向量空间中找出类似于「国王 -&gt; 皇后」和「男人 -&gt; 女人」这样的转化关系（Mikolov 等人 于 2013 年发表论文）。这说明含义可以通过向量转化向前传递。那么对音乐来说是否也可行呢？</p><p>我们首先从多音切片中检测到一些和弦，并观察一对和弦向量，C大调到G大调（I-V）。可以发现，不同的 I-V 向量对之间的夹角都非常相似（如右图所示），甚至可以被想成一个多维的五度圈。这再一次证明了类推的概念可能也存在于音乐 word2vec 空间上，尽管要想发现更明确的例子还需要做更多的调查研究。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/vm.png" alt="vm" style="zoom:40%;" /><h3 id="其它应用-——-音乐生成？"><a href="#其它应用-——-音乐生成？" class="headerlink" title="其它应用 —— 音乐生成？"></a>其它应用 —— 音乐生成？</h3><p>Chuan 等人于 2018 年简要地研究了如何使用该模型替换音乐切片以形成新的音乐。他们表示这只是一个初步的实验，但是该系统可以作为一个表示方法而用于更复杂的系统，例如 LSTM。在论文中可以找到更多相关细节，但下图可以让你对其结果有一个初步的了解。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/vm12.png" style="zoom:40%;" /><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>Chuan、Agres 和 Herremans 于 2018 年创建了一种 word2vec 模型，这种模型可以捕捉到复调音乐的音调属性，而无需将实际的音符输入模型。文章给出了一些令人信服的证据，说明和弦与调的信息可以在新的嵌入中找到，所以可以这样回答标题中的问题：是的，我们能够使用 word2vec 表示复调音乐！现在，将这个表示方法嵌入到其他能够捕捉到音乐的时间信息的模型，这条道路也已经打开了。</p>]]></content>
      
      
      <categories>
          
          <category> NLPer炼丹之旅 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> word2vec </tag>
            
            <tag> MIR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在哪里读人工智能的论文📚？</title>
      <link href="/papers.html"/>
      <url>/papers.html</url>
      
        <content type="html"><![CDATA[<div align="center"><h1 id="一个读人工智能的论文网站：Paperswithcode"><a href="#一个读人工智能的论文网站：Paperswithcode" class="headerlink" title="一个读人工智能的论文网站：Paperswithcode"></a>一个读人工智能的论文网站：<a href="https://paperswithcode.com/sota" target="_blank" rel="noopener">Paperswithcode</a></h1><h2 id="类别丰富，涵盖了各个人工智能的方向"><a href="#类别丰富，涵盖了各个人工智能的方向" class="headerlink" title="类别丰富，涵盖了各个人工智能的方向"></a>类别丰富，涵盖了各个人工智能的方向</h2><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/perasdasd.png" alt=""></p><h2 id="跟进最新最热门的学术论文"><a href="#跟进最新最热门的学术论文" class="headerlink" title="跟进最新最热门的学术论文"></a>跟进最新最热门的学术论文</h2><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/perpaer.png" alt=""></p></div>]]></content>
      
      
      <categories>
          
          <category> NLPer炼丹之旅 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分享 </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>One-Hot编码原理解释👨‍🎓</title>
      <link href="/one-hot.html"/>
      <url>/one-hot.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/One-Hot_Encoding_print.png" alt=""></p><h2 id="在机器学习中为什么要进行-One-Hot-编码？"><a href="#在机器学习中为什么要进行-One-Hot-编码？" class="headerlink" title="在机器学习中为什么要进行 One-Hot 编码？"></a>在机器学习中为什么要进行 One-Hot 编码？</h2><p>入门机器学习应用，尤其是需要对实际数据进行处理时，是很困难的。</p><p>一般来说，机器学习教程会推荐你或要求你，在开始拟合模型之前，先以特定的方式准备好数据。</p><p>其中，一个简单的例子就是对类别数据（Categorical data）进行 One-Hot 编码（又称独热编码）。</p><ul><li>为什么 One-Hot 编码是必要的？</li><li>为什么你不能直接使用数据来拟合模型？</li></ul><p>在本文中，你将得到上述重要问题的答案，并能更好地理解机器学习应用中的数据准备工作。</p><h2 id="什么是类别数据？"><a href="#什么是类别数据？" class="headerlink" title="什么是类别数据？"></a>什么是类别数据？</h2><p>类别数据是一种只有标签值而没有数值的变量。</p><p>它的值通常属于一个大小固定且有限的集合。</p><p>类别变量也常被称为 标称值（nominal）</p><p>下面举例说明：</p><ul><li>宠物（pet）变量包含以下几种值：狗（dog）、猫（cat）。</li><li>颜色（color）变量包含以下几种值：红（red）、绿（green）、蓝（blue）。</li><li>位次（place）变量包含以下几种值：第一（first）、第二（second）和第三（third）。</li></ul><p>以上例子中的每个值都代表着一个不同的类别。</p><p>有些类别彼此间存在一定的自然关系，比如自然的排序关系。</p><p>上述例子中，位次（place）变量的值就有这种自然的排序关系。这种变量被称为序数变量（ordinal variable）。</p><h2 id="类别数据有什么问题？"><a href="#类别数据有什么问题？" class="headerlink" title="类别数据有什么问题？"></a>类别数据有什么问题？</h2><p>有些算法可以直接应用于类别数据。</p><p>比如，你可以不进行任何数据转换，将决策树算法直接应用于类别数据上（取决于具体实现方式）。</p><p>但还有许多机器学习算法并不能直接操作标签数据。这些算法要求所有的输入输出变量都是数值（numeric）。</p><p>通常来说，这种限制主要是因为这些机器学习算法的高效实现造成的，而不是算法本身的限制。</p><p>但这也意味着我们需要把类别数据转换成数值形式。如果输出变量是类别变量，那你可能还得将模型的预测值转换回类别形式，以便在一些应用中展示或使用它们。</p><h2 id="如何将类别数据转换成数值数据？"><a href="#如何将类别数据转换成数值数据？" class="headerlink" title="如何将类别数据转换成数值数据？"></a>如何将类别数据转换成数值数据？</h2><p>这包含两个步骤：</p><ol><li>整数编码</li><li>One-Hot 编码</li></ol><h3 id="1-整数编码"><a href="#1-整数编码" class="headerlink" title="1. 整数编码"></a>1. 整数编码</h3><p>第一步，先要给每个类别值都分配一个整数值。</p><p>比如，用 1 表示红色（red），2 表示绿色（green），3 表示蓝色（blue）。</p><p>这种方式被称为标签编码或者整数编码，可以很轻松地将它还原回类别值。</p><p>对于某些变量来说，这种编码就足够了。</p><p>整数之间存在自然的排序关系，机器学习算法也许可以理解并利用这种关系。</p><p>比如，前面的位次（place）例子中的序数变量就是一个很好的例子。对于它我们只需要进行标签编码就够了。</p><h3 id="2-One-Hot-编码"><a href="#2-One-Hot-编码" class="headerlink" title="2. One-Hot 编码"></a>2. One-Hot 编码</h3><p>但对于不存在次序关系的类别变量，仅使用上述的整数编码是不够的。</p><p>实际上，使用整数编码会让模型假设类别间存在自然的次序关系，从而导致结果不佳或得到意外的结果（预测值落在两个类别的中间）。</p><p>这种情况下，就要对整数表示使用 One-Hot 编码了。One-Hot 编码会去除整数编码，并为每个整数值都创建一个二值变量。</p><p>在颜色（color）的示例中，有 3 种类别，因此需要 3 个二值变量进行编码。对应的颜色位置上将被标为“1”，其它颜色位置上会被标为“0”。</p><p>比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">red, green, blue</span><br><span class="line">1, 0, 0</span><br><span class="line">0, 1, 0</span><br><span class="line">0, 0, 1</span><br></pre></td></tr></table></figure><p>在统计学等领域中，这种二值变量通常被称为“虚拟变量”或“哑变量”（dummy variable）。</p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本文中，你应该了解了为什么在使用机器学习算法时通常要对类别数据进行编码。</p><p>特别要注意：</p><ul><li>类别数据的定义是由一组有限集合中的值构成的变量。</li><li>大多数机器学习算法都需要输入数值变量，并会输出数值变量。</li><li>通过整数编码与 One-Hot 编码可以将类别数据转换为整型数据。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习之旅 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> one-hot模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手实现Markov-Chain💪</title>
      <link href="/markov-python.html"/>
      <url>/markov-python.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/sdsadsaaaa.jpg" alt=""></p><h2 id="学习马尔可夫链及其性质，了解转移矩阵，并用-Python-动手实现！"><a href="#学习马尔可夫链及其性质，了解转移矩阵，并用-Python-动手实现！" class="headerlink" title="学习马尔可夫链及其性质，了解转移矩阵，并用 Python 动手实现！"></a>学习马尔可夫链及其性质，了解转移矩阵，并用 Python 动手实现！</h2><p>马尔可夫链是通常用一组随机变量定义的数学系统，可以根据具体的概率规则进行状态转移。转移的集合满足<strong>马尔可夫性质</strong>，也就是说，转移到任一特定状态的概率只取决于当前状态和所用时间，而与其之前的状态序列无关。马尔可夫链的这个独特性质就是<strong>无记忆性</strong>。</p><p>跟随本教程学会使用马尔可夫链，你就会懂得离散时间马尔可夫链是什么。你还会学习构建（离散时间）马尔可夫链模型所需的组件及其常见特性。接着学习用 Python 及其 <code>numpy</code> 和 <code>random</code> 库来实现一个简单的模型。还要学习用多种方式来表示马尔可夫链，比如状态图和转移矩阵</p><p>开始吧……</p><h2 id="为什么要用马尔可夫链？"><a href="#为什么要用马尔可夫链？" class="headerlink" title="为什么要用马尔可夫链？"></a>为什么要用马尔可夫链？</h2><p>马尔可夫链在数学中有广泛使用。同时也在经济学，博弈论，通信原理，遗传学和金融学领域有广泛应用。通常出现在统计学，尤其是贝叶斯统计，和信息论上下文中。在现实中，马尔可夫链为研究机动车辆的巡航定速系统，抵达机场的乘客的排队序列，货币汇率等问题提供了解决思路。最早由 Google 搜索引擎提出的 PageRank 就是基于马尔可夫过程的算法。Reddit 有个叫子版块模拟器的子版块，帖子和评论全部用马尔可夫链自动生成生成，厉害吧！</p><h2 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h2><p>马尔可夫链是具有马尔可夫性质的随机过程。随机过程或者说具有随机性质是指由一组随机变量定义的数学对象。马尔可夫链要么有离散状态空间（一组随机变量的可能值的集合）要么有离散索引集合（通常表示时间），鉴于此，马尔可夫链有众多变种。而通常所说的「马尔可夫链」是指具有离散时间集合的过程，也就是离散时间马尔可夫链（DTMC）。</p><h2 id="离散时间马尔可夫链"><a href="#离散时间马尔可夫链" class="headerlink" title="离散时间马尔可夫链"></a>离散时间马尔可夫链</h2><p>离散时间马尔可夫链所包含的系统的每一步都处于某个状态，步骤之间的状态随机变化。这些步骤常被比作时间的各个瞬间（不过你也可以想成物理距离或者随便什么离散度量）。离散时间马尔可夫链是随机变量 X1，X2，X3 … 的序列，不过要满足马尔可夫性质，所以转移到下一概率只和现在的状态有关，与之前的状态无关。用概率数学公式表示如下：</p><p>Pr( Xn+1 = x | X1 = x1, X2 = x2, …, Xn = xn) = Pr( Xn+1 = x | Xn = xn)</p><p>可见 Xn+1 的概率只和之前的 Xn 的概率有关。所以只需要知道上一个状态就可以确定现在状态的概率分布，满足条件独立（也就是说：只需要知道现在状态就可以确定下一个状态）。</p><p>Xi 的可能取值构成的可数集合 S 称为马尔可夫链<strong>状态空间</strong>。状态空间可以是任何东西：字母，数字，篮球比分或者天气情况。虽说时间参数通常是离散的，离散时间马尔可夫链的状态空间却没有什么广泛采用的约束条件，还不如参考任意状态空间下的过程。不过许多马尔可夫链的应用都用到了统计分析更简单的有限或可数无穷状态空间。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>马尔可夫链用概率自动机表示（相信我它没有听上去那么复杂！）。系统状态的改变叫做转移。各个状态改变的概率叫做转移概率。概率自动机包括从已知转移到转移方程的概率，将其转换为转移矩阵。</p><p>还可以将马尔可夫链看作有向图，其中图 n 的边标注的是 n 时刻状态转移到 n+1 时刻状态的概率，Pr(Xn+1 = x | Xn = xn)。这个式子可以读做，从已知状态 Xn 转移到状态 Xn+1 的概率。这个概念也可以用从时刻 n 到时刻 n+1 的<strong>转移矩阵</strong>来表示。状态空间的每个状态第一次出现是作为转移矩阵的行，第二次是列。矩阵的每个元素都表示从这一行表示的状态转移到列状态的概率。</p><p>如果马尔可夫链有 N 种状态，转移矩阵就是 N x N 维，其中（I, J）表示从状态 I 转移到状态 J 的概率。此外，转移矩阵一定是概率矩阵，也就是每一行元素之和一定是 1。为什么？因为每一行表示自身的概率分布。</p><p>所以模型的主要特征包括：状态空间，描述了特定转移发生的概率的转移矩阵以及由初始分布给出的状态空间的初始状态。</p><p>好像很复杂？</p><p>我们来看一个简单的例子帮助理解这些概念：</p><p>如果 Cj 难得心情不好，她会跑步，或者大吃特吃冰淇淋（译者注：原文 gooble 应为 gobble），要么打个盹儿来调整。</p><p>根据以往数据，如果她睡了一觉调整心情，第二天她有 60% 的可能去跑步，20% 的可能继续待在床上，还有 20% 的可能吃一大份冰淇淋。</p><p>如果她跑步散心，第二天她有 60% 的可能接着跑步，30% 的可能吃冰淇淋，只有 10% 的可能会去睡觉。</p><p>最后，如果她难过时纵情冰淇淋，第二天只有 10% 的可能性继续吃冰淇淋，有 70% 的可能性跑步，还有 20% 的可能性睡觉。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/markov1.png" style="zoom:50%;" /><p>上面由状态图表示的马尔可夫链有 3 个可能状态：睡觉，跑步和冰淇淋。所以转移矩阵是 3 x 3 矩阵。注意，离开某一状态的箭头的值的和一定是 1，这跟状态矩阵每一行元素之和是 1 一样，都表示概率的分布。转移矩阵中每个元素的含义跟状态图的每个状态类似。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/markov2.png" style="zoom:50%;" /><p>这个例子应该会帮助你理解与马尔可夫链有关的几个不同概念。不过在现实世界中如何应用这一理论呢？</p><p>借助这个例子，你应该能够回答这种问题：「从睡觉状态开始，2 天后 Cj 最后选择跑步（跑步状态）的概率是多少？」</p><p>我们一起算一下。要从睡觉状态转移到跑步状态，Cj 有如下选择：第一天继续睡觉，第二天跑步（0.2 ⋅ 0.6）；第一天换成跑步，第二天继续跑步（0.6 ⋅ 0.6）；第一天去吃冰淇淋，第二天换成跑步（0.2 ⋅ 0.7）。算下来概率是：((0.2 ⋅ 0.6) + (0.6 ⋅ 0.6) + (0.2 ⋅ 0.7)) = 0.62。所以说，从睡觉状态开始，2天后 Cj 处于跑步状态的概率是 62%。</p><p>希望这个例子可以告诉你马尔可夫链网络都可以解决哪些问题。</p><p>同时，还可以更好地理解马尔可夫链的几个重要性质：</p><ul><li>互通性：如果一个马尔可夫链可以从任何状态转移至任何状态，那么它就是不可还原的。换句话说，如果任两个状态之间存在一系列步骤的概率为正，就是不可还原的。</li><li>周期性：如果马尔可夫链只有在大于 1 的某个整数的倍数时返回某状态，那么马尔可夫链的状态是周期性的。因此，从状态「i」开始，只有经过整数倍个周期「k」才能回到「i」，k 是所有满足条件的整数的最大值。如果 k = 1 状态「i」不是周期性的，如果 k &gt; 1，「i」才是周期性的。</li><li>瞬态性和常返性：如果从状态「i」开始，有可能无法回到状态「i」，那么状态「i」有瞬态性。否则具有常返性（或者说持续性）。如果某状态可以在有限步内重现，该状态具有常返性，否则没有常返性。</li><li>遍历性：状态「i」如果满足非周期性和正重现性，它就有遍历性。如果不具有可还原性的马尔可夫链的每个状态都有遍历性，那么这个马尔可夫链也具有遍历性。</li><li>吸收态：如果无法从状态「i」转移到其他状态，「i」处于吸收态。因此，如果 当 i ≠ j 时，pii = 1 且 pij = 0，状态「i」处于吸收态。如果马尔可夫链的每个状态都可以达到吸收态，称其为具有吸收态的马尔可夫链。</li></ul><h2 id="用-Python-实现马尔可夫链"><a href="#用-Python-实现马尔可夫链" class="headerlink" title="用 Python 实现马尔可夫链"></a>用 Python 实现马尔可夫链</h2><p>我们用 Python 来实现一下上面这个例子。当然实际使用的库实现的马尔可夫链的效率会高得多，这里还是给出实例代码帮助你入门……</p><p>先 import 用到的库。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rm</span><br></pre></td></tr></table></figure><p>然后定义状态及其概率，也就是转移矩阵。要记得，因为有三个状态，矩阵是 3 X 3 维的。此外还要定义转移路径，也可以用矩阵表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 状态空间</span></span><br><span class="line">states = [<span class="string">"Sleep"</span>,<span class="string">"Icecream"</span>,<span class="string">"Run"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可能的事件序列</span></span><br><span class="line">transitionName = [[<span class="string">"SS"</span>,<span class="string">"SR"</span>,<span class="string">"SI"</span>],[<span class="string">"RS"</span>,<span class="string">"RR"</span>,<span class="string">"RI"</span>],[<span class="string">"IS"</span>,<span class="string">"IR"</span>,<span class="string">"II"</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 概率矩阵（转移矩阵）</span></span><br><span class="line">transitionMatrix = [[<span class="number">0.2</span>,<span class="number">0.6</span>,<span class="number">0.2</span>],[<span class="number">0.1</span>,<span class="number">0.6</span>,<span class="number">0.3</span>],[<span class="number">0.2</span>,<span class="number">0.7</span>,<span class="number">0.1</span>]]</span><br></pre></td></tr></table></figure><p>别忘了，要保证概率之和是 1。另外在写代码时多打印一些错误信息没什么不好的！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> sum(transitionMatrix[<span class="number">0</span>])+sum(transitionMatrix[<span class="number">1</span>])+sum(transitionMatrix[<span class="number">1</span>]) != <span class="number">3</span>:</span><br><span class="line">    print(<span class="string">"Somewhere, something went wrong. Transition matrix, perhaps?"</span>)</span><br><span class="line"><span class="keyword">else</span>: print(<span class="string">"All is gonna be okay, you should move on!! ;)"</span>)</span><br><span class="line">All <span class="keyword">is</span> gonna be okay, you should move on!! ;)</span><br></pre></td></tr></table></figure><p>现在就要进入正题了。我们要用 <code>numpy.random.choice</code> 从可能的转移集合选出随机样本。代码中大部分参数的含义从参数名就能看出来，不过参数 <code>p</code> 可能比较费解。它是可选参数，可以传入样品集的概率分布，这里传入的是转移矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现了可以预测状态的马尔可夫模型的函数。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activity_forecast</span><span class="params">(days)</span>:</span></span><br><span class="line">    <span class="comment"># 选择初始状态</span></span><br><span class="line">    activityToday = <span class="string">"Sleep"</span></span><br><span class="line">    print(<span class="string">"Start state: "</span> + activityToday)</span><br><span class="line">    <span class="comment"># 应该记录选择的状态序列。这里现在只有初始状态。</span></span><br><span class="line">    activityList = [activityToday]</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 计算 activityList 的概率</span></span><br><span class="line">    prob = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i != days:</span><br><span class="line">        <span class="keyword">if</span> activityToday == <span class="string">"Sleep"</span>:</span><br><span class="line">            change = np.random.choice(transitionName[<span class="number">0</span>],replace=<span class="literal">True</span>,p=transitionMatrix[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> change == <span class="string">"SS"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.2</span></span><br><span class="line">                activityList.append(<span class="string">"Sleep"</span>)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">elif</span> change == <span class="string">"SR"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.6</span></span><br><span class="line">                activityToday = <span class="string">"Run"</span></span><br><span class="line">                activityList.append(<span class="string">"Run"</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prob = prob * <span class="number">0.2</span></span><br><span class="line">                activityToday = <span class="string">"Icecream"</span></span><br><span class="line">                activityList.append(<span class="string">"Icecream"</span>)</span><br><span class="line">        <span class="keyword">elif</span> activityToday == <span class="string">"Run"</span>:</span><br><span class="line">            change = np.random.choice(transitionName[<span class="number">1</span>],replace=<span class="literal">True</span>,p=transitionMatrix[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> change == <span class="string">"RR"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.5</span></span><br><span class="line">                activityList.append(<span class="string">"Run"</span>)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">elif</span> change == <span class="string">"RS"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.2</span></span><br><span class="line">                activityToday = <span class="string">"Sleep"</span></span><br><span class="line">                activityList.append(<span class="string">"Sleep"</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prob = prob * <span class="number">0.3</span></span><br><span class="line">                activityToday = <span class="string">"Icecream"</span></span><br><span class="line">                activityList.append(<span class="string">"Icecream"</span>)</span><br><span class="line">        <span class="keyword">elif</span> activityToday == <span class="string">"Icecream"</span>:</span><br><span class="line">            change = np.random.choice(transitionName[<span class="number">2</span>],replace=<span class="literal">True</span>,p=transitionMatrix[<span class="number">2</span>])</span><br><span class="line">            <span class="keyword">if</span> change == <span class="string">"II"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.1</span></span><br><span class="line">                activityList.append(<span class="string">"Icecream"</span>)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">elif</span> change == <span class="string">"IS"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.2</span></span><br><span class="line">                activityToday = <span class="string">"Sleep"</span></span><br><span class="line">                activityList.append(<span class="string">"Sleep"</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prob = prob * <span class="number">0.7</span></span><br><span class="line">                activityToday = <span class="string">"Run"</span></span><br><span class="line">                activityList.append(<span class="string">"Run"</span>)</span><br><span class="line">        i += <span class="number">1</span>  </span><br><span class="line">    print(<span class="string">"Possible states: "</span> + str(activityList))</span><br><span class="line">    print(<span class="string">"End state after "</span>+ str(days) + <span class="string">" days: "</span> + activityToday)</span><br><span class="line">    print(<span class="string">"Probability of the possible sequence of states: "</span> + str(prob))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测 2 天后的可能状态</span></span><br><span class="line">activity_forecast(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">Start state: Sleep</span><br><span class="line">Possible states: [<span class="string">'Sleep'</span>, <span class="string">'Sleep'</span>, <span class="string">'Run'</span>]</span><br><span class="line">End state after <span class="number">2</span> days: Run</span><br><span class="line">Probability of the possible sequence of states: <span class="number">0.12</span></span><br></pre></td></tr></table></figure><p>结果可以得到从睡觉状态开始的可能转移及其概率。进一步拓展这个函数，可以让它从睡觉状态开始，迭代上几百次，就能得到终止于特定状态的预期概率。下面改写一下 <code>activity_forecast</code> 函数，加一些循环……</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">activity_forecast</span><span class="params">(days)</span>:</span></span><br><span class="line">    <span class="comment"># 选择初始状态</span></span><br><span class="line">    activityToday = <span class="string">"Sleep"</span></span><br><span class="line">    activityList = [activityToday]</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    prob = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i != days:</span><br><span class="line">        <span class="keyword">if</span> activityToday == <span class="string">"Sleep"</span>:</span><br><span class="line">            change = np.random.choice(transitionName[<span class="number">0</span>],replace=<span class="literal">True</span>,p=transitionMatrix[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> change == <span class="string">"SS"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.2</span></span><br><span class="line">                activityList.append(<span class="string">"Sleep"</span>)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">elif</span> change == <span class="string">"SR"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.6</span></span><br><span class="line">                activityToday = <span class="string">"Run"</span></span><br><span class="line">                activityList.append(<span class="string">"Run"</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prob = prob * <span class="number">0.2</span></span><br><span class="line">                activityToday = <span class="string">"Icecream"</span></span><br><span class="line">                activityList.append(<span class="string">"Icecream"</span>)</span><br><span class="line">        <span class="keyword">elif</span> activityToday == <span class="string">"Run"</span>:</span><br><span class="line">            change = np.random.choice(transitionName[<span class="number">1</span>],replace=<span class="literal">True</span>,p=transitionMatrix[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> change == <span class="string">"RR"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.5</span></span><br><span class="line">                activityList.append(<span class="string">"Run"</span>)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">elif</span> change == <span class="string">"RS"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.2</span></span><br><span class="line">                activityToday = <span class="string">"Sleep"</span></span><br><span class="line">                activityList.append(<span class="string">"Sleep"</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prob = prob * <span class="number">0.3</span></span><br><span class="line">                activityToday = <span class="string">"Icecream"</span></span><br><span class="line">                activityList.append(<span class="string">"Icecream"</span>)</span><br><span class="line">        <span class="keyword">elif</span> activityToday == <span class="string">"Icecream"</span>:</span><br><span class="line">            change = np.random.choice(transitionName[<span class="number">2</span>],replace=<span class="literal">True</span>,p=transitionMatrix[<span class="number">2</span>])</span><br><span class="line">            <span class="keyword">if</span> change == <span class="string">"II"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.1</span></span><br><span class="line">                activityList.append(<span class="string">"Icecream"</span>)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">elif</span> change == <span class="string">"IS"</span>:</span><br><span class="line">                prob = prob * <span class="number">0.2</span></span><br><span class="line">                activityToday = <span class="string">"Sleep"</span></span><br><span class="line">                activityList.append(<span class="string">"Sleep"</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prob = prob * <span class="number">0.7</span></span><br><span class="line">                activityToday = <span class="string">"Run"</span></span><br><span class="line">                activityList.append(<span class="string">"Run"</span>)</span><br><span class="line">        i += <span class="number">1</span>    </span><br><span class="line">    <span class="keyword">return</span> activityList</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录每次的 activityList</span></span><br><span class="line">list_activity = []</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># `range` 从第一个参数开始数起，一直到第二个参数（不包含）</span></span><br><span class="line"><span class="keyword">for</span> iterations <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">10000</span>):</span><br><span class="line">        list_activity.append(activity_forecast(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看记录到的所有 `activityList`    </span></span><br><span class="line"><span class="comment">#print(list_activity)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历列表，得到所有最终状态是跑步的 activityList</span></span><br><span class="line"><span class="keyword">for</span> smaller_list <span class="keyword">in</span> list_activity:</span><br><span class="line">    <span class="keyword">if</span>(smaller_list[<span class="number">2</span>] == <span class="string">"Run"</span>):</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算从睡觉状态开始到跑步状态结束的概率</span></span><br><span class="line">percentage = (count/<span class="number">10000</span>) * <span class="number">100</span></span><br><span class="line">print(<span class="string">"The probability of starting at state:'Sleep' and ending at state:'Run'= "</span> + str(percentage) + <span class="string">"%"</span>)</span><br><span class="line"></span><br><span class="line">The probability of starting at state:<span class="string">'Sleep'</span> <span class="keyword">and</span> ending at state:<span class="string">'Run'</span>= <span class="number">62.419999999999995</span>%</span><br></pre></td></tr></table></figure><p>那么问题来了，计算得到的结果为何会趋于 62%？</p><p><strong>注意</strong> 这实际是「大数定律」在发挥作用。大数定律是概率论定律，用来说明在试验次数足够多时，可能性相同的事件发生的频率趋于一致。也就是说，随着试验次数的增加，实际比率会趋于理论或预测的概率。</p><h2 id="马尔可夫思维"><a href="#马尔可夫思维" class="headerlink" title="马尔可夫思维"></a>马尔可夫思维</h2><p>马尔可夫链教程就到此为止了。本文介绍了马尔可夫链及其性质。简单的马尔可夫链是开始学习 Python 数据科学的必经之路。</p>]]></content>
      
      
      <categories>
          
          <category> NLPer炼丹之旅 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov链 </tag>
            
            <tag> 概率图模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动手实现NeuralNetwork💪</title>
      <link href="/python-nn.html"/>
      <url>/python-nn.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>一个帮助初学者理解深度神经网络内部工作机制的指南</p></blockquote><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/dpnn.jpg" alt=""></p><p><strong>写作动机：</strong> 为了使我自己可以更好地理解深度学习，我决定在没有像 TensorFlow 这样的深度学习库的情况下，从零开始构建一个神经网络。我相信，理解神经网络的内部工作原理对任何有追求的数据科学家来说都很重要。</p><p>这篇文章包含了我所学到的东西，希望对你们也有用。</p><h2 id="什么是神经网络？"><a href="#什么是神经网络？" class="headerlink" title="什么是神经网络？"></a>什么是神经网络？</h2><p>大多数介绍神经网络的文章在描述它们时都会与大脑做类比。在不深入研究与大脑类似之处的情况下，我发现将神经网络简单地描述为给定输入映射到期望输出的数学函数更容易理解一些。</p><p>神经网络由以下几个部分组成：</p><ul><li>一个<strong>输入层</strong>，x</li><li>任意数量的<strong>隐含层</strong></li><li>一个<strong>输出层</strong>，<strong>ŷ</strong></li><li>层与层之间的一组<strong>权重</strong>和<strong>偏差</strong>，<strong>W</strong> 和 <strong>b</strong></li><li>每个隐含层中所包含的一个可选的<strong>激活函数</strong>，<strong><em>σ</em></strong>。在本教程中，我们将使用 Sigmoid 激活函数。</li></ul><p>下图展示了 2 层神经网络的架构（<strong>注：在计算神经网络中的层数时，输入层通常被排除在外</strong>）</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/nnp2.png" alt=""></p><p>2 层神经网络的架构</p><p>在 Python 中创建一个神经网络的类很简单。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        self.input      = x</span><br><span class="line">        self.weights1   = np.random.rand(self.input.shape[<span class="number">1</span>],<span class="number">4</span>) </span><br><span class="line">        self.weights2   = np.random.rand(<span class="number">4</span>,<span class="number">1</span>)                 </span><br><span class="line">        self.y          = y</span><br><span class="line">        self.output     = np.zeros(y.shape)</span><br></pre></td></tr></table></figure><p><strong>训练神经网络</strong></p><p>一个简单的 2 层神经网络的输出 <strong><em>ŷ\</em></strong> 如下：</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/nnp1.png" style="zoom:50%;" /><p>你可能注意到了，在上面的等式中，只有权重 <strong><em>W\</em></strong> 和偏差 <strong><em>b\</em></strong> 这两个变量会对输出 <strong><em>ŷ\</em></strong> 产生影响。</p><p>当然，合理的权重和偏差会决定预测的准确程度。将针对输入数据的权重和偏差进行微调的过程就是<strong>训练神经网络</strong>的过程。</p><p>训练过程的每次迭代包括以下步骤：</p><ul><li>计算预测输出的值 <strong><em>ŷ\</em></strong>，即<strong>前馈</strong></li><li>更新权重和偏差，即<strong>反向传播</strong></li></ul><p>下面的序列图展示了这个过程。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/nnp3.png" style="zoom:50%;" /><h3 id="前馈过程"><a href="#前馈过程" class="headerlink" title="前馈过程"></a>前馈过程</h3><p>正如我们在上面的序列图中看到的，前馈只是一个简单的计算过程，对于一个基本的 2 层神经网络，它的输出是：</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/nnp4.png" style="zoom:50%;" /><p>让我们在 Python 代码中添加一个前馈函数来实现这一点。注意，为了简单起见，我们假设偏差为 0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        self.input      = x</span><br><span class="line">        self.weights1   = np.random.rand(self.input.shape[<span class="number">1</span>],<span class="number">4</span>) </span><br><span class="line">        self.weights2   = np.random.rand(<span class="number">4</span>,<span class="number">1</span>)                 </span><br><span class="line">        self.y          = y</span><br><span class="line">        self.output     = np.zeros(self.y.shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.layer1 = sigmoid(np.dot(self.input, self.weights1))</span><br><span class="line">        self.output = sigmoid(np.dot(self.layer1, self.weights2))</span><br></pre></td></tr></table></figure><p>但是，我们仍然需要一种方法来评估预测的“精准程度”（即我们的预测有多好）？而<strong>损失函数</strong>能让我们做到这一点。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>可用的损失函数有很多，而我们对损失函数的选择应该由问题本身的性质决定。在本教程中，我们将使用简单的<strong>平方和误差</strong>作为我们的损失函数。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/nnp5.png" style="zoom:50%;" /><p>这就是说，平方和误差只是每个预测值与实际值之差的总和。我们将差值平方后再计算，以便我们评估误差的绝对值。</p><p><strong>训练的目标是找到能使损失函数最小化的一组最优的权值和偏差。</strong></p><h3 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h3><p>现在我们已经得出了预测的误差（损失），我们还需要找到一种方法将误差<strong>传播</strong>回来，并更新我们的权重和偏差。</p><p>为了得出调整权重和偏差的合适的量，我们需要计算<strong>损失函数对于权重和偏差的导数</strong>。</p><p>回忆一下微积分的知识，计算函数的导数就是计算函数的斜率。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/nnp2122e3.png" style="zoom:50%;" /><p>梯度下降算法</p><p>如果我们已经算出了导数，我们就可以简单地通过增大/减小导数来更新权重和偏差（参见上图）。这就是所谓的<strong>梯度下降</strong>。</p><p>然而，我们无法直接计算损失函数对于权重和偏差的导数，因为损失函数的等式中不包含权重和偏差。 因此，我们需要<strong>链式法则</strong>来帮助我们进行计算。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/nnp223232.png" style="zoom:50%;" /><p>为了更新权重使用链式法则求解函数的导数。注意，为了简单起见，我们只展示了假设为 1 层的神经网络的偏导数。</p><p>哦！这真难看，但它让我们得到了我们需要的东西 —— 损失函数对于权重的导数（斜率），这样我们就可以相应地调整权重。</p><p>现在我们知道要怎么做了，让我们向 Pyhton 代码中添加反向传播函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        self.input      = x</span><br><span class="line">        self.weights1   = np.random.rand(self.input.shape[<span class="number">1</span>],<span class="number">4</span>) </span><br><span class="line">        self.weights2   = np.random.rand(<span class="number">4</span>,<span class="number">1</span>)                 </span><br><span class="line">        self.y          = y</span><br><span class="line">        self.output     = np.zeros(self.y.shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.layer1 = sigmoid(np.dot(self.input, self.weights1))</span><br><span class="line">        self.output = sigmoid(np.dot(self.layer1, self.weights2))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 应用链式法则求出损失函数对于 weights2 和 weights1 的导数</span></span><br><span class="line">        d_weights2 = np.dot(self.layer1.T, (<span class="number">2</span>*(self.y - self.output) * sigmoid_derivative(self.output)))</span><br><span class="line">        d_weights1 = np.dot(self.input.T,  (np.dot(<span class="number">2</span>*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用损失函数的导数(斜率)更新权重</span></span><br><span class="line">        self.weights1 += d_weights1</span><br><span class="line">        self.weights2 += d_weights2</span><br></pre></td></tr></table></figure><p>如果你需要更深入地理解微积分和链式法则在反向传播中的应用，我强烈推荐 3Blue1Brown 的教程。</p><h2 id="融会贯通"><a href="#融会贯通" class="headerlink" title="融会贯通"></a>融会贯通</h2><p>现在我们已经有了前馈和反向传播的完整 Python 代码，让我们将神经网络应用到一个示例中，看看效果如何。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/nnp232323232323.png" style="zoom:50%;" /><p>我们的神经网络应该通过学习得出一组理想的权重来表示这个函数。请注意，仅仅是求解权重的过程对我们来说也并不简单。</p><p>让我们对神经网络进行 1500 次训练迭代，看看会发生什么。观察下图中每次迭代的损失变化，我们可以清楚地看到损失<strong>单调递减至最小值</strong>。这与我们前面讨论的梯度下降算法是一致的。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/nnp2sdsd.png" style="zoom:50%;" /><p>让我们看一下经过 1500 次迭代后神经网络的最终预测（输出）。</p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/sdsdads22.png" alt="" style="zoom:50%;" /><p>1500 次训练迭代后的预测结果</p><p>我们成功了！我们的前馈和反向传播算法成功地训练了神经网络，预测结果收敛于真实值。</p><p>请注意，预测值和实际值之间会存在细微的偏差。我们需要这种偏差，因为它可以防止<strong>过拟合</strong>，并允许神经网络更好地<strong>推广</strong>至不可见数据中。</p><h2 id="后续的学习任务"><a href="#后续的学习任务" class="headerlink" title="后续的学习任务"></a>后续的学习任务</h2><p>幸运的是，我们的学习旅程还未结束。关于神经网络和深度学习，我们还有<strong>很多</strong>内容需要学习。例如：</p><ul><li>除了 Sigmoid 函数，我们还可以使用哪些<strong>激活函数</strong>？</li><li>在训练神经网络时使用<strong>学习率</strong></li><li>使用<strong>卷积</strong>进行图像分类任务</li></ul><p>我将会就这些主题编写更多内容，请在 Medium 上关注我并留意更新！</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>当然，我也在从零开始编写我自己的神经网络的过程中学到了很多。</p><p>虽然像 TensorFlow 和 Keras 这样的深度学习库使得构建深度神经网络变得很简单，即使你不完全理解神经网络内部工作原理也没关系，但是我发现对于有追求的数据科学家来说，深入理解神经网络是很有好处的。</p><p>这个练习花费了我大量的时间，我希望它对你们也有帮助！</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习探索之路 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN成长记(一)：CHAR-RNN🔥</title>
      <link href="/rnn-series1.html"/>
      <url>/rnn-series1.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/RNN-trip.png" alt=""></p><p><strong>提示：</strong>关于 RNN 的内容将横跨好几篇文章，包括基本的 RNN 结构、支持字符级序列生成的纯 TensorFlow 实现等等。而关于 RNN 的后续文章会包含更多高级主题，比如更加复杂的用于机器翻译任务的 Attention 机制等。</p><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>使用循环结构拥有很多优势，最突出的一个优势就是它们能够在内存中存储前一个输入的表示。如此，我们就可以更好的预测后续的输出内容。持续追踪内存中的长数据流会出现很多的问题，比如 BPTT 算法中出现的梯度消失（gradient vanishing）问题就是其中之一。幸运的是，我们可以对架构做出一些改进来解决这个问题。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d352d35342d31332d616d2e706e673f773d363230.png" alt=""></p><h2 id="二、CHAR-RNN"><a href="#二、CHAR-RNN" class="headerlink" title="二、CHAR-RNN"></a>二、CHAR-RNN</h2><p>我们不会去专门实现一个纯 TensorFlow 版本的单字符生成模型。相反，现在这个模型的目标是从每个输入句子中以每次一个字母的方式来读取字符流，并预测下一个字母是什么。在训练期间，我们将句子中的字母提供给网络，并用于生成输出的字母。而在推断（生成）期间，我们则会将上一次的输出作为新的输入（使用随机的 token 作为第一个输入）。</p><p>对于文本数据来说，我们做了一些预处理，请查看这个 GitHub 仓库来获取更多信息。</p><p><strong>输入样例</strong>：Hello there Charlie, how are you? Today I want a nice day. All I want for myself is a car.</p><ul><li>DATA_SIZE：输入的长度，即 <code>len(input)</code>；</li><li>BATCH_SIZE：每批的序列个数；</li><li>NUM_STEPS：每个切片的 token 数，即序列的长度 <code>seq_len</code>；</li><li>STATE_SIZE：每个隐层状态的隐层节点数，即值 <code>H</code>；</li><li>num_batches：数据集小批量化后的批量数</li></ul><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d362d31352d35372d616d2e706e673f773d363230.png" alt=""></p><p><strong>注意：</strong>由于我们是一行一行的将数据输入进 RNN 单元的，因此我们需要一列一列的将数据组成张量输入到网络中去，即我们必须把原始数据进行 reshape 处理。此外，每个字母都将作为一个被嵌入的独热编码（one-hot encoding，译注：又称 1-of-k encoding）的向量输入。在上图中，每个句子数据都被完美的切分进了一组组小批量数据，这只不过是为了达到更好的可视化目的，这样你就可以看到输入是怎样被切分的了。在实际的 -RNN 实现中，我们并不关心一个具体的句子，我们只是将整个输入切分成 num_batches 个批次，每个批次彼此独立，所以每个输入的长度都是 <code>num_steps</code>，即 <code>seq_len</code>。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d362d33302d31372d616d2e706e673f773d363230.png" alt=""></p><h2 id="三、反向传播"><a href="#三、反向传播" class="headerlink" title="三、反向传播"></a>三、反向传播</h2><p>RNN 版本的反向传播 BPTT 刚开始可能有点混乱，尤其是计算隐藏状态对输入的影响之时。下面的代码使用原生 numpy 实现，符合下图中我的公式推导逻辑。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d362d33332d33372d616d2e706e673f773d363230.png" alt=""></p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d362d33352d32392d616d312e706e673f773d363230.png" alt=""></p><p><strong>前向传播：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> xrange(len(inputs)):</span><br><span class="line">    xs[t] = np.zeros((vocab_size,<span class="number">1</span>)) <span class="comment"># 独热编码</span></span><br><span class="line">    xs[t][inputs[t]] = <span class="number">1</span></span><br><span class="line">    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t<span class="number">-1</span>]) + bh) <span class="comment"># 隐藏状态</span></span><br><span class="line">    ys[t] = np.dot(Why, hs[t]) + by <span class="comment"># 下一个字符的未归一化对数似然概率</span></span><br><span class="line">    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) <span class="comment"># 下一个字符的概率</span></span><br><span class="line">    loss += -np.log(ps[t][targets[t],<span class="number">0</span>]) <span class="comment"># softmax（交叉熵损失）</span></span><br></pre></td></tr></table></figure><p><strong>反向传播:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(len(inputs))):</span><br><span class="line">    dy = np.copy(ps[t])</span><br><span class="line">    dy[targets[t]] -= <span class="number">1</span></span><br><span class="line">    dWhy += np.dot(dy, hs[t].T)</span><br><span class="line">    dby += dy</span><br><span class="line">    dh = np.dot(Why.T, dy) + dhnext  <span class="comment"># 反向传播给 h</span></span><br><span class="line">    dhraw = (<span class="number">1</span> - hs[t] * hs[t]) * dh <span class="comment"># 通过 tanh 的非线性进行反向传播</span></span><br><span class="line">    dbh += dhraw</span><br><span class="line">    dWxh += np.dot(dhraw, xs[t].T)</span><br><span class="line">    dWhh += np.dot(dhraw, hs[t<span class="number">-1</span>].T)</span><br><span class="line">    dhnext = np.dot(Whh.T, dhraw)</span><br></pre></td></tr></table></figure><h2 id="张量的形状"><a href="#张量的形状" class="headerlink" title="张量的形状"></a>张量的形状</h2><p>在实现之前，我们来谈谈张量的形状。在这个 CHAR-RNN 的例子上讲述张量形状这个概念有点奇怪，因此我会向你解释如何对其进行批量化以及它们是怎样完成 seq2seq 任务的。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d33312d61742d382d34352d30372d706d2e706e673f773d363230.png" alt=""></p><p>这个任务对于一次性输入整行（全部 <code>batch_size</code> 个序列的）<code>seq_len</code> 这点上有点奇怪。通常来说，我们一次只传递一个批量，每个批量都有 <code>batch_size</code> 个序列，所以形状为<code>(batch_size, seq_len)</code>。我们通常也不会用 <code>seq_len</code> 来做分割，而是取整个序列的长度。对于 seq2seq 任务而言，本系列的第 2、3 和 5 篇文章中看到，我们会将大小为 <code>batch_size</code> 一个批量的序列作为输入，其中每个序列的长度为<code>seq_len</code> 。我们不能像在图中那样指定 <code>seq_len</code>，因为实际的<code>seq_len</code> 会根据全部样本的特点填充到最大值。我们会在比最大长度短的所有句子后填充一些填充符，从而达到最大值。不过现在还不是深入讨论这个问题的时候。</p><h2 id="四、Char-RNN-的-TensorFlow-实现（无-RNN-抽象）"><a href="#四、Char-RNN-的-TensorFlow-实现（无-RNN-抽象）" class="headerlink" title="四、Char-RNN 的 TensorFlow 实现（无 RNN 抽象）"></a>四、Char-RNN 的 TensorFlow 实现（无 RNN 抽象）</h2><p>我们将使用没有 RNN 类抽象的纯 TensorFlow 进行实现。同时还将使用我们自己的权重集来真正理解输入数据的流向以及输出是如何生成的。在这里我们只讨论代码里一些重点部分，而完整的代码我将给出相关链接。如果你想要使用 TF RNN 类进行实现，请转到本文第五小节。</p><p><strong>重点：</strong></p><p>首先我想讨论下如何生成批量化的数据。你可能注意到了，我们有一个额外的步骤，那就是将数据进行批量化处理，然后再将数据分割进 <code>seq_len</code>。这么做的原因是为了消除 RNN 结构中 BPTT 算法中产生的梯度消失问题。本质上来说，我们并不能同时处理多个字符。这是因为在反向传播中，如果序列太长，梯度就会下降得很快。因此，一个简单的技巧是保存一个 <code>seq_len</code> 长度的输出状态，然后将其作为下一个 <code>seq_len</code> 的 <code>initial_state</code>。这种由我们自行选择（使用 BPTT 来）处理的个数和更新频率的做法，就是所谓的截断反向传播（truncated backpropagation）。<code>initial_state</code> 从 0 开始，并在每轮计算中进行重置。因此，我们仍然能在一个特定的批次中从之前的 <code>seq_len</code> 序列里保存表示的某些类型。这么做的原因在于，在字符这种级别上，一个极小的序列并不能够学习到足够多的表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(FLAGS, raw_data)</span>:</span></span><br><span class="line">    raw_X, raw_y = raw_data</span><br><span class="line">    data_length = len(raw_X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从原始数据中创建批量数据</span></span><br><span class="line">    num_batches = FLAGS.DATA_SIZE // FLAGS.BATCH_SIZE <span class="comment"># 每批的 token</span></span><br><span class="line">    data_X = np.zeros([num_batches, FLAGS.BATCH_SIZE], dtype=np.int32)</span><br><span class="line">    data_y = np.zeros([num_batches, FLAGS.BATCH_SIZE], dtype=np.int32)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches):</span><br><span class="line">        data_X[i, :] = raw_X[FLAGS.BATCH_SIZE * i: FLAGS.BATCH_SIZE * (i+<span class="number">1</span>)]</span><br><span class="line">        data_y[i, :] = raw_y[FLAGS.BATCH_SIZE * i: FLAGS.BATCH_SIZE * (i+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 尽管每个批次都有很多的 token</span></span><br><span class="line">    <span class="comment"># 但我们每次只想输入 seq_len 个 token</span></span><br><span class="line">    feed_size = FLAGS.BATCH_SIZE // FLAGS.SEQ_LEN</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(feed_size):</span><br><span class="line">        X = data_X[:, i * FLAGS.SEQ_LEN:(i+<span class="number">1</span>) * FLAGS.SEQ_LEN]</span><br><span class="line">        y = data_y[:, i * FLAGS.SEQ_LEN:(i+<span class="number">1</span>) * FLAGS.SEQ_LEN]</span><br><span class="line">        <span class="keyword">yield</span> (X, y)</span><br></pre></td></tr></table></figure><p>下面是使用我们自己的权重的代码。<code>rnn_cell</code> 函数用来接收来自前一个单元的输入和状态，从而生成 RNN 的输出，同时也是下一个单元的输入状态。下一个函数 <code>rnn_logits</code> 使用权重将我们的 RNN 输出进行转换，从而通过 softmax 生成 logits 概率并用于分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">(FLAGS, rnn_input, state)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_cell'</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        W_input = tf.get_variable(<span class="string">'W_input'</span>,</span><br><span class="line">            [FLAGS.NUM_CLASSES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">        W_hidden = tf.get_variable(<span class="string">'W_hidden'</span>,</span><br><span class="line">            [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">        b_hidden = tf.get_variable(<span class="string">'b_hidden'</span>, [FLAGS.NUM_HIDDEN_UNITS],</span><br><span class="line">            initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    <span class="keyword">return</span> tf.tanh(tf.matmul(rnn_input, W_input) +</span><br><span class="line">                   tf.matmul(state, W_hidden) + b_hidden)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_logits</span><span class="params">(FLAGS, rnn_output)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax'</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        W_softmax = tf.get_variable(<span class="string">'W_softmax'</span>,</span><br><span class="line">            [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_CLASSES])</span><br><span class="line">        b_softmax = tf.get_variable(<span class="string">'b_softmax'</span>,</span><br><span class="line">            [FLAGS.NUM_CLASSES], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    <span class="keyword">return</span> tf.matmul(rnn_output, W_softmax) + b_softmax</span><br></pre></td></tr></table></figure><p>我们将输入和独热编码在 RNN 的批处理中进行 reshape 操作。然后，我们就可以使用 <code>rnn_cell</code>、<code>rnn_logits</code> 和 softmax 来运行我们的 RNN 从而预测下一个 token 了。你可以看到，我们生成的状态与我们在这个简单实现中的 RNN 输出是一致的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, FLAGS)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 占位符</span></span><br><span class="line">        self.X = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="literal">None</span>],</span><br><span class="line">            name=<span class="string">'input_placeholder'</span>)</span><br><span class="line">        self.y = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="literal">None</span>],</span><br><span class="line">            name=<span class="string">'labels_placeholder'</span>)</span><br><span class="line">        self.initial_state = tf.zeros([FLAGS.NUM_BATCHES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备输入</span></span><br><span class="line">        X_one_hot = tf.one_hot(self.X, FLAGS.NUM_CLASSES)</span><br><span class="line">        rnn_inputs = [tf.squeeze(i, squeeze_dims=[<span class="number">1</span>]) \</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> tf.split(<span class="number">1</span>, FLAGS.SEQ_LEN, X_one_hot)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义 RNN cell</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_cell'</span>):</span><br><span class="line">            W_input = tf.get_variable(<span class="string">'W_input'</span>,</span><br><span class="line">                [FLAGS.NUM_CLASSES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">            W_hidden = tf.get_variable(<span class="string">'W_hidden'</span>,</span><br><span class="line">                [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">            b_hidden = tf.get_variable(<span class="string">'b_hidden'</span>,</span><br><span class="line">                [FLAGS.NUM_HIDDEN_UNITS],</span><br><span class="line">                initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建 RNN</span></span><br><span class="line">        state = self.initial_state</span><br><span class="line">        rnn_outputs = []</span><br><span class="line">        <span class="keyword">for</span> rnn_input <span class="keyword">in</span> rnn_inputs:</span><br><span class="line">            state = rnn_cell(FLAGS, rnn_input, state)</span><br><span class="line">            rnn_outputs.append(state)</span><br><span class="line">        self.final_state = rnn_outputs[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Logits 概率及预测</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'softmax'</span>):</span><br><span class="line">            W_softmax = tf.get_variable(<span class="string">'W_softmax'</span>,</span><br><span class="line">                [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_CLASSES])</span><br><span class="line">            b_softmax = tf.get_variable(<span class="string">'b_softmax'</span>,</span><br><span class="line">                [FLAGS.NUM_CLASSES],</span><br><span class="line">                initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line"></span><br><span class="line">        logits = [rnn_logits(FLAGS, rnn_output) <span class="keyword">for</span> rnn_output <span class="keyword">in</span> rnn_outputs]</span><br><span class="line">        self.predictions = [tf.nn.softmax(logit) <span class="keyword">for</span> logit <span class="keyword">in</span> logits]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 损失与优化</span></span><br><span class="line">        y_as_list = [tf.squeeze(i, squeeze_dims=[<span class="number">1</span>]) \</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> tf.split(<span class="number">1</span>, FLAGS.SEQ_LEN, self.y)]</span><br><span class="line">        losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logit, label) \</span><br><span class="line">            <span class="keyword">for</span> logit, label <span class="keyword">in</span> zip(logits, y_as_list)]</span><br><span class="line">        self.total_loss = tf.reduce_mean(losses)</span><br><span class="line">        self.train_step = tf.train.AdagradOptimizer(</span><br><span class="line">            FLAGS.LEARNING_RATE).minimize(self.total_loss)</span><br></pre></td></tr></table></figure><p>我们偶尔也会从模型中进行采样。对于采样而言，可以选择使用 logits 概率中的最大值，或者在选择的类别中引入 <code>temperature</code> 参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, FLAGS, sampling_type=<span class="number">1</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    initial_state = tf.zeros([<span class="number">1</span>,FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">    predictions = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理预设 token</span></span><br><span class="line">    state = initial_state</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> FLAGS.START_TOKEN:</span><br><span class="line">        idx = FLAGS.char_to_idx[char]</span><br><span class="line">        idx_one_hot = tf.one_hot(idx, FLAGS.NUM_CLASSES)</span><br><span class="line">        rnn_input = tf.reshape(idx_one_hot, [<span class="number">1</span>, <span class="number">65</span>])</span><br><span class="line">        state =  rnn_cell(FLAGS, rnn_input, state)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在预设 token 后进行预测</span></span><br><span class="line">    logit = rnn_logits(FLAGS, state)</span><br><span class="line">    prediction = tf.argmax(tf.nn.softmax(logit), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    predictions.append(prediction.eval())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> token_num <span class="keyword">in</span> range(FLAGS.PREDICTION_LENGTH<span class="number">-1</span>):</span><br><span class="line">        idx_one_hot = tf.one_hot(prediction, FLAGS.NUM_CLASSES)</span><br><span class="line">        rnn_input = tf.reshape(idx_one_hot, [<span class="number">1</span>, <span class="number">65</span>])</span><br><span class="line">        state =  rnn_cell(FLAGS, rnn_input, state)</span><br><span class="line">        logit = rnn_logits(FLAGS, state)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对分布进行缩放</span></span><br><span class="line">        <span class="comment"># temperature 越高，产生的新词越多，也就越炫酷</span></span><br><span class="line">        <span class="comment"># 但同时也需要更多的样本</span></span><br><span class="line">        next_char_dist = logit/FLAGS.TEMPERATURE</span><br><span class="line">        next_char_dist = tf.exp(next_char_dist)</span><br><span class="line">        next_char_dist /= tf.reduce_sum(next_char_dist)</span><br><span class="line"></span><br><span class="line">        dist = next_char_dist.eval()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 单字符采样</span></span><br><span class="line">        <span class="keyword">if</span> sampling_type == <span class="number">0</span>:</span><br><span class="line">            prediction = tf.argmax(tf.nn.softmax(</span><br><span class="line">                                    next_char_dist), <span class="number">1</span>)[<span class="number">0</span>].eval()</span><br><span class="line">        <span class="keyword">elif</span> sampling_type == <span class="number">1</span>:</span><br><span class="line">            prediction = FLAGS.NUM_CLASSES - <span class="number">1</span></span><br><span class="line">            point = random.random()</span><br><span class="line">            weight = <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, FLAGS.NUM_CLASSES):</span><br><span class="line">                weight += dist[<span class="number">0</span>][index]</span><br><span class="line">                <span class="keyword">if</span> weight &gt;= point:</span><br><span class="line">                    prediction = index</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Pick a valid sampling_type!"</span>)</span><br><span class="line">        predictions.append(prediction)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><p>我们还需要看看如何向数据流中传递 <code>initial_state</code> 参数。为了避免梯度消失的出现，每次处理完一个序列后，它和 <code>final_state</code> 都会被更新。注意，我们将零初始状态作为起始状态，然后在将这个状态传递给随后的序列，并将前一个序列的 <code>final_state</code> 作为新的输入状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">state = np.zeros([FLAGS.NUM_BATCHES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, (input_X, input_y) <span class="keyword">in</span> enumerate(epoch):</span><br><span class="line">predictions, total_loss, state, _= model.step(sess, input_X,</span><br><span class="line"> input_y, state)</span><br><span class="line">training_losses.append(total_loss)</span><br></pre></td></tr></table></figure><h2 id="五、使用-TF-RNN-实现"><a href="#五、使用-TF-RNN-实现" class="headerlink" title="五、使用 TF RNN 实现"></a>五、使用 TF RNN 实现</h2><p>与上面不同的是，在下面这个实现中，我们将使用 TensorFlow 的 NN 工具来创建 RNN 抽象类。在使用这些类之前，理解这些类的输入内容、内部操作及输出结果是很重要的。由于我们仍然是使用基本的 <code>rnn_cell</code>，因此我们将使用截断误差反向传播，但是如果使用 GRU 或 LSTM，就没有必要了。其实，只需将整个数据分割成 <code>batch_size</code>，然后处理整个序列就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">(FLAGS)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取 cell 类型</span></span><br><span class="line">    <span class="keyword">if</span> FLAGS.MODEL == <span class="string">'rnn'</span>:</span><br><span class="line">        rnn_cell_type = tf.nn.rnn_cell.BasicRNNCell</span><br><span class="line">    <span class="keyword">elif</span> FLAGS.MODEL == <span class="string">'gru'</span>:</span><br><span class="line">        rnn_cell_type = tf.nn.rnn_cell.GRUCell</span><br><span class="line">    <span class="keyword">elif</span> FLAGS.MODEL == <span class="string">'lstm'</span>:</span><br><span class="line">        rnn_cell_type = tf.nn.rnn_cell.BasicLSTMCell</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"Choose a valid RNN unit type."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单一 cell</span></span><br><span class="line">    single_cell = rnn_cell_type(FLAGS.NUM_HIDDEN_UNITS)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Dropout</span></span><br><span class="line">    single_cell = tf.nn.rnn_cell.DropoutWrapper(single_cell,</span><br><span class="line">        output_keep_prob=<span class="number">1</span>-FLAGS.DROPOUT)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个状态作为单个 cell</span></span><br><span class="line">    stacked_cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * FLAGS.NUM_LAYERS)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> stacked_cell</span><br></pre></td></tr></table></figure><p>上面的代码创建的是我们特定的 RNN 结构。我们可以从许多不同的 RNN 单元类型中进行选择，但是在这里你可以看到三个最常见的类型（BasicRNN、GRU 和 LSTM）。我们用一定数量的隐藏单元来创建每个 RNN 单元。然后，我们可以在每个单元层之后之后添加一个 Dropout 层来进行正则化处理。最后，我们可以通过复制 <code>single_cell</code> 来实现堆叠的 RNN 结构。注意，<code>state_is_tuple=True</code> 条件被附加到了 <code>single_cell</code> 和 <code>stacked_cell</code> 里。这保证了在给定序列的每个输入之后返回一个包含状态的元组。如果使用 LSTM 单元，上述语句为真；否则无视。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_inputs</span><span class="params">(FLAGS, input_data)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_inputs'</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        W_input = tf.get_variable(<span class="string">"W_input"</span>,</span><br><span class="line">            [FLAGS.NUM_CLASSES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># &lt;BATCH_SIZE, seq_len, num_hidden_units&gt;</span></span><br><span class="line">    embeddings = tf.nn.embedding_lookup(W_input, input_data)</span><br><span class="line">    <span class="comment"># &lt;seq_len, BATCH_SIZE, num_hidden_units&gt;</span></span><br><span class="line">    <span class="comment"># BATCH_SIZE will be in columns bc we feed in row by row into RNN.</span></span><br><span class="line">    <span class="comment"># 1st row = 1st tokens from each batch</span></span><br><span class="line">    <span class="comment">#inputs = [tf.squeeze(i, [1]) for i in tf.split(1, FLAGS.SEQ_LEN, embeddings)]</span></span><br><span class="line">    <span class="comment"># NO NEED if using dynamic_rnn(time_major=False)</span></span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_softmax</span><span class="params">(FLAGS, outputs)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_softmax'</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        W_softmax = tf.get_variable(<span class="string">"W_softmax"</span>,</span><br><span class="line">            [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_CLASSES])</span><br><span class="line">        b_softmax = tf.get_variable(<span class="string">"b_softmax"</span>, [FLAGS.NUM_CLASSES])</span><br><span class="line"></span><br><span class="line">    logits = tf.matmul(outputs, W_softmax) + b_softmax</span><br><span class="line">    <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>这里的 <code>rnn_inputs</code> 函数与原生 TensorFlow 版本的实现由一些不同。正如你所看到的，我们不再需要 reshape 输入。这是因为 <code>tf.nn.dynamic_rnn</code> 会帮我们处理来自 RNN 的 output 和 state。这是一种效率非常高的 RNN 抽象，它还要求输入的数据不被预先 reshape，因此我们所有全部内容都是嵌入的。<code>rnn_softmax</code> 类提供的 logits 功能和前面所实现内容的完全一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, FLAGS)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="string">''' 数据占位符'''</span></span><br><span class="line">        self.input_data = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="literal">None</span>])</span><br><span class="line">        self.targets = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="literal">None</span>])</span><br><span class="line"></span><br><span class="line">        <span class="string">''' RNN 单元 '''</span></span><br><span class="line">        self.stacked_cell = rnn_cell(FLAGS)</span><br><span class="line">        self.initial_state = self.stacked_cell.zero_state(</span><br><span class="line">            FLAGS.NUM_BATCHES, tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="string">''' RNN 输入 '''</span></span><br><span class="line">        <span class="comment"># 嵌入权重 W_input)</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_inputs'</span>):</span><br><span class="line">            W_input = tf.get_variable(<span class="string">"W_input"</span>,</span><br><span class="line">                [FLAGS.NUM_CLASSES, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line">        inputs = rnn_inputs(FLAGS, self.input_data)</span><br><span class="line"></span><br><span class="line">        <span class="string">''' RNN 输出 '''</span></span><br><span class="line">        <span class="comment"># outputs: &lt;seq_len, BATCH_SIZE, num_hidden_units&gt;</span></span><br><span class="line">        <span class="comment"># state: &lt;BATCH_SIZE, num_layers*num_hidden_units&gt;</span></span><br><span class="line">        outputs, state = tf.nn.dynamic_rnn(cell=self.stacked_cell, inputs=inputs,</span><br><span class="line">                                           initial_state=self.initial_state)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># &lt;seq_len*BATCH_SIZE, num_hidden_units&gt;</span></span><br><span class="line">        outputs = tf.reshape(tf.concat(<span class="number">1</span>, outputs), [<span class="number">-1</span>, FLAGS.NUM_HIDDEN_UNITS])</span><br><span class="line"></span><br><span class="line">        <span class="string">''' 处理 RNN 输出 '''</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_softmax'</span>):</span><br><span class="line">            W_softmax = tf.get_variable(<span class="string">"W_softmax"</span>,</span><br><span class="line">                [FLAGS.NUM_HIDDEN_UNITS, FLAGS.NUM_CLASSES])</span><br><span class="line">            b_softmax = tf.get_variable(<span class="string">"b_softmax"</span>, [FLAGS.NUM_CLASSES])</span><br><span class="line">        <span class="comment"># Logit</span></span><br><span class="line">        self.logits = rnn_softmax(FLAGS, outputs)</span><br><span class="line">        self.probabilities = tf.nn.softmax(self.logits)</span><br><span class="line"></span><br><span class="line">        <span class="string">''' Loss '''</span></span><br><span class="line">        y_as_list = tf.reshape(self.targets, [<span class="number">-1</span>])</span><br><span class="line">        self.loss = tf.reduce_mean(</span><br><span class="line">            tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">                self.logits, y_as_list))</span><br><span class="line">        self.final_state = state</span><br><span class="line"></span><br><span class="line">        <span class="string">''' 优化 '''</span></span><br><span class="line">        self.lr = tf.Variable(<span class="number">0.0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">        trainable_vars = tf.trainable_variables()</span><br><span class="line">        <span class="comment"># 梯度截断防止梯度消失或梯度爆炸</span></span><br><span class="line">        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, trainable_vars),</span><br><span class="line">                                          FLAGS.GRAD_CLIP)</span><br><span class="line">        optimizer = tf.train.AdamOptimizer(self.lr)</span><br><span class="line">        self.train_optimizer = optimizer.apply_gradients(</span><br><span class="line">            zip(grads, trainable_vars))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存模型的组件</span></span><br><span class="line">        self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">        self.saver = tf.train.Saver(tf.all_variables())</span><br></pre></td></tr></table></figure><p>还要注意的是，我们不会手动的在嵌入之前对输入 token 进行独热编码，这是因为<code>rnn_inputs</code> 函数里的 <code>tf.nn.embedding_lookup</code> 会自动帮我们完成。</p><p>为了生成输出，我们使用了 <code>tf.nn.dynamic_rnn</code> ，其输出结果为每个输入的输出以及返回状态（即包含上一次每个输入批次的状态的元组）。最后，我们将输出进行了 reshape ，从而得到 logits 概率并用于与 targets 进行比较。</p><p>注意到 <code>self.initial_state</code> 由 <code>stacked_cell.zero_state</code> 初始化，我们只需要指定的 <code>batch_size</code> 就够了。对于这里的 <code>NUM_BATCHES</code> 请查看前面的张量形状一节中的说明。有一种替代方法可以不包含初始状态，<code>dynamic_rnn()</code> 会自行处理，我们所需要做的就是指定数据类型（即<code>dtype = tf.float32</code> 等)。可惜我们并不能这样做，因为我们要把序列的 <code>final_state</code> 作为了下一个序列的 <code>initial_state</code> 。你可能还会注意到，尽管 <code>self.initial_state</code> 不是占位符，我们还是把前一次 <code>final_state</code> 传给了新的 <code>initial_state</code>。当然，我们可以通过重新定义 <code>step()</code> 里的 <code>self.initial_state</code> 来输入自己的初始值。不管怎样，一旦用到 <code>input_feeds</code> ，我们就需要计算 <code>output_feed</code>，而如果没有用到，那么就会跳回使用重载之前的值（也就是 <code>stacked_cell.zero_state</code>）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, sess, batch_X, batch_y, initial_state=None)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> initial_state == <span class="literal">None</span>:</span><br><span class="line">        input_feed = &#123;self.input_data: batch_X,</span><br><span class="line">                      self.targets: batch_y&#125;</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_feed = &#123;self.input_data: batch_X,</span><br><span class="line">                      self.targets: batch_y,</span><br><span class="line">                      self.initial_state: initial_state&#125;</span><br><span class="line"></span><br><span class="line">    output_feed = [self.loss,</span><br><span class="line">                   self.final_state,</span><br><span class="line">                   self.logits,</span><br><span class="line">                   self.train_optimizer]</span><br><span class="line">    outputs = sess.run(output_feed, input_feed)</span><br><span class="line">    <span class="keyword">return</span> outputs[<span class="number">0</span>], outputs[<span class="number">1</span>], outputs[<span class="number">2</span>], outputs[<span class="number">3</span>]</span><br></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>我们来看看结果。这绝不是一种惊天动地的创造，但我确实是用了 <code>temperature</code> 而不是 <code>argmax</code> 进行生成。因此，我们可以看到生成结果里包含很多新奇的创意，但同时错误也很多（包括语法、拼写、排序等）。我只让网络训练了 10 轮，但已经开始看到单词和句子结构了，甚至还能看到每个角色的表演台词（数据集是莎士比亚的作品）。为了获得不错的结果，可以让它通宵在 GPU 上进行训练。</p><p>看到这，估计连莎士比亚都要给跪了。</p><p><strong>更新</strong>：我对典型输入、输出和状态张量的形状有很多疑问。</p><ul><li><strong>输入</strong>：[num_batches, seq_len, num_classes]</li><li><strong>输出</strong>：[num_batches, seq_len, num_hidden_units] （每个状态的全部输出）</li><li><strong>状态</strong>：[num_batches, num_hidden_units] （上一次状态的输出）</li></ul><p>在下一篇文章中，我们将处理变长序列，展示文本分类的具体实现。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习探索之路 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> CHAR-RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN成长记(二)：文本分类🔥</title>
      <link href="/rnn-series2.html"/>
      <url>/rnn-series2.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>在第一篇文章中，我们看到了如何使用 TensorFlow 实现一个简单的 RNN 架构。现在我们将使用这些组件并将其应用到文本分类中去。主要的区别在于，我们不会像 CHAR-RNN 模型那样输入固定长度的序列，而是使用长度不同的序列。</p></blockquote><h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><p>这个任务的数据集选用了来自 Cornell 大学的语句情绪极性数据集，它包含了 5331 个正面和负面情绪的句子。这是一个非常小的数据集，但足够用来演示如何使用循环神经网络进行文本分类了。</p><h2 id="预处理步骤"><a href="#预处理步骤" class="headerlink" title="预处理步骤"></a>预处理步骤</h2><ol><li>清洗句子并切分成一个个 token；</li><li>将句子转换为数值 token；</li><li>保存每个句子的序列长。</li></ol><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30352d61742d372d33322d33362d706d2e706e673f773d363230.png" alt=""></p><p>如上图所示，我们希望在计算完成时立即对句子的情绪做出预测。引入额外的填充符会带来过多噪声，这样的话你模型的性能就会不太好。<strong>注意</strong>：我们填充序列的唯一原因是因为需要以固定大小的批量输入进 RNN。下面你会看到，使用动态 RNN 还能避免在序列完成后的不必要计算。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">model</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, FLAGS)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 占位符</span></span><br><span class="line">        self.inputs_X = tf.placeholder(tf.int32,</span><br><span class="line">            shape=[<span class="literal">None</span>, <span class="literal">None</span>], name=<span class="string">'inputs_X'</span>)</span><br><span class="line">        self.targets_y = tf.placeholder(tf.float32,</span><br><span class="line">            shape=[<span class="literal">None</span>, <span class="literal">None</span>], name=<span class="string">'targets_y'</span>)</span><br><span class="line">        self.dropout = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># RNN 单元</span></span><br><span class="line">        stacked_cell = rnn_cell(FLAGS, self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># RNN 输入</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_inputs'</span>):</span><br><span class="line">            W_input = tf.get_variable(<span class="string">"W_input"</span>,</span><br><span class="line">                [FLAGS.en_vocab_size, FLAGS.num_hidden_units])</span><br><span class="line"></span><br><span class="line">        inputs = rnn_inputs(FLAGS, self.inputs_X)</span><br><span class="line">        <span class="comment">#initial_state = stacked_cell.zero_state(FLAGS.batch_size, tf.float32)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># RNN 输出</span></span><br><span class="line">        seq_lens = length(self.inputs_X)</span><br><span class="line">        all_outputs, state = tf.nn.dynamic_rnn(cell=stacked_cell, inputs=inputs,</span><br><span class="line">            sequence_length=seq_lens, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 由于使用了 seq_len[0]，state 自动包含了上一次的对应输出</span></span><br><span class="line">        <span class="comment"># 因为 state 是一个带有张量的元组</span></span><br><span class="line">        outputs = state[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 处理 RNN 输出</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'rnn_softmax'</span>):</span><br><span class="line">            W_softmax = tf.get_variable(<span class="string">"W_softmax"</span>,</span><br><span class="line">                [FLAGS.num_hidden_units, FLAGS.num_classes])</span><br><span class="line">            b_softmax = tf.get_variable(<span class="string">"b_softmax"</span>, [FLAGS.num_classes])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Logits</span></span><br><span class="line">        logits = rnn_softmax(FLAGS, outputs)</span><br><span class="line">        probabilities = tf.nn.softmax(logits)</span><br><span class="line">        self.accuracy = tf.equal(tf.argmax(</span><br><span class="line">            self.targets_y,<span class="number">1</span>), tf.argmax(logits,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 损失函数</span></span><br><span class="line">        self.loss = tf.reduce_mean(</span><br><span class="line">            tf.nn.sigmoid_cross_entropy_with_logits(logits, self.targets_y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化</span></span><br><span class="line">        self.lr = tf.Variable(<span class="number">0.0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">        trainable_vars = tf.trainable_variables()</span><br><span class="line">        <span class="comment"># 使用梯度截断来避免梯度消失和梯度爆炸</span></span><br><span class="line">        grads, _ = tf.clip_by_global_norm(</span><br><span class="line">            tf.gradients(self.loss, trainable_vars), FLAGS.max_gradient_norm)</span><br><span class="line">        optimizer = tf.train.AdamOptimizer(self.lr)</span><br><span class="line">        self.train_optimizer = optimizer.apply_gradients(</span><br><span class="line">            zip(grads, trainable_vars))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下面是用于采样的值</span></span><br><span class="line">        <span class="comment"># (在每个单词后生成情绪)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 取所有输出作为第一个输入序列</span></span><br><span class="line">        <span class="comment"># (由于采样，只需一个输入序列)</span></span><br><span class="line">        sampling_outputs = all_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Logits</span></span><br><span class="line">        sampling_logits = rnn_softmax(FLAGS, sampling_outputs)</span><br><span class="line">        self.sampling_probabilities = tf.nn.softmax(sampling_logits)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存模型的组件</span></span><br><span class="line">        self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">        self.saver = tf.train.Saver(tf.all_variables())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, sess, batch_X, batch_y=None, dropout=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        forward_only=True, sampling=False)</span>:</span></span><br><span class="line"></span><br><span class="line">        input_feed = &#123;self.inputs_X: batch_X,</span><br><span class="line">                      self.targets_y: batch_y,</span><br><span class="line">                      self.dropout: dropout&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> forward_only:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> sampling:</span><br><span class="line">                output_feed = [self.loss,</span><br><span class="line">                               self.accuracy]</span><br><span class="line">            <span class="keyword">elif</span> sampling:</span><br><span class="line">                input_feed = &#123;self.inputs_X: batch_X,</span><br><span class="line">                              self.dropout: dropout&#125;</span><br><span class="line">                output_feed = [self.sampling_probabilities]</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 训练</span></span><br><span class="line">            output_feed = [self.train_optimizer,</span><br><span class="line">                           self.loss,</span><br><span class="line">                           self.accuracy]</span><br><span class="line"></span><br><span class="line">        outputs = sess.run(output_feed, input_feed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> forward_only:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> sampling:</span><br><span class="line">                <span class="keyword">return</span> outputs[<span class="number">0</span>], outputs[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">elif</span> sampling:</span><br><span class="line">                <span class="keyword">return</span> outputs[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 训练</span></span><br><span class="line">            <span class="keyword">return</span> outputs[<span class="number">0</span>], outputs[<span class="number">1</span>], outputs[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>上面的代码就是我们的模型代码，它在训练的过程中使用了输入的文本。<strong>注意</strong>：为了清楚起见，我们决定将批量数据的大小保存在我们的输入和目标占位符中，但是我们应该让它们独立于一个特定的批量大小之外。由于这个特定的批量大小依赖于 <code>batch_size</code>，如果我们这么做，那么我们就还得输入一个 <code>initial_state</code>。我们通过嵌入他们来为每个数据序列来输入 token。实践策略表明，我们在输入文本上使用 skip-gram 模型预训练嵌入权重能够取得更好的性能。</p><p>在此模型中，我们再次使用 <code>dynamic_rnn</code>，但是这次我们提供了<code>sequence_length</code> 参数的值，它是一个包含每个序列长度的列表。这样，我们就可以避免在输入序列的最后一个词之后进行的不必要的计算。<strong><code>length</code></strong> 函数就用来获取这个列表的长度，如下所示。当然，我们也可以在外面计算<code>seq_len</code>，再通过占位符进行传递。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">length</span><span class="params">(data)</span>:</span></span><br><span class="line">relevant = tf.sign(tf.abs(data))</span><br><span class="line">length = tf.reduce_sum(relevant, reduction_indices=<span class="number">1</span>)</span><br><span class="line">length = tf.cast(length, tf.int32)</span><br><span class="line"><span class="keyword">return</span> length</span><br></pre></td></tr></table></figure><p>由于我们填充符 token 为 0，因此可以使用每个 token 的 sign 性质来确定它是否是一个填充符 token。如果输入大于 0，则 <code>tf.sign</code> 为 1；如果输入为 0，则为 <code>tf.sign</code> 为 0。这样，我们可以逐步通过列索引来获得 sign 值为正的 token 数量。至此，我们可以将这个长度提供给 <code>dynamic_rnn</code> 了。</p><p><strong>注意</strong>：我们可以很容易地在外部计算 <code>seq_lens</code>，并将其作为占位符进行传参。这样我们就不用依赖于 <code>PAD_ID = 0</code> 这个性质了。</p><p>一旦我们从 RNN 拿到了所有的输出和最终状态，我们就会希望分离对应输出。对于每个输入来说，将具有不同的对应输出，因为每个输入长度不一定不相同。由于我们将 <code>seq_len</code> 传给了 <code>dynamic_rnn</code>，而 <code>state</code> 又是最后一个对应输出，我们可以通过查看 <code>state</code> 来找到对应输出。注意，我们必须取 <code>state[0]</code>，因为返回的 <code>state</code> 是一个张量的元组。</p><p>其他需要注意的事情：我并没有使用 <strong><code>initial_state</code></strong>，而是直接给 <code>dynamic_rnn</code> 设置 <code>dtype</code>。此外，<code>dropout</code> 将根据 <code>forward_only</code> 与否，作为参数传递给 <strong><code>step()</code></strong>。</p><h2 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h2><p>总的来说，除了单个句子的预测外，我还想为具有一堆样本句子整体情绪进行预测。我希望看到的是，每个单词都被 RNN 读取后，将之前的单词分值保存在内存中，从而查看预测分值是怎样变化的。举例如下（值越接近 0 表明越靠近负面情绪）：</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30352d61742d382d33342d35312d706d2e706e673f773d363230.png" alt=""></p><p><strong>注意</strong>：这是一个非常简单的模型，其数据集非常有限。主要目的只是为了阐明它是如何搭建以及如何运行的。为了获得更好的性能，请尝试使用数据量更大的数据集，并考虑具体的网络架构，比如 Attention 模型、Concept-Aware 词嵌入以及隐喻（symbolization to name）等等。</p><h2 id="损失屏蔽（这里不需要）"><a href="#损失屏蔽（这里不需要）" class="headerlink" title="损失屏蔽（这里不需要）"></a>损失屏蔽（这里不需要）</h2><p>最后，我们来计算 cost。你可能会注意到我们没有做任何损失屏蔽（loss masking）处理，因为我们分离了对应输出，仅用于计算损失函数。然而，对于其他诸如机器翻译的任务来说，我们的输出很有可能还来自填充符 token。我们不想考虑这些输出，因为传递了 <code>seq_lens</code> 参数的 <code>dynamic_rnn</code> 将返回 0。下面这个例子比较简单，只用来说明这个实现大概是怎么回事；我们这里再一次使用了填充符 token 为 0 的性质：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向量化 logits 和目标</span></span><br><span class="line">targets = tf.reshape(targets, [<span class="number">-1</span>]) <span class="comment"># 将张量 targets 转为向量</span></span><br><span class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets)</span><br><span class="line">mask = tf.sign.(tf.to_float(targets)) <span class="comment"># targets 为 0 则输出为 0, target &lt; 0 则输出为 -1, 否则 为 1</span></span><br><span class="line">masked_losses = mask*losses <span class="comment"># 填充符所在位置的贡献为 0</span></span><br></pre></td></tr></table></figure><p>首先我们要将 logits 和 targets 向量化。为了使 logits 向量化，一个比较好的办法是将 <code>dynamic_rnn</code> 的输出向量化为 <code>[-1，num_hidden_units]</code> 的形状，然后乘以 softmax 权重 <code>[num_hidden_units，num_classes]</code>。通过损失屏蔽操作，就可以消除填充符所在位置贡献的损失。</p><h2 id="张量形状变化的参考"><a href="#张量形状变化的参考" class="headerlink" title="张量形状变化的参考"></a>张量形状变化的参考</h2><p>原始未处理过的文本 <code>X</code> 形状为 <code>[N,]</code> 而 <code>y</code> 的形状为 <code>[N, C]</code>，其中 <code>C</code> 是输出类别的数量（这些是手动完成的，但我们需要使用独热编码来处理多类情况）。</p><p>然后 <code>X</code> 被转化为 token 并进行填充，变成了 <code>[N, ]</code>。我们还需要传递形状为 <code>[N,]</code> 的 <code>seq_len</code> 参数，包含每个句子的长度。</p><p>现在 <code>X</code>、<code>seq_len</code> 和 <code>y</code> 通过这个模型首先嵌入为 <code>[NXD]</code>，其中 D 是嵌入维度。<code>X</code> 便从 <code>[N, ]</code> 转换为了 <code>[N, , D]</code>。回想一下，X 在这里有一个中间表示，它被独热编码为了 <code>[N, , ]</code>。但我们并不需要这么做，因为我们只需要使用对应词的索引，然后从词嵌入权重中取值就可以了。</p><p>我们需要将这个嵌入后的 <code>X</code> 传递给 <code>dynamic_rnn</code> 并返回 <code>all_outputs</code> （<code>[N, , D]</code>）以及 <code>state</code>（<code>[1, N, D]</code>）。由于我们输入了 <code>seq_lens</code>，对于我们而言它就是最后一个对应的状态。从维度的角度来说，你可以看到， <code>all_outputs</code> 就是来自 RNN 的对于每个句子中的每个词的全部输出结果。然而，<code>state</code> 仅仅只是每个句子的最后一个对应输出。</p><p>现在我们要输入 softmax 权重，但在此之前，我们需要通过取第一个索引（<code>state[0]</code>）来把状态从 <code>[1,N,D]</code> 转换为<code>[N,D]</code>。如此便可以通过与 softmax 权重 <code>[D,C]</code> 的点积，来得到形状为 <code>[N,C]</code> 的输出。其中，我们做指数级 softmax 运算，然后进行正则化，最终结合形状为 <code>[N,C]</code> 的 <code>target_y</code> 来计算损失函数。</p><p><strong>注意</strong>：如果你使用了基本的 RNN 或者 GRU，从 <code>dynamic_rnn</code> 返回的 <code>all_outputs</code> 和 <code>state</code> 的形状是一样的。但是如果使用 LSTM 的话，<code>all_outputs</code> 的形状就是 <code>[N, , D]</code> 而 <code>state</code> 的形状为 <code>[1, 2, N, D]</code>。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习探索之路 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN成长记(三)：Encoder-Decoder🔥</title>
      <link href="/rnn-series3.html"/>
      <url>/rnn-series3.html</url>
      
        <content type="html"><![CDATA[<p>在本文中，我将介绍基本的编码器（encoder）和解码器（decoder），用于处理诸如机器翻译之类的 seq2seq 任务。我们不会在这篇文章中介绍注意力机制，而在下一篇文章中去实现它。</p><p>如下图所示，我们将输入序列输入给编码器，然后将生成一个最终的隐藏状态，并将其输入到解码器中。即编码器的最后一个隐藏状态就是解码器的新初始状态。我们将使用 softmax 来处理解码器输出，并将其与目标进行比较，从而计算我们的损失函数。这里的主要区别在于，我没有向编码器的输入添加 EOS（译注：句子结束符，end-of-sentence）token，同时我也没有让编码器对句子进行反向读取。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31392d61742d342d34382d30332d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-19 at 4.48.03 PM.png"></p><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>我想创建一个非常小的数据集来使用（20 个英语和西班牙语的句子）。本教程的重点是了解如何构建一个编码解码器系统，而不是去关注这个系统对诸如机器翻译和其他 seq2seq 处理等任务的处理。所以我自己写了几个句子，然后把它们翻译成西班牙语。这就是我们的数据集。</p><p>首先，我们将这些句子分隔为 token，然后将这些 token 转换为 token ID。在这个过程中，我们收集一个词汇字典和一个反向词汇字典，以便在 token 和 token ID 之间来回转换。对于我们的目标语言（西班牙语）来说，我们将添加一个额外的 EOS token。然后，我们会将源 token 和目标 token 都填充到（对应数据集中最长句子的）最大长度。这是我们模型的输入数据。对于编码器而言，我们将填充后的源内容直接进行输入，而对于目标内容做进一步处理，以获得我们的解码器输入和输出。</p><p>最后，输入结果是这个样子的：</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31392d61742d342d32302d35342d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-19 at 4.20.54 PM.png"></p><p>这只是某个批次中的一个样本。其中 0 是填充的值，1 是 GO token，2 则是 EOS token。下图是数据变换更一般的表示形式。请无视目标权重，我们不会在实现中使用它们。</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31312d31362d61742d352d30392d31302d706d2e706e673f773d363230.png" alt="screen-shot-2016-11-16-at-5-09-10-pm"></p><h2 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h2><p>编码器只接受编码器的输入，而我们唯一关心的是最终的隐藏状态。这个隐藏的状态包含了所有输入的信息。我们不会像原始论文所建议的那样反转编码器的输入，因为我们使用的是 <code>dynamic_rnn</code> 的 <code>seq_len</code>。它会基于 <code>seq_len</code> 自动返回最后一个对应的隐藏状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'encoder'</span>) <span class="keyword">as</span> scope:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># RNN 编码器单元</span></span><br><span class="line">    self.encoder_stacked_cell = rnn_cell(FLAGS, self.dropout,</span><br><span class="line">        scope=scope)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 嵌入 RNN 编码器输入</span></span><br><span class="line">    W_input = tf.get_variable(<span class="string">"W_input"</span>,</span><br><span class="line">        [FLAGS.en_vocab_size, FLAGS.num_hidden_units])</span><br><span class="line">    self.embedded_encoder_inputs = rnn_inputs(FLAGS,</span><br><span class="line">        self.encoder_inputs, FLAGS.en_vocab_size, scope=scope)</span><br><span class="line">    <span class="comment">#initial_state = encoder_stacked_cell.zero_state(FLAGS.batch_size, tf.float32)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># RNN 编码器的输出</span></span><br><span class="line">    self.all_encoder_outputs, self.encoder_state = tf.nn.dynamic_rnn(</span><br><span class="line">        cell=self.encoder_stacked_cell,</span><br><span class="line">        inputs=self.embedded_encoder_inputs,</span><br><span class="line">        sequence_length=self.en_seq_lens, time_major=<span class="literal">False</span>,</span><br><span class="line">        dtype=tf.float32)</span><br></pre></td></tr></table></figure><p>我们将使用这个最终的隐藏状态作为解码器的新初始状态。</p><h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>这个简单的解码器将编码器的最终的隐藏状态作为自己的初始状态。我们还将接入解码器的输入，并使用 RNN 解码器来处理它们。输出的结果将通过 softmax 进行归一化处理，然后与目标进行比较。注意，解码器输入从一个 GO token 开始，从而用来预测第一个目标 token。解码器输入的最后一个对应的 token 则是用来预测 EOS 目标 token 的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'decoder'</span>) <span class="keyword">as</span> scope:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始状态是编码器的最后一个对应状态</span></span><br><span class="line">    self.decoder_initial_state = self.encoder_state</span><br><span class="line"></span><br><span class="line">    <span class="comment"># RNN 解码器单元</span></span><br><span class="line">    self.decoder_stacked_cell = rnn_cell(FLAGS, self.dropout,</span><br><span class="line">        scope=scope)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 嵌入 RNN 解码器输入</span></span><br><span class="line">    W_input = tf.get_variable(<span class="string">"W_input"</span>,</span><br><span class="line">        [FLAGS.sp_vocab_size, FLAGS.num_hidden_units])</span><br><span class="line">    self.embedded_decoder_inputs = rnn_inputs(FLAGS, self.decoder_inputs,</span><br><span class="line">        FLAGS.sp_vocab_size, scope=scope)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># RNN 解码器的输出</span></span><br><span class="line">    self.all_decoder_outputs, self.decoder_state = tf.nn.dynamic_rnn(</span><br><span class="line">        cell=self.decoder_stacked_cell,</span><br><span class="line">        inputs=self.embedded_decoder_inputs,</span><br><span class="line">        sequence_length=self.sp_seq_lens, time_major=<span class="literal">False</span>,</span><br><span class="line">        initial_state=self.decoder_initial_state)</span><br></pre></td></tr></table></figure><p>那填充值会发生什么呢？它们也会预测一些输出目标，而我们并不关心这些内容，但如果我们把它们考虑进去，它们仍然会影响我们的损失函数。接下来我们将屏蔽掉这些损失以消除对目标结果的影响。</p><h2 id="损失屏蔽"><a href="#损失屏蔽" class="headerlink" title="损失屏蔽"></a>损失屏蔽</h2><p>我们会检查目标，并将目标中被填充的部分屏蔽为 0。因此，当我们获得最后一个有关的解码器 token 时，目标就会是表示 EOS 的 token ID。而对于下一个解码器的输入而言，目标就会是 PAD ID，这也就是屏蔽开始的地方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Logit</span></span><br><span class="line">self.decoder_outputs_flat = tf.reshape(self.all_decoder_outputs,</span><br><span class="line">    [<span class="number">-1</span>, FLAGS.num_hidden_units])</span><br><span class="line">self.logits_flat = rnn_softmax(FLAGS, self.decoder_outputs_flat,</span><br><span class="line">    scope=scope)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失屏蔽</span></span><br><span class="line">targets_flat = tf.reshape(self.targets, [<span class="number">-1</span>])</span><br><span class="line">losses_flat = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    self.logits_flat, targets_flat)</span><br><span class="line">mask = tf.sign(tf.to_float(targets_flat))</span><br><span class="line">masked_losses = mask * losses_flat</span><br><span class="line">masked_losses = tf.reshape(masked_losses,  tf.shape(self.targets))</span><br><span class="line">self.loss = tf.reduce_mean(</span><br><span class="line">    tf.reduce_sum(masked_losses, reduction_indices=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>注意到可以使用 PAD ID 为 0 这个事实作为屏蔽手段，我们便只需计算（一个批次中样本的）每一行损失之和即可，然后取所有样本损失的平均值，从而得到一个批次的损失。这时，我们就可以通过最小化这个损失函数来进行训练了。</p><p>以下是训练结果：</p><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31392d61742d342d35362d31382d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-19 at 4.56.18 PM.png"></p><p>我们不会在这里做任何的模型推断，但是你可以在接下来的关于注意力机制的文章中看到。如果你真的想在这里实现模型推断，使用相同的模型就可以了，但你还得将预测目标的结果作为输入接入下一个 RNN 解码器单元。同时你还要将相同的权重集嵌入解码器中，并将其作为 RNN 的另一个输入。这意味着对于初始的 GO token 而言，你得嵌入一些伪造的 token 进行输入。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>这个编码解码器模型非常简单，但是在理解 seq2seq 实现之前，它是一个必要的基础。在下一篇 RNN 教程中，我们将涵盖 Attention 模型及其在编码解码器模型结构上的优势。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习探索之路 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> Encoder </tag>
            
            <tag> Decoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN成长记(四)：Attention机制🔥</title>
      <link href="/rnn-series4.html"/>
      <url>/rnn-series4.html</url>
      
        <content type="html"><![CDATA[<p>在这篇文章里，我们将尝试使用带有注意力机制的编码器-解码器（encoder-decoder）模型来解决序列到序列（seq-seq）问题</p><p><a href="https://github.com/ajarai/casual-digressions/blob/master/notes/images/rnn_attention/attention.png?raw=true" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/attention.png" alt="attention.png"></a></p><p>首先，让我们来一窥整个模型的架构并且讨论其中一些有趣的部分，然后我们会在先前实现的不带有注意力机制的编码器-解码器模型基础之上，添加注意力机制。我们将慢慢引入注意力机制，并实现模型的推断。。<strong>注意</strong>：这个模型并非当下最好的模型，更何况这些数据还是我在几分钟内草率地编写的。这篇文章旨在帮助你理解使用注意力机制的模型，从而你能够运用到更大的数据集上，并且取得非常不错的结果。</p><h2 id="带有注意力机制的编码器-解码器模型："><a href="#带有注意力机制的编码器-解码器模型：" class="headerlink" title="带有注意力机制的编码器-解码器模型："></a>带有注意力机制的编码器-解码器模型：</h2><p><a href="https://camo.githubusercontent.com/17fd799b46e633deb5e3be7ee9d0540916de6506/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31392d61742d352d32372d33392d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31392d61742d352d32372d33392d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-19 at 5.27.39 PM.png"></a></p><p>这张图片是第一张图的更为具体的版本，包含了更多细节。让我们从编码器开始讲起，直到最后解码器的输出。首先，我们的输入数据是经过填充（Padding）和词嵌入（Embedding）处理的向量，我们将这些向量交给带有一系列 cell（上图中蓝色的 RNN 单元）的 RNN 网络，这些 cell 的输出称为隐藏状态（hidden state，上图中的h0，h1等)，它们被初始化为零，但在输入数据之后，这些隐藏状态会改变并且持有一些非常有价值的信息。如果你使用的是一个 LSTM 网络（RNN 的一种），我们会把 cell 的状态 c 和隐藏状态 h 一起向前传递给下一个 cell。对于每一个输入（上图中的 X0等），在每一个 cell 上我们都会得到一个隐藏状态的输出，这个输出也会作为下一个 cell 输入的一部分。我们把每个神经元的输出记作 h1 到 hN，这些输出将会成为我们注意力模型的输入。</p><p>在我们深入探讨注意力机制之前，先来看看解码器是怎么处理它的输入以及如何产生输出的。目标语言经过同样的词嵌入处理后作为解码器的输入，以 GO 标识开始，以 EOS 和其后的一些填充部分作为结束。解码器的 RNN cell 同样有着隐藏状态，并且和上面一样，被初始化为零且随着数据的输入而产生变化。这样看来，解码器和编码器似乎没有什么不同。事实上，它们的不同之处在于解码器还会接收一个由注意力机制产生的上下文向量 ci作为输入。在接下来的部分里，我们将详细地讨论上下文向量是如何产生的，它是基于编码器的所有输入以及前面解码器 cell 的隐藏状态所产生的一个非常重要的成果：上下文向量能够指导我们在编码器产生的输入上如何分配注意力，来更好地预测接下来的输出。</p><p>解码器的每一个 cell 利用编码器产生的输入，和前一个 cell 的隐藏状态以及注意力机制产生的上下文向量来计算，最后经过 softmax 函数产生最终的目标输出。值得注意的是，在训练的过程中，每个 RNN cell 只使用这三个输出来获得目标的输出，然而在推断阶段中，我们并不知道解码器的下一个输入是什么。因此我们将使用解码器之前的预测结果来作为新的输入。</p><p>现在，让我们仔细看看注意力机制是怎么产生上下文向量的。</p><h2 id="注意力机制："><a href="#注意力机制：" class="headerlink" title="注意力机制："></a>注意力机制：</h2><p><a href="https://camo.githubusercontent.com/7430ee857f426f1bf7b967badfc263285b9583dc/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31392d61742d352d32372d34392d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31392d61742d352d32372d34392d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-19 at 5.27.49 PM.png"></a></p><p>上图是注意力机制的示意图，让我们先关注注意力层的输入和输出部分：我们利用编码器产生的所有隐藏状态以及上一个解码器 cell 的输出，来给每一个解码器 cell 生成对应的上下文向量。首先，这些输入都会经过一层 tanh 函数来产生一个形状为 [N, H] 的输出矩阵e，编码器中每个 cell 的输出都会产生对应解码器中第 i 个 cell 的一个 eij。接下来对矩阵 e 应用一次 softmax 函数，就能得到一个关于各个隐藏状态的概率，我们把这个结果记作 alpha。然后再利用 alpha 和原来的隐藏状态矩阵 h 相乘，使得每个 h 中的每一个隐藏状态获得个权重，最后进行求和就得到了形状为 [N, H] 的上下文向量 ci，实际上这就是编码器产生的输入的一个带有权重分布的表示。</p><p>在训练开始，这个上下文向量可能会比较随意，但是随着训练的进行，我们的模型将会不断地学习编码器产生的输入中哪一部分是重要的，从而帮助我们在解码器这一端产生更好的结果。</p><h2 id="Tensorflow-实现："><a href="#Tensorflow-实现：" class="headerlink" title="Tensorflow 实现："></a>Tensorflow 实现：</h2><p>现在让我们来实现这个模型，其中最重要的部分就是注意力机制。我们将使用一个单向的 GRU 编码器和解码器，和前面那篇<a href="https://theneuralperspective.com/2016/11/20/recurrent-neural-networks-rnn-part-3-encoder-decoder/" target="_blank" rel="noopener"><strong>文章</strong></a>里使用的非常类似，区别在于这里的解码器将会额外地使用上下文向量（表示注意力分配）来作为输入。另外，我们还将使用 Tensorflow 里的 <code>embedding_attention_decoder()</code>接口。</p><p>首先，让我们来了解一下将要处理并传递给编码器/解码器的数据集。</p><h3 id="数据"><a href="#数据" class="headerlink" title="数据:"></a>数据:</h3><p>我为模型创建了一个很小的数据集：20 个英语和对应的西班牙语句子。这篇教程的重点是让你了解如何建立一个带有软注意力机制的编码器-解码器模型，来解决像机器翻译等的序列到序列问题。所以我写了关于我自己的 20 个英文句子，然后把他们翻译成对应的西班牙语，这就是我们的数据。</p><p>首先，我们把这些句子变成一系列 token，再把 token 转换成对应的词汇 id。在这个处理过程中，我们会建立一个词汇词典，使我们能够从 token 和词汇 id 之间完成转换。对于我们的目标语言（西班牙语），我们会额外地添加一个 EOS 标识。接下来我们将对源语言和目标语言转换得来的一组 token 进行填充操作，将它们补齐至最大长度（分别是它们各自的数据集中的最长句子长度），这将成为最终我们要喂给我们模型的数据。我们把经过填充的源语言数据传给编码器，但我们还会对目标语言的输入做一些额外的操作以获得解码器的输入和输出。</p><p>最后，输入就长成下面这个样子：</p><p><a href="https://camo.githubusercontent.com/eceba4e462ca8400bb05f04e6107c71019299032/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31392d61742d342d32302d35342d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31392d61742d342d32302d35342d706d2e706e673f773d363230-20200520160216200.png" alt="Screen Shot 2016-11-19 at 4.20.54 PM.png"></a></p><p>这只是数据集中的一个例子，向量里的 0 都是填充的部分，1 是 GO 标识，2 则是一个 EOS 标识。下图是数据处理过程更一般的表示，你可以忽略掉 target weights 这一部分，因为我们的实现中不会用到它。</p><p><a href="https://camo.githubusercontent.com/560d5259bc15e599b7e266bfcdea11d776b7faeb/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31312d31362d61742d352d30392d31302d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31312d31362d61742d352d30392d31302d706d2e706e673f773d363230-20200520160223020.png" alt="screen-shot-2016-11-16-at-5-09-10-pm"></a></p><h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>我们通过 <code>encoder_inputs</code> 来给编码器输入数据。输入数据的是一个形状为 <strong>[N, max_len]</strong> 的矩阵，通过词嵌入变成 <strong>[N, max_len, H]</strong>。编码器是一个动态 RNN，经过它的处理之后，我们得到一个形状为 <strong>[N, max_len, H]</strong> 的输出，以及一个状态矩阵，形为 <strong>[N, H]</strong>（这就是所有句子经 RNN 网络后最后一个 cell 相关的状态）。这些都将作为我们编码器的输出。</p><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>在讨论注意力机制之前，先来看看解码器的输入和输出。解码器的初始状态就是由编码器传递来的，每个句子经过 RNN 网络后最有一个 cell 的状态（形为 <strong>[N, H]</strong>)。Tensorflow 的 <code>embedding_attention_decoder()</code> 函数要求解码器的输入是按先后顺序排列的（句子中词的先后）的列表，所以我们把 <strong>[N, max_len]</strong> 的输入转换为 max_len 长的列表 <strong>[N]</strong>。我们还使用经过 softmax 作用的权重矩阵处理解码器的输出，来创建我们的输出投影权重。我们将时序列表（即经过转换的 [N, max_len]）、初始状态、注意力矩阵以及投影权重作为参数传递给 <code>embedding_attention_deocder()</code> 函数，得到输出（形状为 <strong>[max_len, N, H] *<em>的输出以及状态矩阵 *</em>[N, H]</strong>）。我们得到的输出也是按时间先后排列的，我们将对它们进行 flatten 操作并且应用 softmax 函数得到一个形为 [N * max_len, C] 的矩阵。然后我们同样对目标输出进行 reshape 操作，从 <strong>[N, max_len]</strong> 变成 <strong>[N * max_len,]</strong> ，再利用 <code>sparse_softmax_cross_entropy_with_logits()</code> 来计算 loss 。接下来我们会对 loss 进行一些遮蔽操作，来避免填充操作对 loss 造成的影响。</p><h3 id="注意力"><a href="#注意力" class="headerlink" title="注意力:"></a>注意力:</h3><p>最后，总算到了注意力机制这一部分。我们已经知道了输入和输出，我们把一系列参数（时序列表、初始状态、注意力矩阵这些编码器的输出）交给了 <code>embedded_attention_decoder()</code> 函数，但在这其中究竟发生了什么？首先， 我们会创建一系列权重来对输入进行嵌入操作，我们把这些权重命名为 W_embedding。在通过输入生成解码器的输出之后，我们会开始一个循环函数，来决定将哪一部分输出交给下一个解码器作为输入。在训练过程中，我们通常不会把前一个解码器单元的输出传递给下一个，所以这里的循环函数是 None。而在推理期间，我们会这样做，所以这里的循环函数就会使用 <code>_extract_argmax_and_embed()</code>，它的用处就如它的名字所言（提取参数并且嵌入）。得到解码器单元的输出之后，让它和 softmax 后的权重矩阵相乘（output_projection），并将它的形状从 <strong>[N, H]</strong> 转换成 <strong>[N, C]</strong>，再使用同样 W_embedding 来替代经过嵌入操作的输出(<strong>[N, H]</strong>)，再将经过处理的输出作为下一个解码器单元的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果我们需要预测下一个词语的话，使用如下的循环函数</span></span><br><span class="line">loop_function = _extract_argmax_and_embed(</span><br><span class="line">    W_embedding, output_projection,</span><br><span class="line">    update_embedding_for_previous) <span class="keyword">if</span> feed_previous <span class="keyword">else</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><h2 id=""><a href="#" class="headerlink" title=""></a><a href="https://camo.githubusercontent.com/1882c15e40189689bb53c5bd950fec946bfc7337/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d32322d61742d372d35332d34302d616d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d32322d61742d372d35332d34302d616d2e706e673f773d363230.png" alt="Screen Shot 2016-11-22 at 7.53.40 AM.png"></a></h2><p>另外一个关于循环函数可选的参数是 <code>update_embedding_</code> ，如果设置为 False，那么在我们对解码器的输出（GO token 除外）进行嵌入操作的时候，就会停止在 W_embedding 权重上使用梯度更新。因此，虽然我们在两个地方使用了 W_embedding，但它的值只依赖于我们在解码器的输入上使用的词嵌入而不是在输出上（GO token 除外）。然后，我们就可以把经过嵌入操作的时序解码器输入、初始状态、注意力矩阵以及循环函数交给 <code>attention_decoder()</code> 函数了。</p><p><code>attention_decoder()</code> 函数是注意力机制的核心，这其中有一些额外的操作是文章开头那篇论文中没有提到的。回忆一下，注意力机制将会使用我们的注意力矩阵（编码器的输出）以及前一个解码器单元的状态，这些值将被传入一个 tanh 层，通过隐藏状态得到一个 e_ij（用来衡量句子对齐的程度的变量）。然后，我们将使用 softmax 函数将它转换为 alpha_ij 用于和与原始注意力矩阵相乘。我们对这个相乘之后的向量进行求和，这就是我们的新的上下文向量c_i。最终，这个上下文向量将被用来产生我们新的解码器的输出。</p><p>主要的不同之处在于，我们的注意力矩阵（编码器的输出）和前一解码器单元的状态不是简简单单通过一个 <code>_linear()</code> 函数能够处理，并且应用常规的 tanh 函数的。我们需要一些额外的步骤来解决这个问题：首先，对注意力矩阵使用一个 1x1 的卷积，这能够帮助我们在注意力矩阵中提取重要的 features，而不是直接处理原有的数据——你可以回想一下卷积层在图样识别中重要的特征提取作用。这一步能够让我们拥有更好的特征，但带来的一个问题就是我们需要用一个 4 维的向量来表示注意力矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">形状转换:</span></span><br><span class="line"><span class="string">    初始的隐藏状态:</span></span><br><span class="line"><span class="string">        [N, max_len, H]</span></span><br><span class="line"><span class="string">    reshape 成 4D 的向量:</span></span><br><span class="line"><span class="string">        [N, max_len, 1, H] = N 张 [max_len, 1, H] 形状的图片</span></span><br><span class="line"><span class="string">        所以我们可以在上面应用滤波器</span></span><br><span class="line"><span class="string">    滤波器:</span></span><br><span class="line"><span class="string">        [1, 1, H, H] = [height, width, depth, # num filters]</span></span><br><span class="line"><span class="string">    使用 stride 为 1 和 padding 为 1 的卷积:</span></span><br><span class="line"><span class="string">        H = ((H - F + 2P) / S) + 1 =</span></span><br><span class="line"><span class="string">            ((max_len - 1 + 2)/1) + 1 = height'</span></span><br><span class="line"><span class="string">        W = ((W - F + 2P) / S) + 1 = ((1 - 1 + 2)/1) + 1 = 3</span></span><br><span class="line"><span class="string">        K = K = H</span></span><br><span class="line"><span class="string">        结果就是把</span></span><br><span class="line"><span class="string">            [N, max_len, H] 变成了 [N, height', 3, H]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">hidden = tf.reshape(attention_states,</span><br><span class="line">    [<span class="number">-1</span>, attn_length, <span class="number">1</span>, attn_size]) <span class="comment"># [N, max_len, 1, H]</span></span><br><span class="line">hidden_features = []</span><br><span class="line">attention_softmax_weights = []</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> xrange(num_heads):</span><br><span class="line">    <span class="comment"># 滤波器</span></span><br><span class="line">    k = tf.get_variable(<span class="string">"AttnW_%d"</span> % a,</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, attn_size, attn_size]) <span class="comment"># [1, 1, H, H]</span></span><br><span class="line">    hidden_features.append(tf.nn.conv2d(hidden, k, [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], <span class="string">"SAME"</span>))</span><br><span class="line">    attention_softmax_weights.append(tf.get_variable(</span><br><span class="line">        <span class="string">"W_attention_softmax_%d"</span> % a, [attn_size]))</span><br></pre></td></tr></table></figure><p>这就意味着，为了处理经过转换的 4 维注意力矩阵和前一解码器单元状态，我们需要把后者也转换成 4 维的表示。这个操作很简单，只要将前一解码器单元的状态通过一个 MLP 的处理，就能把它变成一个 4 维的 tensor，从而匹配注意力矩阵的转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">y = tf.nn.rnn_cell._linear(</span><br><span class="line">    args=query, output_size=attn_size, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># reshape 成 4 D</span></span><br><span class="line">y = tf.reshape(y, [<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>, attn_size]) <span class="comment"># [N, 1, 1, H]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 Alpha</span></span><br><span class="line">s = tf.reduce_sum(</span><br><span class="line">    attention_softmax_weights[a] *</span><br><span class="line">    tf.nn.tanh(hidden_features[a] + y), [<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">a = tf.nn.softmax(s)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算上下文向量 c</span></span><br><span class="line">c = tf.reduce_sum(tf.reshape(</span><br><span class="line">    a, [<span class="number">-1</span>, attn_length, <span class="number">1</span>, <span class="number">1</span>])*hidden, [<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">cs.append(tf.reshape(c, [<span class="number">-1</span>, attn_size]))</span><br></pre></td></tr></table></figure><p>将注意力矩阵和前一解码器单元的状态都进行过转换之后，我们就可以进行 tanh 操作了。我们将 tanh 后的结果和 softmax 得到的权重进行相乘、求和，再应用一次 softmax 函数得到 alpha_ij。最后，我们将 alphas 经过 reshape 后和初始注意力矩阵相乘，进行求和之后得到我们的上下文向量 c_i。</p><p>接下来就可以挨个地处理解码器的输入了。先讨论训练过程，我们不在乎解码器的输出因为输入最终都会变成输出，所以这里的循环函数是 None。我们将通过一个使用 <code>_linear()</code>函数的 MLP 以及前一个上下文向量来处理解码器输入（初始化为零），然后和前一个解码器单元的状态一起交给 dynamic_rnn 单元得到输出。我们一次处理所有样本数据的同一时刻的 token，因为我们需要从当时索引的最后一个 token 所对应的前一个状态。按时序排列的输入使我们在一批数据中这样做更为高效，<strong>这</strong>就是为什么我们需要输入变成一个时序列表的原因。</p><p>得到动态 RNN 的输出和状态之后，我们就能够根据新的状态计算出新的上下文向量。cell 的输出和新的上下文向量再通过一个 MLP，最终就能得到我们的解码器输出。这些额外的 MLP 并没有在解码器的示意图中画出，但他们是我们得到输出必要的额外步骤。值得注意的是，cell 的输出和 attention_decoder 的输出的形状都是<strong>[max_len, N, H]</strong>。</p><p>而当我们在进行推断的时候，循环函数不再是 None，而是 <code>_extract_argmax_and_append()</code>。这个函数会接收前一个解码器单元的输出，而我们新的解码器单元的输入就是先前的输出经过 softmax 之后的结果，接下来对它进行重嵌入操作。在利用注意力矩阵进行w完所有处理之后，将 prev 将被更新为新预测的输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 依次处理解码器的输入</span></span><br><span class="line"><span class="keyword">for</span> i, inp <span class="keyword">in</span> enumerate(decoder_inputs):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">        tf.get_variable_scope().reuse_variables()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> loop_function <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> prev <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"loop_function"</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">            inp = loop_function(prev, i)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把输入和注意力向量合并</span></span><br><span class="line">    input_size = inp.get_shape().with_rank(<span class="number">2</span>)[<span class="number">1</span>]</span><br><span class="line">    x = tf.nn.rnn_cell._linear(</span><br><span class="line">        args=[inp]+attns, output_size=input_size, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解码器 RNN</span></span><br><span class="line">    cell_outputs, state = cell(x, state) <span class="comment"># our stacked cell</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过注意力拿到上下文向量</span></span><br><span class="line">    attns = attention(state)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'attention_output_projection'</span>):</span><br><span class="line">        output = tf.nn.rnn_cell._linear(</span><br><span class="line">            args=[cell_outputs]+attns, output_size=output_size,</span><br><span class="line">            bias=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> loop_function <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        prev = output</span><br><span class="line">    outputs.append(output)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> outputs, state</span><br></pre></td></tr></table></figure><p>然后，我们处理从 attention_decoder 得到的输出：使用 softmax 函数、进行 flatten 操作，最后和目标输出进行比较并计算 loss。</p><h2 id="细节"><a href="#细节" class="headerlink" title="细节:"></a>细节:</h2><h3 id="Sampled-Softmax"><a href="#Sampled-Softmax" class="headerlink" title="Sampled Softmax"></a>Sampled Softmax</h3><p>在机器翻译这样的序列对序列的任务上使用注意力机制模型的效果是非常出色的，但常常因为语料库的巨大带来问题。特别是在我们训练时，计算解码器的输出的 softmax 是非常耗费资源的，解决的办法就是使用 sampled softmax</p><p>下面是 sampled softmax 的代码，注意这里的权重和我们在解码器上使用的 output_projection 是一样的，因为使用它们的目的都是相同的：把解码器的输出（长度为 H 的向量）转换成对应类别数量长度的向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampled_loss</span><span class="params">(inputs, labels)</span>:</span></span><br><span class="line">    labels = tf.reshape(labels, [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">    <span class="comment"># We need to compute the sampled_softmax_loss using 32bit floats to</span></span><br><span class="line">    <span class="comment"># avoid numerical instabilities.</span></span><br><span class="line">    <span class="comment"># 我们使用32位的浮点数来计算 sampled_softmax_loss ，以避免数值不稳定</span></span><br><span class="line">    local_w_t = tf.cast(w_t, tf.float32)</span><br><span class="line">    local_b = tf.cast(b, tf.float32)</span><br><span class="line">    local_inputs = tf.cast(inputs, tf.float32)</span><br><span class="line">    <span class="keyword">return</span> tf.cast(</span><br><span class="line">            tf.nn.sampled_softmax_loss(local_w_t, local_b,</span><br><span class="line">                local_inputs, labels,</span><br><span class="line">                num_samples, self.target_vocab_size),</span><br><span class="line">            dtype)</span><br><span class="line">softmax_loss_function = sampled_loss</span><br></pre></td></tr></table></figure><p>接下来，我们可以利用 seq_loss 函数来计算 loss，其中的权重向量除了目标输出为 PAD token 的部分是 0，其他都是 1。值得注意的是，我们只会在训练过程中使用 sampled softmax，而在进行预测的过程中，我们会对整个语料库进行采样，使用常规的 softmax，而不仅仅只是一部分最为近似的语料。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    losses.append(sequence_loss(</span><br><span class="line">      outputs, targets, weights,</span><br><span class="line">      softmax_loss_function=softmax_loss_function))</span><br></pre></td></tr></table></figure><h3 id="带有-buckets-的模型"><a href="#带有-buckets-的模型" class="headerlink" title="带有 buckets 的模型:"></a>带有 buckets 的模型:</h3><p>另外一种常见的附加结构是使用 <code>tf.nn.seq2seq.model_with_buckets()</code> 函数，这种 buckets 模型的优点在于缩短了注意力矩阵向量的长度。在先前的模型中，我们会把注意力向量应用在 max_len 长度的 hidden states 上。而在这里，我们只要对相关的一部分应用注意力向量，因为 PAD token 是完全可以被忽略的。我们可以选择对应的 buckets 使得句子中的 PAD token 尽可能的少。</p><p>但我个人觉得这个方法有一点粗糙，而且如果真的想要避免处理 PAD token 的话，我会建议使用 seq_lens 这个属性来过滤掉编码器输出中的 PAD token，或者当我们在计算上下文向量的时候，我们可以把每个句子中 PAD token 对应的 hidden state 置为 0。这种方法有点复杂，所以我们不在这里实现它，但 buckets 对于 PAD token 带来的噪音确实不是一种优雅的解决方法。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h2><p>注意力机制是研究的一个热门，并且也存在很多变种。无论在什么情况下，这种模型在序列对序列的任务上总是能有非常出色的表现，所以我非常喜欢使用它。请谨慎地分割训练集和验证集，因为这种模型很容易过拟合从而在验证集上产生非常糟糕的表现。在接下来的文章里，我们会使用注意力机制来解决设计内存和逻辑推理的更为复杂的任务。</p><h2 id="矩阵形状分析"><a href="#矩阵形状分析" class="headerlink" title="矩阵形状分析:"></a>矩阵形状分析:</h2><p>编码器的输出形为 <strong>[N, max_len]</strong>，经过嵌入操作之后转变为 <strong>[N, max_len, H]</strong>，然后交给编码器 RNN。编码器的输出形为 <strong>[N, max_len, H]</strong>，状态矩阵形为 <strong>[N, H]</strong>，其中包含了各个样本最后的 cell 的状态。</p><p>编码器的输出和注意力向量的形状都是 <strong>[N, max_len, H]</strong>。</p><p>解码器的输出形为 <strong>[N, max_len]</strong>，会被转换为一个 <strong>max_len</strong> 长度的时序列表，其中每个向量的形状为 <strong>N</strong>。解码器的初始状态就是编码器形为 <strong>[N, H]</strong> 的状态矩阵。在将数据输入解码器 RNN 之前，数据会被进行嵌入操作，变成一个 max_len 长度的时序列表，其中的每个向量形状为 [N, H]。输入数据可能是真实的解码器输入，或者在进行预测的时候，就是由前一个解码器 cell 产生的输出。前一个解码器 cell 在前一刻产生的输出形为 <strong>[N, H]</strong>，将经过一层 softmax 层（输出投影）而变成 <strong>[N, C]</strong>。然后使用我们在输入上使用的权重向量，再一次进行嵌入操作变回 <strong>[N, H]</strong>。这些输入将被喂给解码器 RNN，从而产生解码器形为 <strong>[max_len, N, H]</strong> 的输出以及状态矩阵 <strong>[N, H]</strong>。输出将被进行 flatten 操作而变成 <strong>[N* max_len, H]</strong> 并且和同样经过 flatten 操作的目标输出进行比较（同样形为 <strong>[N* max_len, H]</strong>)。如果目标输出中有 PAD token 的话，在计算 loss 的时候会进行一些遮蔽操作，接下来就是 backprop 了。</p><p>在解码器 RNN 内部，同样有一些形状转变的操作。首先注意力向量（编码器输出）形为 <strong>[N, max_len, H]</strong>，将被转化为一个 4 维的向量 <strong>[N, max_len, 1, H]</strong>（这样我们就可以使用卷积操作了）并且利用卷积来提取有用的特征。这些隐藏特征的形状也是 4 维，<strong>[N, height , 3, H]</strong>。解码器的前一隐藏状态x向量，形为 <strong>[N, H]</strong>，同样是注意力机制的一个输入。这个隐藏状态向量经过一个 MLP 变成 <strong>[N, H]</strong> （这么做的原因是为了防止前一隐藏状态的第二维（H）和 attention_size 不同，在这里同样是 H）。接下来这个隐藏状态向量同样被转换成一个 4 维向量 <strong>[N, 1, 1, H]</strong>，这样我们就可以将它和隐藏特征相结合。我们对相加的结果使用 tanh 函数，再通过 softmax 函数得到 alpha_ij，其形状为 <strong>[N, max_len, 1, 1]</strong> （这代表了每个样本中各个隐藏状态的概率）。这个 alpha 和原始的隐藏状态相乘，得到形为 <strong>[N, max_len, 1, H]</strong>的向量，再进行求和得到形为 <strong>[N, H]</strong> 的上下文向量。</p><p>上下文向量和解码器的形为 <strong>[N, H]</strong> 的输入结合，无论这个输入是来自解码器的输入数据（训练时候）还是来自前一个 cell 的预测（预测时候），这个输入只是长度为 <strong>max_len</strong> 列表中形为 <strong>[N, H]</strong> 向量的其中一个。首先我们让它和前一个上下文向量相加（初始化为全 0 的 <strong>[N, H]</strong> 矩阵），回想一下我们的来自于解码器输入的数据是一个时序列表，长度为 <strong>N</strong>，其中的向量形为 <strong>[max_len, ]</strong>，这就是为什么输入的形状都是 <strong>[N, H]</strong>。相加的结果将经过一层 MLP，得到一个形为 <strong>[N, H]</strong> 的输出，这和状态矩阵（形状为 <strong>[N, H]</strong>）将被交给我们的动态 RNN cell 。得到的输出 cell_outputs 形为 <strong>[N, H]</strong>，并且状态矩阵同样为 <strong>[N, H]</strong>。这个新的状态矩阵将会成为们下一个解码器的输入。我们对 max_len 个输入进行这样的操作，从而得到了一个长度为 max_len 的是列表，其中的向量都是 [N, H]。在从解码器得到这个输出和状态矩阵之后，我们将新的状态矩阵传给 attention 函数得到新的上下文向量，新的上下文向量形为 <strong>[N, H]</strong>，并且和形为 <strong>[N, H]</strong> 的输出相加，再一次应用 MLP，转换成形为 <strong>[N, H]</strong> 的向量。最后，如果我们在进行预测，新的 prev 将会成为我们的最终输出（prev 初始为 none）。prev 将会成为 loop_function 的输入，来得到下一个解码器的输出。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习探索之路 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN成长记(五)：LSTM-GRU🔥</title>
      <link href="/rnn-series5.html"/>
      <url>/rnn-series5.html</url>
      
        <content type="html"><![CDATA[<p>在本文中，我们将探索并尝试创建我们自己定义的 RNN 单元。不过在此之前，我们需要先仔细研究简单的 RNN，再逐步深入较为复杂的单元（如 LSTM 与 GRU）。我们会分析这些单元在 tensorflow 中的实现代码，最终参照这些代码来创建我们的自定义单元。</p><h2 id="基本-RNN："><a href="#基本-RNN：" class="headerlink" title="基本 RNN："></a>基本 RNN：</h2><p>对于传统的 RNN 来说，最大的问题就在于每个单元的重复输入都是静态的，因此我们无法充分学习到长期的依赖情况。你回想一下基本 RNN 单元，就会发现所有操作都是单一的 tanh 运算。</p><p><a href="https://camo.githubusercontent.com/e2e06d7548e05136770657d10745d3704cb000e5/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d352d35342d31332d616d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31302f73637265656e2d73686f742d323031362d31302d30342d61742d352d35342d31332d616d2e706e673f773d363230-20200520160347807.png" alt="screen-shot-2016-10-04-at-5-54-13-am"></a></p><p>对于解决短期依赖情况的问题来说，这种结构已经够用了；但如果我们希望通过有效的长期记忆来预测目标，则需要使用更稳定强大的 RNN 单元 —— LSTM。</p><h2 id="长短期记忆网络（LSTM）："><a href="#长短期记忆网络（LSTM）：" class="headerlink" title="长短期记忆网络（LSTM）："></a>长短期记忆网络（LSTM）：</h2><p>LSTM 的结构可以让我们在更多的操作中进行长期的信息控制。传统的 RNN 仅有一个输出，其既作为隐藏状态表示也作为此单元的输出端。</p><p><a href="https://camo.githubusercontent.com/e96358309e47b8389ede73b05cd41d2c82e32f04/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d32352d30342d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d32352d30342d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-16 at 6.25.04 PM.png"></a></p><p>这种结构缺乏对信息的控制，无法存住对许多步之后有用的信息。而 LSTM 有两种不同的输出。其中一种仍与前面的传统结构一样，既作为隐藏状态表示也作为单元输出；但 LSTM 单元还有另一种输出 - 单元状态 C。这也是 LSTM 精髓所在，让我们仔细研究它。</p><p><a href="https://camo.githubusercontent.com/bd7256bea2a35247b080c720df03b025701bdd50/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d32382d30362d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d32382d30362d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-16 at 6.28.06 PM.png"></a></p><h3 id="遗忘门："><a href="#遗忘门：" class="headerlink" title="遗忘门："></a>遗忘门：</h3><p>第一个要介绍的门就是遗忘门。这个门可以让我们选择性地传递信息以决定单元的状态。我将公式罗列在下，后面介绍其它的门时也会如此。</p><p><a href="https://camo.githubusercontent.com/ff214a722f09094fe7e59d550cd97f93a103446c/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d33302d33382d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d33302d33382d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-16 at 6.30.38 PM.png"></a></p><p><a href="https://camo.githubusercontent.com/4d5790d3306ceaf5d17b6b4ab21f3d5133e50e79/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d33392d31372d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d33392d31372d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-16 at 6.39.17 PM.png"></a></p><p>你可以参考类似 tf 的 _linear 函数来实现它。不过遗忘门的主要要点是对输入与隐藏状态前应用了 sigmoid。那么这个 sigmoid 的作用是什么？请回想一下，sigmoid 会输出在 [0, 1] 范围的值，在此我们将其应用于 [N X H] 的矩阵，因此会得到 NXH 个 sigmoid 算出的值。如果 sigmoid 得到 0 值，那么其对应的隐藏值就会失效；如果 sigmoid 得到 1 值，那么此隐藏值将会被应用在计算中。而处于 0 和 1 之间的值将会允许一部分的信息继续传递。这样就能很好地通过阻塞与选择性地传递输入单元的数据，以达到控制信息的目的。</p><p>这就是遗忘门。它是我们的单元得到最终结果前的第一个步骤。下面介绍另一个操作：输入门。</p><h3 id="输入门："><a href="#输入门：" class="headerlink" title="输入门："></a>输入门：</h3><p>输入门将获取我们的输入值 X 以及在前面的隐藏状态，并对它们进行两次运算。首先会通过 sigmoid 门来选择性地允许部分数据输入，接着将其与输入值的 tanh 值相乘。</p><p><a href="https://camo.githubusercontent.com/80272cbc8c54034e60a816e4a18aba35b8c18463/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d34382d30372d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d34382d30372d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-16 at 6.48.07 PM.png"></a></p><p>这儿的 tanh 与前面的 sigmoid 操作不同。请回忆一下，tanh 会将输入值改变为 [-1, 1] 范围内的值。它本质上通过非线性的方式改变了输入的表示。这一步与我们在基本 RNN 单元中进行的操作一致，不过在此我们将两值的乘积加上遗忘门得到的值得到本单元的状态值。</p><p>遗忘门与输入门的操作可以看做同时保存了旧状态（C_{t-1}）的一部分与新变换（tanh）单元状态（C~_t）的一部分。这些权重将会通过我们数据的训练学到需要保存多少数据以及如何进行正确的变换。</p><h3 id="输出门："><a href="#输出门：" class="headerlink" title="输出门："></a>输出门：</h3><p>最后一个门是输出门，它利用输入值、前面的隐藏状态值以及新单元状态值来共同决定新隐藏状态的表示。</p><p><a href="https://camo.githubusercontent.com/71807a3fe2fab5226bf2fe9745046bec745f88a0/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d35342d32392d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d362d35342d32392d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-16 at 6.54.29 PM.png"></a></p><p>该步骤依旧涉及到了 sigmoid，将它的值与单元状态的 tanh 值相乘以决定信息的去留。需要注意这一步的 tanh 计算与输入门的 tanh 计算不同，此步不再是神经网络的计算，而仅仅是单纯、不带任何权重地计算单元状态值的 tanh 值。这样我们就能强制单元状态矩阵 [NXH] 的值处于 [-1, 1] 的范围内。</p><h3 id="变体"><a href="#变体" class="headerlink" title="变体"></a>变体</h3><p>RNN 单元有许多种变体，不过目前 LSTM 的竞争对手是正在被广泛使用的 GRU（Gated Recurrent Unit）。</p><h2 id="GRU（Gated-Recurrent-Unit）："><a href="#GRU（Gated-Recurrent-Unit）：" class="headerlink" title="GRU（Gated Recurrent Unit）："></a>GRU（Gated Recurrent Unit）：</h2><p>GRU 的主要原理是将遗忘门与输入门结合成一个更新门。</p><p><a href="https://camo.githubusercontent.com/dff41dc0c5ea4aae156feeaa3b91e384c43661a4/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d372d30312d31352d706d2e706e673f773d363230" target="_blank" rel="noopener"><img src="https://picreso.oss-cn-beijing.aliyuncs.com/68747470733a2f2f7468656e657572616c70657273706563746976652e66696c65732e776f726470726573732e636f6d2f323031362f31312f73637265656e2d73686f742d323031362d31312d31362d61742d372d30312d31352d706d2e706e673f773d363230.png" alt="Screen Shot 2016-11-16 at 7.01.15 PM.png"></a></p><p>在实际使用中，GRU 的性能与 LSTM 相当，但其计算量更小，因此它现在日益流行。</p><h2 id="原生-Tensorflow-实现："><a href="#原生-Tensorflow-实现：" class="headerlink" title="原生 Tensorflow 实现："></a>原生 Tensorflow 实现：</h2><p>我们先观察一下 Tensorflow 官方对于 GRU 单元的实现代码，主要关注其函数调用方式、输入以及输出。然后我们会复制它的结构用于创建我们自己的单元。本文将主要关注 GRU，因为它在大多数情况下性能与 LSTM 相当且复杂度更低。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GRUCell</span><span class="params">(RNNCell)</span>:</span></span><br><span class="line">  <span class="string">"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_units, input_size=None, activation=tanh)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> input_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      logging.warn(<span class="string">"%s: The input_size parameter is deprecated."</span>, self)</span><br><span class="line">    self._num_units = num_units</span><br><span class="line">    self._activation = activation</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self._num_units</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Gated recurrent unit (GRU) with nunits cells."""</span></span><br><span class="line">    <span class="keyword">with</span> vs.variable_scope(scope <span class="keyword">or</span> type(self).__name__):  <span class="comment"># "GRUCell"</span></span><br><span class="line">      <span class="keyword">with</span> vs.variable_scope(<span class="string">"Gates"</span>):  <span class="comment"># Reset gate and update gate.</span></span><br><span class="line">        <span class="comment"># We start with bias of 1.0 to not reset and not update.</span></span><br><span class="line">        r, u = array_ops.split(<span class="number">1</span>, <span class="number">2</span>, _linear([inputs, state],</span><br><span class="line">                                             <span class="number">2</span> * self._num_units, <span class="literal">True</span>, <span class="number">1.0</span>))</span><br><span class="line">        r, u = sigmoid(r), sigmoid(u)</span><br><span class="line">      <span class="keyword">with</span> vs.variable_scope(<span class="string">"Candidate"</span>):</span><br><span class="line">        c = self._activation(_linear([inputs, r * state],</span><br><span class="line">                                     self._num_units, <span class="literal">True</span>))</span><br><span class="line">      new_h = u * state + (<span class="number">1</span> - u) * c</span><br><span class="line">    <span class="keyword">return</span> new_h, new_h</span><br></pre></td></tr></table></figure><p>GRUCell 类由 <strong>init</strong> 函数开始执行。在 <strong>init</strong> 函数中定义了单元的数量与其使用的激活函数。其激活函数一般是 tanh，不过也可以使用 sigmoid 来使得值固定在 [0, 1] 范围内方便我们控制信息流。另外，它还有两个在调用时会返回 self._num_units 的属性。最后定义了 <strong>call</strong> 函数，它将处理输入值并得出新的隐藏值。回忆一下，GRU 没有类似 LSTM 的单元状态值。</p><p>在 <strong>call</strong> 中，我们首先计算 r 和 u（u 是前面图中的 z）。在这步中，我们没有单独去计算它们，而是以乘以 2 倍 num_units 的形式合并了权重，再将结果分割成两份得到它们（split(dim, num_splits, value)）。然后对得到的值应用 sigmoid 激活函数，以选择性地控制信息流。接着计算 c 的值，用它计算新隐藏状态表示值。你可能发现它计算 new_h 的顺序和之前颠倒了，不过由于权重会同时进行训练，因此代码仍能正常运行。</p><p>其它的单元代码都与此代码类似，你弄明白了上面的代码就能轻松解释其它单元的代码。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习探索之路 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
            <tag> RNN </tag>
            
            <tag> GRU </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>404</title>
      <link href="/404.html"/>
      <url>/404.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<center><p><img src="https://picreso.oss-cn-beijing.aliyuncs.com/circle-cropped.png" style="width:10%;" /></center></p><div align="center" style="font-size:120%"><p>An AI Lover🤖 A Crazy Coder💻  A Life Explorer🌈</p><h3 id="About-Me👨‍🎓"><a href="#About-Me👨‍🎓" class="headerlink" title="About Me👨‍🎓"></a>About Me👨‍🎓</h3><p>I call myself <code>Jarvis Liu</code> from 《IRON MAN》</p><p><strong>一个四肢💪不发达，头脑🧠也不简单的Coder</strong> <strong>👓</strong></p><p><strong>热爱编程</strong>💻，<strong>热爱分享</strong>📚，<strong>热爱人工智能</strong>🤖️，<strong>热爱生活</strong>🌈</p><hr><h3 id="Location："><a href="#Location：" class="headerlink" title="Location："></a>Location：</h3><p>Chengdu🍵 of China🇨🇳</p><h3 id="School："><a href="#School：" class="headerlink" title="School："></a>School：</h3><p>University of Electronic Science and Technology of China🇨🇳</p><h3 id="Research"><a href="#Research" class="headerlink" title="Research:"></a>Research:</h3><p>一个在读大学本科生，软件工程专业👨‍🎓。</p><p>热爱一切技术🔥</p><p>喜欢折腾电子产品🖥📱💻</p><p>主要方向是<code>自然语言处理</code>💬，<code>机器学习</code>🤖，<code>深度学习</code>🌊</p><p><strong>🌈梦想着用人工智能🤖️去让世界更美好，为社会做贡献🤔</strong></p><h2 id="Social"><a href="#Social" class="headerlink" title="Social"></a>Social</h2><h3 id="If-you-want-to-find-me👀"><a href="#If-you-want-to-find-me👀" class="headerlink" title="If you want to find me👀"></a>If you want to find me👀</h3><p>WeChat💬：<code>k08101203</code></p>]]></content>
      
    </entry>
    
    
  
</search>
